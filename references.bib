@book{abernathey_datascience:2024,
  title = {An {{Introduction}} to {{Earth}} and {{Environmental Data Science}}},
  author = {Abernathey, Ryan},
  date = {2024},
  url = {https://earth-env-data-science.github.io/intro.html},
  urldate = {2024-12-17},
  abstract = {History: This book grew out of a course developed at Columbia University called Research Computing in Earth Science. It was written mostly by Ryan Abernathey, with significant contributions from Ke...}
}

@article{amonkar_heatingcooling:2023,
  title = {Differential Effects of Climate Change on Average and Peak Demand for Heating and Cooling across the Contiguous {{USA}}},
  author = {Amonkar, Yash and Doss-Gollin, James and Farnham, David J. and Modi, Vijay and Lall, Upmanu},
  date = {2023-11-01},
  journaltitle = {Communications Earth \& Environment},
  shortjournal = {Commun Earth Environ},
  volume = {4},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2662-4435},
  doi = {10.1038/s43247-023-01048-1},
  url = {https://www.nature.com/articles/s43247-023-01048-1},
  urldate = {2023-11-01},
  abstract = {While most electricity systems are designed to handle peak demand during summer months, long-term energy pathways consistent with deep decarbonization generally electrify building heating, thus increasing electricity demand during winter. A key question is how climate variability and change will affect peak heating and cooling demand in an electrified future. We conduct a spatially explicit analysis of trends in temperature-based proxies of electricity demand over the past 70\,years. Average annual demand for heating (cooling) decreases (increases) over most of the contiguous US. However, while climate change drives robust increases in peak cooling demand, trends in peak heating demand are generally smaller and less robust. Because the distribution of temperature exhibits a long left tail, severe cold snaps dominate the extremes of thermal demand. As building heating electrifies, system operators must account for these events to ensure reliability.},
  issue = {1}
}

@book{applegate_raes:2015,
  title = {Risk {{Analysis}} in the {{Earth Sciences}}},
  author = {Applegate, Patrick and Keller, Klaus},
  date = {2015-10-26},
  publisher = {Leanpub},
  url = {https://leanpub.next/raes},
  urldate = {2025-07-21},
  abstract = {A free e-textbook that teaches the Earth science and statistical concepts needed to assess climate-related risks, using the R programming environment.}
}

@article{arribas_assessment:2022,
  title = {Climate Risk Assessment Needs Urgent Improvement},
  author = {Arribas, Alberto and Fairgrieve, Ross and Dhu, Trevor and Bell, Juliet and Cornforth, Rosalind and Gooley, Geoff and Hilson, Chris J. and Luers, Amy and Shepherd, Theodore G. and Street, Roger and Wood, Nick},
  date = {2022-08-25},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {4326},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-31979-w},
  url = {https://www.nature.com/articles/s41467-022-31979-w},
  urldate = {2023-03-23},
  abstract = {Existing constraints in current climate risk assessments make them inappropriate to effectively assess the true exposure of society and businesses to climate-related risk. Using the key constraints to guide a conceptual framework, we identify four cross-cutting and inter-related critical paths for improvement.},
  issue = {1}
}

@article{arrow_discount:2013,
  title = {Determining Benefits and Costs for Future Generations},
  author = {Arrow, K. and Cropper, M. and Gollier, C. and Groom, B. and Heal, G. and Newell, R. and Nordhaus, W. and Pindyck, R. and Pizer, W. and Portney, P. and Sterner, T. and Tol, R. S. J. and Weitzman, M.},
  date = {2013-07-26},
  journaltitle = {Science},
  volume = {341},
  number = {6144},
  eprint = {23888025},
  eprinttype = {pubmed},
  pages = {349--350},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1235665},
  url = {https://science.sciencemag.org/content/341/6144/349},
  urldate = {2020-04-30},
  abstract = {In economic project analysis, the rate at which future benefits and costs are discounted relative to current values often determines whether a project passes the benefit-cost test. This is especially true of projects with long time horizons, such as those to reduce greenhouse gas (GHG) emissions. Whether the benefits of climate policies, which can last for centuries, outweigh the costs, many of which are borne today, is especially sensitive to the rate at which future benefits are discounted. This is also true of other policies, e.g., affecting nuclear waste disposal or the construction of long-lived infrastructure. The United States and others should consider adopting a different approach to estimating costs and benefits in light of uncertainty. The United States and others should consider adopting a different approach to estimating costs and benefits in light of uncertainty.}
}

@book{arrow_values:1951,
  title = {Social Choice and Individual Values},
  author = {Arrow, Kenneth J.},
  date = {1951},
  publisher = {Wiley},
  location = {New York}
}

@article{bankes_exploratory:1993,
  title = {Exploratory Modeling for Policy Analysis},
  author = {Bankes, Steve},
  date = {1993-06-01},
  journaltitle = {Operations Research},
  shortjournal = {Operations Research},
  volume = {41},
  number = {3},
  pages = {435--449},
  issn = {0030-364X},
  doi = {10/c7rgcr},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.41.3.435},
  urldate = {2019-02-19},
  abstract = {Exploratory modeling is using computational experiments to assist in reasoning about systems where there is significant uncertainty. While frequently confused with the use of models to consolidate knowledge into a package that is used to predict system behavior, exploratory modeling is a very different kind of use, requiring a different methodology for model development. This paper distinguishes these two broad classes of model use describes some of the approaches used in exploratory modeling, and suggests some technological innovations needed to facilitate it.}
}

@article{bastani_gpt4:2025,
  title = {Generative {{AI}} without Guardrails Can Harm Learning: {{Evidence}} from High School Mathematics},
  author = {Bastani, Hamsa and Bastani, Osbert and Sungu, Alp and Ge, Haosen and Kabakcı, Özge and Mariman, Rei},
  date = {2025-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {122},
  number = {26},
  pages = {e2422633122},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2422633122},
  url = {https://www.pnas.org/doi/10.1073/pnas.2422633122},
  urldate = {2025-07-18},
  abstract = {Generative AI is poised to revolutionize how humans work, and has already demonstrated promise in significantly improving human productivity. A key question is how generative AI affects learning—namely, how humans acquire new skills as they perform tasks. Learning is critical to long-term productivity, especially since generative AI is fallible and users must check its outputs. We study this question via a field experiment where we provide nearly a thousand high school math students with access to generative AI tutors. To understand the differential impact of tool design on learning, we deploy two generative AI tutors: one that mimics a standard ChatGPT interface (“GPT Base”) and one with prompts designed to safeguard learning (“GPT Tutor”). Consistent with prior work, our results show that having GPT-4 access while solving problems significantly improves performance (48\% improvement in grades for GPT Base and 127\% for GPT Tutor). However, we additionally find that when access is subsequently taken away, students actually perform worse than those who never had access (17\% reduction in grades for GPT Base)—i.e., unfettered access to GPT-4 can harm educational outcomes. These negative learning effects are largely mitigated by the safeguards in GPT Tutor. Without guardrails, students attempt to use GPT-4 as a “crutch” during practice problem sessions, and subsequently perform worse on their own. Thus, decision-makers must be cautious about design choices underlying generative AI deployments to preserve skill learning and long-term productivity.}
}

@book{bishop_deeplearning:2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  date = {2024},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urldate = {2025-02-25},
  isbn = {978-3-031-45467-7 978-3-031-45468-4}
}

@book{blitzstein_probability:2019,
  title = {Introduction to {{Probability}}, {{Second Edition}}},
  author = {Blitzstein, Joseph K. and Hwang, Jessica},
  date = {2019-02-08},
  edition = {2nd Edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {Boca Raton},
  url = {http://probabilitybook.net},
  abstract = {Developed from celebrated Harvard statistics lectures, Introduction to Probability provides essential language and tools for understanding statistics, randomness, and uncertainty. The book explores a wide variety of applications and examples, ranging from coincidences and paradoxes to Google PageRank and Markov chain Monte Carlo (MCMC). Additional application areas explored include genetics, medicine, computer science, and information theory. The authors present the material in an accessible style and motivate concepts using real-world examples. Throughout, they use stories to uncover connections between the fundamental distributions in statistics and conditioning to reduce complicated problems to manageable pieces.The book includes many intuitive explanations, diagrams, and practice problems. Each chapter ends with a section showing how to perform relevant simulations and calculations in R, a free statistical software environment. The second edition adds many new examples, exercises, and explanations, to deepen understanding of the ideas, clarify subtle concepts, and respond to feedback from many students and readers. New supplementary online resources have been developed, including animations and interactive visualizations, and the book has been updated to dovetail with these resources.  Supplementary material is available on Joseph Blitzstein’s website www. stat110.net. The supplements include:Solutions to selected exercisesAdditional practice problemsHandouts including review material and sample exams Animations and interactive visualizations created in connection with the edX online version of Stat 110.Links to lecture videos available on ITunes U and YouTube There is also a complete instructor's solutions manual available to instructors who require the book for a course.},
  isbn = {978-1-138-36991-7},
  pagetotal = {634}
}

@article{bonnafous_waterrisk:2017,
  title = {A Water Risk Index for Portfolio Exposure to Climatic Extremes: Conceptualization and an Application to the Mining Industry},
  author = {Bonnafous, Luc and Lall, Upmanu and Siegel, Jason},
  date = {2017},
  journaltitle = {Hydrology and Earth System Sciences},
  volume = {21},
  number = {4},
  pages = {2075--2106},
  doi = {10/f96k67},
  url = {https://www.hydrol-earth-syst-sci.net/21/2075/2017/},
  abstract = {Corporations, industries and non-governmental organizations have become increasingly concerned with growing water risks in many parts of the world. Most of the focus has been on water scarcity and competition for the resource between agriculture, urban users, ecology and industry. However, water risks are multi-dimensional. Water-related hazards include flooding due to extreme rainfall, persistent drought and pollution, either due to industrial operations themselves, or to the failure of infrastructure. Most companies have risk management plans at each operational location to address these risks to a certain design level. The residual risk may or may not be managed, and is typically not quantified at a portfolio scale, i.e. across many sites. Given that climate is the driver of many of these extreme events, and there is evidence of quasi-periodic climate regimes at inter-annual and decadal timescales, it is possible that a portfolio is subject to persistent, multi-year exceedances of the design level. In other words, for a multi-national corporation, it is possible that there is correlation in the climate-induced portfolio water risk across its operational sites as multiple sites may experience a hazard beyond the design level in a given year. Therefore, from an investor's perspective, a need exists for a water risk index that allows for an exploration of the possible space and/or time clustering in exposure across many sites contained in a portfolio. This paper represents a first attempt to develop an index for financial exposure of a geographically diversified, global portfolio to the time-varying risk of climatic extremes using long daily global rainfall datasets derived from climate re-analysis models. Focusing on extreme daily rainfall amounts and using examples from major mining companies, we illustrate how the index can be developed. We discuss how companies can use it to explore their corporate exposure, and what they may need to disclose to investors and regulators to promote transparency as to risk exposure and mitigation efforts. For the examples of mining companies provided, we note that the actual exposure is substantially higher than would be expected in the absence of space and time correlation of risk as is usually tacitly assumed. We also find evidence for the increasing exposure to climate-induced risk, and for decadal variability in exposure. The relative vulnerability of different portfolios to multiple extreme events in a given year is also demonstrated.}
}

@article{bullard_dismantling:1999,
  title = {Dismantling {{Environmental Racism}} in the {{USA}}},
  author = {Bullard, Robert D.},
  date = {1999-02-01},
  journaltitle = {Local Environment},
  volume = {4},
  number = {1},
  pages = {5--19},
  publisher = {Routledge},
  issn = {1354-9839},
  doi = {10.1080/13549839908725577},
  url = {https://doi.org/10.1080/13549839908725577},
  urldate = {2024-04-01},
  abstract = {A growing body of evidence reveals that people of colour and low‐income persons have borne greater environmental and health risks than the society at large in their neighbourhoods, workplaces and playgrounds. Over the past decade or so, grassroots activists have attempted to change the way government implements environmental, health and civil rights laws. A new movement has emerged in opposition to environmental racism and environmental injustice. Over the past two decades or so, grassroots activists have had some success in changing the way the federal government treats communities of colour and their inhabitants. Grassroots groups have also organised, educated and empowered themselves to improve the way health and environmental policies are administered. Environmentalism is now equated with social justice and civil rights.}
}

@article{busby_cascadingrisks:2021,
  title = {Cascading Risks: {{Understanding}} the 2021 Winter Blackout in {{Texas}}},
  author = {Busby, Joshua W. and Baker, Kyri and Bazilian, Morgan D. and Gilbert, Alex Q. and Grubert, Emily and Rai, Varun and Rhodes, Joshua D. and Shidore, Sarang and Smith, Caitlin A. and Webber, Michael E.},
  date = {2021-07-01},
  journaltitle = {Energy Research \& Social Science},
  shortjournal = {Energy Research \& Social Science},
  volume = {77},
  pages = {102106},
  publisher = {Elsevier},
  location = {Amsterdam},
  issn = {2214-6296},
  doi = {10.1016/j.erss.2021.102106},
  url = {outage},
  urldate = {2021-06-03},
  abstract = {The Texas freeze of February 2021 left more than 4.5 million customers (more than 10 million people) without electricity at its peak, some for several days. The freeze had cascading effects on other services reliant upon electricity including drinking water treatment and medical services. Economic losses from lost output and damage are estimated to be \$130 billion in Texas alone. In the wake of the freeze, there has been major fallout among regulators and utilities as actors sought to apportion blame and utilities and generators began to settle up accounts. This piece offers a retrospective on what caused the blackouts and the knock-on effects on other services, the subsequent financial and political effects of the freeze, and the implications for Texas and the country going forward. Texas failed to sufficiently winterize its electricity and gas systems after 2011. Feedback between failures in the two systems made the situation worse. Overall, the state faced outages of 30 GW of electricity as demand reached unprecedented highs. The gap between production and demand forced the non-profit grid manager, the Electric Reliability Council of Texas (ERCOT), to cut off supply to millions of customers or face a systems collapse that by some accounts was minutes away. The 2021 freeze suggests a need to rethink the state’s regulatory approach to energy to avoid future such outcomes. Weatherization, demand response, and expanded interstate interconnections are potential solutions Texas should consider to avoid generation losses, reduce demand, and tap neighboring states’ capacity.}
}

@book{coles_extremes:2001,
  title = {An Introduction to Statistical Modeling of Extreme Values},
  author = {Coles, Stuart},
  date = {2001},
  series = {Springer Series in Statistics},
  publisher = {Springer},
  location = {London},
  isbn = {1-85233-459-2}
}

@online{condon_climateservices:2023,
  type = {SSRN Scholarly Paper},
  title = {Climate Services: The Business of Physical Risk},
  author = {Condon, Madison},
  date = {2023-03-22},
  number = {4396826},
  location = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=4396826},
  urldate = {2023-03-22},
  abstract = {A growing number of investors, insurers, financial services providers, and nonprofits rely on information about localized physical climate risks, like floods, hurricanes, and wildfires. The outcomes of these risk projections have significant consequences in the economy, including allocating investment capital, impacting housing prices and demographic shifts, and prioritizing adaptation infrastructure projects. The climate risk information available to individual citizens and municipalities, however, is limited and expensive to access. Further, many providers of climate services use black box models that make overseeing the scientific rigor of their methodologies impossible— a concern given scientific critiques that many may be obfuscating the uncertainty in their projections. Municipalities that want to challenge insurance and bond rating determinations must rally significant resources for modeling and data, a scattershot policing method at best. And when companies have access to sophisticated modeling about future impacts— some of them potentially devastating for entire communities—the decision to share that information has been largely left up to the corporation.This Article argues that actionable and transparent information about our climate-changed future is a public good that the private sector cannot be depended upon to provide equitably or reliably. Further, all private climate services rely on upstream climate data and models that were collected and produced by an enormous network of public institutions. There are important lessons to be learned from the recent success of special interests in pressing for the privatization of weather data and services—a trend that has knock-on effects for weather forecasts globally. This Article urges state and federal governments to invest in their own climate services capacity at a scale not currently contemplated. Risk assessments lacking a scientific basis can lead to maladaptation across the economy. While it is a potentially limited matter of consumer protection or tort liability when a consultancy over-promises its analytical capabilities, it is a much larger problem if regulators themselves misunderstand the limits of uncertainty when designing risk oversight.},
  pubstate = {prepublished}
}

@article{condon_myopia:2021,
  ids = {condon_:2022},
  title = {Market Myopia's Climate Bubble},
  author = {Condon, Madison},
  date = {2021-02-09},
  journaltitle = {Utah Law Review},
  volume = {63},
  number = {2022},
  doi = {10.2139/ssrn.3782675},
  url = {https://papers.ssrn.com/abstract=3782675},
  urldate = {2021-04-13},
  abstract = {A growing number of financial institutions, ranging from BlackRock to the Bank of England, have warned that markets may not be accurately incorporating climate change-related risks into asset prices. This Article seeks to explain how this mispricing can exist at the level of individual assets drawing from scholarship on corporate governance and the mechanisms of market (in)efficiency. Market actors: 1. Lack the fine-grained asset-level data they need in order to assess risk exposure; 2.  Continue to rely on outdated means of assessing risk; 3. Have misaligned incentives resulting in climate-specific agency costs; 4. Have myopic biases exacerbated by climate change misinformation; and 5. Are impeded by captured regulators distorting the market. Further, trends in institutional share ownership reinforce apathy regarding assessment of firm-specific fundamentals, especially over long-term horizons. This underpricing of corporate climate risk contributes to the negative effects of climate change itself, as the mispricing of risk in the present leads to a misallocation of investment capital, hindering future adaptation and subsidizing future fossil combustion. These risks could accumulate to the macroeconomic scale, generating a systemic risk to the financial system. While a broad array of government interventions are necessary to mitigate climate related financial risks, this Article focuses on proposals for corporate governance and securities regulation—and their limits. Signals from the Biden Administration suggest that mandatory climate risk disclosure regulation from the Securities and Exchange Commission is forthcoming. This Article argues that climate risk disclosure is necessary, though alone not sufficient, to address the widespread disregard of corporate climate exposure.}
}

@book{cressie_spatiotemporal:2011,
  title = {Statistics for Spatio-Temporal Data},
  author = {Cressie, Noel A. C. and Wikle, Christopher K.},
  date = {2011},
  publisher = {Wiley},
  location = {Hoboken, N.J.},
  isbn = {978-0-471-69274-4}
}

@article{daigavane_:2021,
  title = {Understanding {{Convolutions}} on {{Graphs}}},
  author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
  date = {2021-09-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {9},
  pages = {e32},
  issn = {2476-0757},
  doi = {10.23915/distill.00032},
  url = {https://distill.pub/2021/understanding-gnns},
  urldate = {2025-07-07},
  abstract = {Understanding the building blocks and design choices of graph neural networks.}
}

@book{debreu_axiomatic:1959,
  title = {Theory of Value; an Axiomatic Analysis of Economic Equilibrium},
  author = {Debreu, Gerard},
  date = {1959},
  publisher = {Yale Univeristy Press},
  location = {New Haven, CT},
  pagetotal = {114}
}

@article{deconto_antarctica:2016,
  title = {Contribution of {{Antarctica}} to Past and Future Sea-Level Rise},
  author = {DeConto, Robert M. and Pollard, David},
  date = {2016-03},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {531},
  number = {7596},
  pages = {591--597},
  issn = {1476-4687},
  doi = {10.1038/nature17145},
  url = {http://www.nature.com/articles/nature17145},
  urldate = {2020-02-20},
  abstract = {Climate and ice-sheet modelling that includes ice fracture dynamics reveals that Antarctica could contribute more than a metre of sea-level rise by 2100 and more than 13\,metres by 2500, if greenhouse gas emissions continue unabated.}
}

@article{demoel_reducing:2014,
  title = {Evaluating the Effect of Flood Damage-Reducing Measures: A Case Study of the Unembanked Area of {{Rotterdam}}, the {{Netherlands}}},
  author = {family=Moel, given=Hans, prefix=de, useprefix=true and family=Vliet, given=Mathijs, prefix=van, useprefix=true and Aerts, Jeroen C. J. H.},
  date = {2014-06-01},
  journaltitle = {Regional Environmental Change},
  shortjournal = {Reg Environ Change},
  volume = {14},
  number = {3},
  pages = {895--908},
  issn = {1436-378X},
  doi = {10.1007/s10113-013-0420-z},
  url = {https://doi.org/10.1007/s10113-013-0420-z},
  urldate = {2021-03-16},
  abstract = {Empirical evidence of increasing flood damages and the prospect of climatic change has initiated discussions in the flood management community on how to effectively manage flood risks. In the Netherlands, the framework of multi-layer safety (MLS) has been introduced to support this risk-based approach. The MLS framework consists of three layers: (i) prevention, (ii) spatial planning and (iii) evacuation. This paper presents a methodology to evaluate measures in the second layer, such as wet proofing, dry proofing or elevating buildings. The methodology uses detailed land-use data for the area around the city of Rotterdam (up to building level) that has recently become available. The vulnerability of these detailed land-use classes to flooding is assessed using the stage–damage curves from different international models. The methodology is demonstrated using a case study in the unembanked area of Rotterdam in the Netherlands, as measures from the second layer may be particularly effective there. The results show that the flood risk in the region is considerable: EUR 36 million p.a. A large part (almost 60~\%) of this risk results from industrial land use, emphasising the need to give this category more attention in flood risk assessments. It was found that building level measures could substantially reduce flood risks in the region because of the relatively low inundation levels of buildings. Risk to residential buildings would be reduced by 40~\% if all buildings would be wet-proofed, by 89~\% if all buildings would be dry-proofed and elevating buildings over 100~cm would render the risk almost zero. While climate change could double the risk in 2100, such building level measures could easily nullify this effect. Despite the high potential of such measures, actual implementation is still limited. This is partly caused by the lack of knowledge regarding these measures by most Dutch companies and the legal impossibility for municipalities to enforce most of these measures as they would go beyond the building codes established at the national level.}
}

@article{deneufville_parkinggarage:2006,
  title = {Real {{Options}} by {{Spreadsheet}}: {{Parking Garage Case Example}}},
  author = {family=Neufville, given=Richard, prefix=de, useprefix=true and Scholtes, Stefan and Wang, Tao},
  date = {2006},
  journaltitle = {Journal of Infrastructure Systems},
  shortjournal = {Journal of Infrastructure Systems},
  volume = {12},
  number = {2},
  pages = {107--111},
  doi = {10.1061/(asce)1076-0342(2006)12:2(107)},
  url = {https://ascelibrary.org/doi/abs/10.1061/(ASCE)1076-0342(2006)12:2(107)},
  urldate = {2018-12-15}
}

@article{dittes_uncertainty:2018,
  title = {Managing Uncertainty in Flood Protection Planning with Climate Projections},
  author = {Dittes, Beatrice and Špačková, Olga and Schoppa, Lukas and Straub, Daniel},
  date = {2018},
  journaltitle = {Hydrology and Earth System Sciences},
  volume = {22},
  number = {4},
  pages = {2511--2526},
  doi = {10.5194/hess-22-2511-2018},
  url = {https://www.hydrol-earth-syst-sci.net/22/2511/2018/}
}

@article{doss-gollin_review:2023,
  title = {Improving the Representation of Climate Risks in Long-Term Electricity Systems Planning: A Critical Review},
  author = {Doss-Gollin, James and Amonkar, Yash and Schmeltzer, Katlyn and Cohan, Daniel},
  date = {2023-08-16},
  journaltitle = {Current Sustainable/Renewable Energy Reports},
  shortjournal = {Curr Sustainable Renewable Energy Rep},
  issn = {2196-3010},
  doi = {10.1007/s40518-023-00224-3},
  url = {https://doi.org/10.1007/s40518-023-00224-3},
  urldate = {2023-08-16},
  abstract = {Purpose of Review Electricity systems face substantial climate risks which are escalating due to electrification, renewable energy intermittency, population changes, and the intensifying impacts of climate change such as extreme temperatures and weather-induced infrastructure damage. This critical review investigates climate risks to the electricity sector and scrutinizes the methodologies used to represent climate risk in long-term electricity system planning studies. Recent Findings Climate risks to electricity systems are driven by extreme weather, average weather, technology, and other social and technological factors. All are expected to evolve in the future. Future climate risks to electricity systems depend on interactions between each, and thus assessing future climate risks to electricity systems requires exploring a wide range of possible futures.  Summary Many studies rely on weather data and socio-economic scenarios that are inadequate to fully characterize climate risks to present and future electricity systems. We advocate for more holistic assessments that incorporate comprehensive weather data, acknowledge dynamic multi-sector interactions, and employ adaptive and robust methodologies.},
  preprint = {https://eartharxiv.org/repository/view/5684/}
}

@article{doss-gollin_robustadaptation:2019,
  title = {Robust Adaptation to Multiscale Climate Variability},
  author = {Doss-Gollin, James and Farnham, David J. and Steinschneider, Scott and Lall, Upmanu},
  date = {2019-06-07},
  journaltitle = {Earth's Future},
  volume = {7},
  number = {7},
  pages = {734--747},
  issn = {2328-4277},
  doi = {10.1029/2019ef001154},
  url = {http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019EF001154},
  urldate = {2019-06-09},
  abstract = {The assessment and implementation of structural or financial instruments for climate risk mitigation requires projections of future climate risk over the operational life of each proposed instrument. A point often neglected in the climate adaptation literature is that the physical sources of predictability differ between projects with long and short planning periods: while historical and paleo climate records emphasize modes of variability, anthropogenic climate change is expected to alter their occurrence at longer time scales. In this paper we present a set of stylized experiments to assess the uncertainties and biases involved in estimating future climate risk over a finite future period, given a limited observational record. These experiments consider both quasi-periodic and secular change for the underlying risk, as well as statistical models for estimating this risk from an N-year historical record. The uncertainty of IPCC-like future scenarios is considered through an equivalent sample size N. The relative importance of estimating the short- or long-term risk extremes depends on the investment life M. Shorter design lives are preferred for situations where inter-annual to decadal variability can be successfully identified and predicted, suggesting the importance of sequential investment strategies for adaptation.},
  open = {true},
  repo = {https://github.com/jdossgollin/2018-robust-adaptation-cyclical-risk}
}

@article{doss-gollin_subjective:2023,
  title = {A Subjective {{Bayesian}} Framework for Synthesizing Deep Uncertainties in Climate Risk Management},
  author = {Doss-Gollin, James and Keller, Klaus},
  date = {2023-01},
  journaltitle = {Earth's Future},
  volume = {11},
  number = {1},
  issn = {2328-4277},
  doi = {10.1029/2022EF003044},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022EF003044},
  urldate = {2022-12-31},
  abstract = {Projections of nonstationary climate risks can vary considerably from one source to another, posing considerable communication and decision-analytical challenges. One such challenge is how to present trade-offs under deep uncertainty in a salient and interpretable manner. Some common approaches include analyzing a small subset of projections or treating all considered projections as equally likely. These approaches can underestimate risks, hide deep uncertainties, and are mostly silent on which assumptions drive decision-relevant outcomes. Here we introduce and demonstrate a transparent Bayesian framework for synthesizing deep uncertainties to inform climate risk management. The first step of this workflow is to generate an ensemble of simulations representing possible futures and analyze them through standard exploratory modeling techniques. Next, a small set of probability distributions representing subjective beliefs about the likelihood of possible futures is used to weight the scenarios. Finally, these weights are used to compute and characterize trade-offs, conduct robustness checks, and reveal implicit assumptions. We demonstrate the framework through a didactic case study analyzing how high to elevate a house to manage coastal flood risks.},
  preprint = {https://doi.org/10.1002/essoar.10511798.4},
  repo = {https://github.com/jdossgollin/2022-elevation-robustness}
}

@article{doss-gollin_txtreme:2021,
  title = {How Unprecedented Was the {{February}} 2021 {{Texas}} Cold Snap?},
  author = {Doss-Gollin, James and Farnham, David J. and Lall, Upmanu and Modi, Vijay},
  date = {2021-06-08},
  journaltitle = {Environmental Research Letters},
  shortjournal = {Environ. Res. Lett.},
  issn = {1748-9326},
  doi = {10.1088/1748-9326/ac0278},
  url = {e},
  urldate = {2021-05-18},
  abstract = {Winter storm Uri brought severe cold to the southern United States in February 2021, causing a cascading failure of interdependent systems in Texas where infrastructure was not adequately prepared for such cold. In particular, the failure of interconnected energy systems restricted electricity supply just as demand for heating spiked, leaving millions of Texans without heat or electricity, many for several days. This motivates the question: did historical storms suggest that such temperatures were known to occur, and if so with what frequency? We compute a temperature-based proxy for heating demand and use this metric to answer the question “what would the aggregate demand for heating have been had historic cold snaps occurred with today’s population?”. We find that local temperatures and the inferred demand for heating per capita across the region served by the Texas Interconnection were more severe during a storm in December 1989 than during February 2021, and that cold snaps in 1951 and 1983 were nearly as severe. Given anticipated population growth, future storms may lead to even greater infrastructure failures if adaptive investments are not made. Further, electricity system managers should prepare for trends in electrification of heating to drive peak annual loads on the Texas Interconnection during severe winter storms.},
  open = {true},
  preprint = {https://eartharxiv.org/repository/view/2122/},
  repo = {https://github.com/jdossgollin/2021-TXtreme}
}

@book{downey_thinkbayes:2021,
  title = {Think {{Bayes}}},
  author = {Downey, Allen B.},
  date = {2021-05-18},
  publisher = {"O'Reilly Media, Inc."},
  url = {https://allendowney.github.io/ThinkBayes2/},
  abstract = {If you know how to program, you're ready to tackle Bayesian statistics. With this book, you'll learn how to solve statistical problems with Python code instead of mathematical formulas, using discrete probability distributions rather than continuous mathematics. Once you get the math out of the way, the Bayesian fundamentals will become clearer and you'll begin to apply these techniques to real-world problems.Bayesian statistical methods are becoming more common and more important, but there aren't many resources available to help beginners. Based on undergraduate classes taught by author Allen B. Downey, this book's computational approach helps you get a solid start.Use your programming skills to learn and understand Bayesian statisticsWork with problems involving estimation, prediction, decision analysis, evidence, and Bayesian hypothesis testingGet started with simple examples, using coins, dice, and a bowl of cookiesLearn computational methods for solving real-world problems},
  isbn = {978-1-4920-8941-4},
  pagetotal = {369}
}

@article{ellsberg_ambiguity:1961,
  title = {Risk, Ambiguity, and the {{Savage}} Axioms},
  author = {Ellsberg, Daniel},
  date = {1961-11-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {Q J Econ},
  volume = {75},
  number = {4},
  pages = {643--669},
  publisher = {Oxford Academic},
  issn = {0033-5533},
  doi = {10.2307/1884324},
  url = {http://academic.oup.com/qje/article/75/4/643/1913802},
  urldate = {2020-04-10},
  abstract = {Abstract.  I. Are there uncertainties that are not risks? 643. — II. Uncertainties that are not risks, 647. — III. Why are some uncertainties not risks? — 656.}
}

@article{farnham_credibly:2018,
  ids = {farnham_orbrep:2018},
  title = {Regional Extreme Precipitation Events: Robust Inference from Credibly Simulated {{GCM}} Variables},
  author = {Farnham, David J and Doss-Gollin, James and Lall, Upmanu},
  date = {2018},
  journaltitle = {Water Resources Research},
  volume = {54},
  number = {6},
  doi = {10.1002/2017wr021318},
  url = {https://doi.org/10.1002/2017wr021318},
  abstract = {eneral circulation models (GCMs) have been demonstrated to produce estimates of precipitation, including the frequency of extreme precipitation, with substantial bias and uncertainty relative to their representation of other fields. Thus, while theory predicts changes in the hydrologic cycle under anthropogenic warming, there is generally low confidence in future projections of extreme precipitation frequency for specific river basins. In this paper, we explore whether a GCM simulates large-scale atmospheric circulation indices that are associated with regional extreme precipitation (REP) days more accurately than it simulates REP days themselves, and thus whether conditional simulation of the precipitation events based on the circulation indices may improve the simulation of REP events. We show that a coupled Geophysical Fluid Dynamics Laboratory GCM simulates too many springtime REP days in the Ohio River Basin in historical (1950–2005) simulations. The GCM, however, does credibly simulate the distributional and persistence properties of several indices (which represent the large-scale atmospheric pressure features, local atmospheric moisture content, and local vertical velocity) that are shown to modulate the likelihood of REP occurrence in the reanalysis/observational record. We show that simulation of REP events based on the GCM-based atmospheric indices greatly reduces the bias of GCM REP frequency relative to the observed record. The simulation is conducted via a Bayesian regression model by imposing the empirical relationship between observed REP occurrence and the reanalysis-based atmospheric indices. Application of this model to future (2006–2100) representative concentration pathway 8.5 scenario suggests an increasing trend in springtime REP incidence in the study region. The proposed approach of simulating precipitation events of interest, particularly those poorly represented in GCMs, with a statistical model based on climate indices that are reasonably simulated by GCMs could be applied to subseasonal to seasonal forecasts as well as future projections.},
  open = {true},
  repo = {https://github.com/d-farnham/ORB\_Paper/}
}

@article{fletcher_mombasa:2019,
  title = {Learning about Climate Change Uncertainty Enables Flexible Water Infrastructure Planning},
  author = {Fletcher, Sarah and Lickley, Megan and Strzepek, Kenneth},
  date = {2019-04-16},
  journaltitle = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1782},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09677-x},
  url = {http://www.nature.com/articles/s41467-019-09677-x},
  urldate = {2019-04-23},
  abstract = {Water resources planning requires infrastructure development consider regional climatic uncertainties. Here the authors introduce a new dynamic planning framework that captures opportunities to learn about climate change over time. By applying it to reservoir planning in Kenya, they show the value of flexible approaches in responding to learning.}
}

@article{fletcher_waterresourcesequity:2022,
  title = {Equity in {{Water Resources Planning}}: {{A Path Forward}} for {{Decision Support Modelers}}},
  author = {Fletcher, Sarah and Hadjimichael, Antonia and Quinn, Julianne and Osman, Khalid and Giuliani, Matteo and Gold, David and Figueroa, Anjuli Jain and Gordon, Bethany},
  date = {2022-07},
  journaltitle = {Journal of Water Resources Planning and Management},
  shortjournal = {J. Water Resour. Plann. Manage.},
  volume = {148},
  number = {7},
  pages = {02522005},
  issn = {0733-9496, 1943-5452},
  doi = {10.1061/(ASCE)WR.1943-5452.0001573},
  url = {https://ascelibrary.org/doi/10.1061/%28ASCE%29WR.1943-5452.0001573},
  urldate = {2023-11-25}
}

@article{frank_newjersey:2022,
  entrysubtype = {newspaper},
  title = {Bold {{New Jersey Shore Flood Rules Could Be Blueprint}} for {{Entire U}}.{{S}}. {{Coast}}},
  author = {Frank, Thomas},
  date = {2022-08-22},
  journaltitle = {Scientific American},
  url = {https://www.scientificamerican.com/article/bold-new-jersey-shore-flood-rules-could-be-blueprint-for-entire-u-s-coast/},
  urldate = {2023-01-11},
  abstract = {Coastal flood zones where development is restricted will be based on future climate change projections, not past floods}
}

@book{friedman_elements:2001,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  date = {2001},
  volume = {1},
  publisher = {Springer series in statistics Springer, Berlin}
}

@article{garner_slrise:2018,
  title = {Using Direct Policy Search to Identify Robust Strategies in Adapting to Uncertain Sea-Level Rise and Storm Surge},
  author = {Garner, Gregory G. and Keller, Klaus},
  date = {2018-09-01},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  volume = {107},
  pages = {96--104},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2018.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S1364815217307727},
  urldate = {2019-09-17},
  abstract = {Sea-level rise poses considerable risks to coastal communities, ecosystems, and infrastructure. Decision makers are faced with uncertain sea-level projections when designing a strategy for coastal adaptation. The traditional methods are often silent on tradeoffs as well as the effects of tail-area events and of potential future learning. Here we reformulate a simple sea-level rise adaptation model to address these concerns. We show that Direct Policy Search yields improved solution quality, with respect to Pareto-dominance in the objectives, over the traditional approach under uncertain sea-level rise projections and storm surge. Additionally, the new formulation produces high quality solutions with less computational demands than an intertemporal optimization approach. Our results illustrate the utility of multi-objective adaptive formulations for the example of coastal adaptation and point to wider-ranging application in climate change adaptation decision problems.}
}

@book{gelman_bda3:2014,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  date = {2014},
  edition = {3},
  publisher = {Chapman \& Hall/CRC Boca Raton, FL, USA}
}

@article{gelman_philosophy:2013,
  ids = {gelman:2013},
  title = {Philosophy and the Practice of {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  date = {2013},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  volume = {66},
  number = {1},
  eprint = {1006.3868},
  eprinttype = {arXiv},
  pages = {8--38},
  issn = {2044-8317},
  doi = {10/f4k2h4},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x},
  urldate = {2020-04-01},
  abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.}
}

@book{gelman_regression:2021,
  title = {Regression and Other Stories},
  author = {Gelman, Andrew},
  namea = {Hill, Jennifer and Vehtari, Aki},
  nameatype = {collaborator},
  date = {2021},
  series = {Analytical Methods for Social Research},
  publisher = {Cambridge University Press},
  location = {Cambridge, United Kingdom ;},
  abstract = {"Many textbooks on regression focus on theory and the simplest examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is a book about how to use regression to solve real problems of comparison, estimation, prediction, and causal inference. It focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use fresh out of the box"--Back cover},
  isbn = {978-1-107-67651-0}
}

@article{gelman_subjectiveobjective:2017,
  title = {Beyond Subjective and Objective in Statistics},
  author = {Gelman, Andrew and Hennig, Christian},
  date = {2017},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {180},
  number = {4},
  pages = {967--1033},
  issn = {1467-985X},
  doi = {10.1111/rssa.12276},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12276},
  urldate = {2020-07-18},
  abstract = {Decisions in statistical data analysis are often justified, criticized or avoided by using concepts of objectivity and subjectivity. We argue that the words ‘objective’ and ‘subjective’ in statistics discourse are used in a mostly unhelpful way, and we propose to replace each of them with broader collections of attributes, with objectivity replaced by transparency, consensus, impartiality and correspondence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. Together with stability, these make up a collection of virtues that we think is helpful in discussions of statistical foundations and practice. The advantage of these reformulations is that the replacement terms do not oppose each other and that they give more specific guidance about what statistical science strives to achieve. Instead of debating over whether a given statistical method is subjective or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize desirable attributes such as transparency and acknowledgement of multiple perspectives as complementary goals. We demonstrate the implications of our proposal with recent applied examples from pharmacology, election polling and socio-economic stratification. The aim of the paper is to push users and developers of statistical methods towards more effective use of diverse sources of information and more open acknowledgement of assumptions and goals.}
}

@unpublished{gelman_workflow:2020,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
  date = {2020-11-03},
  eprint = {2011.01808},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2011.01808},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2020-11-04},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.}
}

@article{ghil_extremes:2011,
  title = {Extreme Events: Dynamics, Statistics and Prediction},
  author = {Ghil, M and Yiou, P and Hallegatte, S and Malamud, B D and Naveau, P and Soloviev, A and Friederichs, P and Keilis-Borok, V and Kondrashov, D and Kossobokov, V and Mestre, O and Nicolis, C and Rust, H W and Shebalin, P and Vrac, M and Witt, A and Zaliapin, I},
  date = {2011},
  journaltitle = {Nonlinear Processes in Geophysics},
  volume = {18},
  number = {3},
  pages = {295--350},
  publisher = {Copernicus GmbH},
  doi = {10/fvzxvv},
  url = {http://www.nonlin-processes-geophys.net/18/295/2011/},
  abstract = {We review work on extreme events, their causes and consequences, by a group of European and American researchers involved in a three-year project on these topics. The review covers theoretical aspects of time series analysis and of extreme value theory, as well as of the deterministic modeling of extreme events, via continuous and discrete dynamic models. The applications include climatic, seismic and socio-economic events, along with their prediction. {$<$}br{$><$}br{$>$} Two important results refer to (i) the complementarity of spectral analysis of a time series in terms of the continuous and the discrete part of its power spectrum; and (ii) the need for coupled modeling of natural and socio-economic systems. Both these results have implications for the study and prediction of natural hazards and their human impacts.}
}

@article{gidaris_multiplehazard:2017,
  title = {Multiple-{{Hazard Fragility}} and {{Restoration Models}} of {{Highway Bridges}} for {{Regional Risk}} and {{Resilience Assessment}} in the {{United States}}: {{State-of-the-Art Review}}},
  author = {Gidaris, Ioannis and Padgett, Jamie E. and Barbosa, Andre R. and Chen, Suren and Cox, Daniel and Webb, Bret and Cerato, Amy},
  date = {2017-03-01},
  journaltitle = {Journal of Structural Engineering},
  volume = {143},
  number = {3},
  pages = {04016188},
  publisher = {American Society of Civil Engineers},
  issn = {1943-541X},
  doi = {10.1061/(ASCE)ST.1943-541X.0001672},
  url = {https://ascelibrary.org/doi/10.1061/%28ASCE%29ST.1943-541X.0001672},
  urldate = {2024-01-22},
  abstract = {AbstractHighway bridges are one of the most vulnerable constituents of transportation networks when exposed to one or more natural hazards, such as earthquakes, hurricanes, tsunamis, and riverine floods. To facilitate and enhance prehazard and posthazard ...}
}

@article{gilligan_beyondwickedness:2020,
  title = {Beyond Wickedness: Managing Complex Systems and Climate Change},
  author = {Gilligan, Jonathan M. and Vandenbergh, Michael P.},
  date = {2020},
  journaltitle = {Vanderbilt Law Review},
  shortjournal = {Vand. L. Rev.},
  volume = {73},
  pages = {1777--1810},
  url = {https://wp0.vanderbilt.edu/lawreview/2020/12/beyond-wickedness-managing-complex-systems-and-climate-change/},
  urldate = {2020-12-23}
}

@article{greene_nhmm:2011,
  title = {Downscaling Projections of {{Indian}} Monsoon Rainfall Using a Non‐homogeneous Hidden {{Markov}} Model},
  author = {Greene, Arthur M and Robertson, Andrew W and Smyth, Padhraic and Triglia, Scott},
  date = {2011-01},
  journaltitle = {Quart J Royal Meteoro Soc},
  volume = {137},
  number = {655},
  pages = {347--359},
  publisher = {Wiley Online Library},
  doi = {10.1002/qj.788}
}

@article{hausfather_scenarios:2020,
  title = {Emissions – the 'business as Usual' Story Is Misleading},
  author = {Hausfather, Zeke and Peters, Glen P.},
  date = {2020-01},
  journaltitle = {Nature},
  volume = {577},
  number = {7792},
  pages = {618--620},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-020-00177-3},
  url = {http://www.nature.com/articles/d41586-020-00177-3},
  urldate = {2020-04-21},
  abstract = {Stop using the worst-case scenario for climate warming as the most likely outcome — more-realistic baselines make for better policy.},
  issue = {7792}
}

@book{helsel_waterresources:2020,
  title = {Statistical Methods in Water Resources},
  author = {Helsel, Dennis R. and Hirsch, Robert M. and Ryberg, Karen R. and Archfield, Stacey A. and Gilroy, Edward J.},
  date = {2020},
  journaltitle = {Techniques and Methods},
  publisher = {U.S. Geological Survey},
  issn = {2328-7055},
  doi = {10.3133/tm4A3},
  url = {https://pubs.usgs.gov/publication/tm4A3},
  urldate = {2023-08-30},
  abstract = {This text began as a collection of class notes for a course on applied statistical methods for hydrologists taught at the U.S. Geological Survey (USGS) National Training Center. Course material was formalized and organized into a textbook, first published in 1992 by Elsevier as part of their Studies in Environmental Science series. In 2002, the work was made available online as a USGS report.The text has now been updated as a USGS Techniques and Methods Report. It is intended to be a text in applied statistics for hydrology, environmental science, environmental engineering, geology, or biology that addresses distinctive features of environmental data. For example, water resources data tend to have many variables with a lower bound of zero, tend to be more skewed than data from many other disciplines, commonly contain censored data (less than values), and assumptions that the data are normally distributed are not appropriate. Computer-intensive methods (bootstrapping...}
}

@article{herman_control:2020,
  title = {Climate Adaptation as a Control Problem: {{Review}} and Perspectives on Dynamic Water Resources Planning under Uncertainty},
  author = {Herman, Jonathan D. and Quinn, Julianne D. and Steinschneider, Scott and Giuliani, Matteo and Fletcher, Sarah},
  date = {2020-01-07},
  journaltitle = {Water Resources Research},
  pages = {e24389},
  issn = {1944-7973},
  doi = {10.1029/2019wr025502},
  url = {http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019WR025502},
  urldate = {2020-01-11},
  abstract = {Climate change introduces substantial uncertainty to water resources planning, and raises the key question: when, or under what conditions, should adaptation occur? A number of recent studies aim to identify policies mapping future observations to actions—in other words, framing climate adaptation as an optimal control problem. This paper uses the control paradigm to review and classify recent dynamic planning studies according to their approaches to uncertainty characterization, policy structure, and solution methods. We propose a set of research gaps and opportunities in this area centered on the challenge of characterizing uncertainty, which prevents the unambiguous application of control methods to this problem. These include: exogenous uncertainty in forcing, model structure, and parameters propagated through a chain of climate and hydrologic models; endogenous uncertainty in human-environmental system dynamics across multiple scales; and sampling uncertainty due to the finite length of historical observations and future projections. Recognizing these challenges, several opportunities exist to improve the use of control methods for climate adaptation, namely: how problem context and understanding of climate processes might assist with uncertainty quantification and experimental design; out-of-sample validation and robustness of optimized adaptation policies; and monitoring and data assimilation, including trend detection, Bayesian inference, and indicator variable selection. We conclude with a summary of recommendations for dynamic water resources planning under climate change through the lens of optimal control.}
}

@article{herman_robustness:2015,
  title = {How Should Robustness Be Defined for Water Systems Planning under Change?},
  author = {Herman, Jonathan D. and Reed, Patrick M. and Zeff, Harrison B. and Characklis, Gregory W.},
  date = {2015-10-01},
  journaltitle = {Journal of Water Resources Planning and Management},
  shortjournal = {Journal of Water Resources Planning and Management},
  volume = {141},
  number = {10},
  pages = {04015012},
  doi = {10.1061/(asce)wr.1943-5452.0000509},
  url = {http://ascelibrary.org/doi/abs/10.1061/(ASCE)WR.1943-5452.0000509},
  urldate = {2018-12-22},
  abstract = {Water systems planners have long recognized the need for robust solutions capable of withstanding deviations from the conditions for which they were designed. Robustness analyses have shifted from expected utility to exploratory bottom-up approaches which identify vulnerable scenarios prior to assigning likelihoods. Examples include Robust Decision Making (RDM), Decision Scaling, Info-Gap, and Many-Objective Robust Decision Making (MORDM). We propose a taxonomy of robustness frameworks to compare and contrast these approaches based on their methods of (1) alternative generation, (2) sampling of states of the world, (3) quantification of robustness measures, and (4) sensitivity analysis to identify important uncertainties. Building from the proposed taxonomy, we use a regional urban water supply case study in the Research Triangle region of North Carolina to illustrate the decision-relevant consequences that emerge from each of these choices. Results indicate that the methodological choices in the taxonomy lead to the selection of substantially different planning alternatives, underscoring the importance of an informed definition of robustness. Moreover, the results show that some commonly employed methodological choices and definitions of robustness can have undesired consequences when ranking decision alternatives. For the demonstrated test case, recommendations for overcoming these issues include: (1) decision alternatives should be searched rather than prespecified, (2) dominant uncertainties should be discovered through sensitivity analysis rather than assumed, and (3) a carefully elicited multivariate satisficing measure of robustness allows stakeholders to achieve their problem-specific performance requirements. This work emphasizes the importance of an informed problem formulation for systems facing challenging performance tradeoffs and provides a common vocabulary to link the robustness frameworks widely used in the field of water systems planning.}
}

@article{herman_salib:2017,
  title = {{{SALib}}: {{An}} Open-Source {{Python}} Library for {{Sensitivity Analysis}}},
  author = {Herman, Jon and Usher, Will},
  date = {2017-01-10},
  journaltitle = {Journal of Open Source Software},
  volume = {2},
  number = {9},
  pages = {97},
  issn = {2475-9066},
  doi = {10.21105/joss.00097},
  url = {https://joss.theoj.org/papers/10.21105/joss.00097},
  urldate = {2020-04-10},
  abstract = {Herman et al, (2017), SALib: An open-source Python library for Sensitivity Analysis, Journal of Open Source Software, 2(9), 97, doi:10.21105/joss.00097}
}

@article{jafino_distributional:2022,
  title = {Evaluating the Distributional Fairness of Alternative Adaptation Policies: A Case Study in {{Vietnam}}’s Upper {{Mekong Delta}}},
  author = {Jafino, Bramka Arga and Kwakkel, Jan H. and Klijn, Frans},
  date = {2022-08-02},
  journaltitle = {Climatic Change},
  shortjournal = {Climatic Change},
  volume = {173},
  number = {3},
  pages = {17},
  issn = {1573-1480},
  doi = {10.1007/s10584-022-03395-y},
  url = {https://doi.org/10.1007/s10584-022-03395-y},
  urldate = {2024-04-01},
  abstract = {To support equitable adaptation planning, quantitative assessments should consider the fairness of the distribution of outcomes to different people. What constitutes a fair distribution, however, is a normative question. In this study, we explore the use of different moral principles drawn from theories of distributive justice to evaluate fairness. We use adaptation planning in Vietnam Mekong Delta as a case study. We evaluate the preference ranking of six alternative policies for seven moral principles across an ensemble of scenarios. Under the baseline scenario, each principle yields distinctive preference rankings, though most principles identify the same policy as the most preferred one. Across the ensemble of scenarios, the commonly used utilitarian principle yields the most stable ranking, while rankings from other principles are more sensitive to uncertainty. The sufficientarian and the envy-free principles yield the most distinctive ranking of policies, with a median ranking correlation of only 0.07 across all scenarios. Finally, we identify scenarios under which using these two principles results in reversed policy preference rankings. Our study highlights the importance of considering multiple moral principles in evaluating the fairness of adaptation policies, as this would reduce the possibility of maladaptation.}
}

@book{james_statlearn:2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {Springer New York},
  location = {New York, NY},
  isbn = {978-1-4614-7137-0}
}

@book{jaynes_probability:2003,
  title = {Probability Theory: The Logic of Science},
  author = {Jaynes, Edwin T.},
  date = {2003},
  publisher = {Cambridge University Press},
  location = {New York, NY},
  isbn = {978052159271}
}

@article{jongman_exposure:2012,
  title = {Global Exposure to River and Coastal Flooding: Long Term Trends and Changes},
  author = {Jongman, Brenden and Ward, Philip J and Aerts, Jeroen C J H},
  date = {2012-10},
  journaltitle = {Global Environmental Change},
  volume = {22},
  number = {4},
  pages = {823--835},
  doi = {10.1016/j.gloenvcha.2012.07.004},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0959378012000830}
}

@article{kasprzyk_mordm:2013,
  title = {Many Objective Robust Decision Making for Complex Environmental Systems Undergoing Change},
  author = {Kasprzyk, Joseph R. and Nataraj, Shanthi and Reed, Patrick M. and Lempert, Robert J.},
  date = {2013-04-01},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  volume = {42},
  pages = {55--71},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2012.12.007},
  url = {http://www.sciencedirect.com/science/article/pii/S1364815212003131},
  urldate = {2018-12-29},
  abstract = {This paper introduces many objective robust decision making (MORDM). MORDM combines concepts and methods from many objective evolutionary optimization and robust decision making (RDM), along with extensive use of interactive visual analytics, to facilitate the management of complex environmental systems. Many objective evolutionary search is used to generate alternatives for complex planning problems, enabling the discovery of the key tradeoffs among planning objectives. RDM then determines the robustness of planning alternatives to deeply uncertain future conditions and facilitates decision makers' selection of promising candidate solutions. MORDM tests each solution under the ensemble of future extreme states of the world (SOW). Interactive visual analytics are used to explore whether solutions of interest are robust to a wide range of plausible future conditions (i.e., assessment of their Pareto satisficing behavior in alternative SOW). Scenario discovery methods that use statistical data mining algorithms are then used to identify what assumptions and system conditions strongly influence the cost-effectiveness, efficiency, and reliability of the robust alternatives. The framework is demonstrated using a case study that examines a single city's water supply in the Lower Rio Grande Valley (LRGV) in Texas, USA. Results suggest that including robustness as a decision criterion can dramatically change the formulation of complex environmental management problems as well as the negotiated selection of candidate alternatives to implement. MORDM also allows decision makers to characterize the most important vulnerabilities for their systems, which should be the focus of ex post monitoring and identification of triggers for adaptive management.}
}

@article{keller_management:2021,
  title = {Climate Risk Management},
  author = {Keller, Klaus and Helgeson, Casey and Srikrishnan, Vivek},
  date = {2021},
  journaltitle = {Annual Review of Earth and Planetary Sciences},
  volume = {49},
  number = {1},
  pages = {95--116},
  publisher = {Annual Reviews},
  issn = {0084-6597},
  doi = {10.1146/annurev-earth-080320-055847}
}

@online{kingma_adam:2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-30},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2025-03-31},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {prepublished}
}

@article{kopp_evolving:2017,
  title = {Evolving Understanding of {{Antarctic}} Ice-Sheet Physics and Ambiguity in Probabilistic Sea-Level Projections},
  author = {Kopp, Robert E. and DeConto, Robert M. and Bader, Daniel A. and Hay, Carling C. and Horton, Radley M. and Kulp, Scott and Oppenheimer, Michael and Pollard, David and Strauss, Benjamin H.},
  date = {2017},
  journaltitle = {Earth's Future},
  volume = {5},
  number = {12},
  pages = {1217--1233},
  issn = {2328-4277},
  doi = {10.1002/2017ef000663},
  url = {https://doi.org/10.1002/2017ef000663},
  urldate = {2020-02-16},
  abstract = {Mechanisms such as ice-shelf hydrofracturing and ice-cliff collapse may rapidly increase discharge from marine-based ice sheets. Here, we link a probabilistic framework for sea-level projections to a small ensemble of Antarctic ice-sheet (AIS) simulations incorporating these physical processes to explore their influence on global-mean sea-level (GMSL) and relative sea-level (RSL). We compare the new projections to past results using expert assessment and structured expert elicitation about AIS changes. Under high greenhouse gas emissions (Representative Concentration Pathway [RCP] 8.5), median projected 21st century GMSL rise increases from 79 to 146 cm. Without protective measures, revised median RSL projections would by 2100 submerge land currently home to 153 million people, an increase of 44 million. The use of a physical model, rather than simple parameterizations assuming constant acceleration of ice loss, increases forcing sensitivity: overlap between the central 90\% of simulations for 2100 for RCP 8.5 (93–243 cm) and RCP 2.6 (26–98 cm) is minimal. By 2300, the gap between median GMSL estimates for RCP 8.5 and RCP 2.6 reaches {$>$}10 m, with median RSL projections for RCP 8.5 jeopardizing land now occupied by 950 million people (versus 167 million for RCP 2.6). The minimal correlation between the contribution of AIS to GMSL by 2050 and that in 2100 and beyond implies current sea-level observations cannot exclude future extreme outcomes. The sensitivity of post-2050 projections to deeply uncertain physics highlights the need for robust decision and adaptive management frameworks.}
}

@article{kopp_probabilistic:2014,
  title = {Probabilistic 21st and 22nd Century Sea-Level Projections at a Global Network of Tide-Gauge Sites},
  author = {Kopp, Robert E. and Horton, Radley M. and Little, Christopher M. and Mitrovica, Jerry X. and Oppenheimer, Michael and Rasmussen, D. J. and Strauss, Benjamin H. and Tebaldi, Claudia},
  date = {2014},
  journaltitle = {Earth's Future},
  volume = {2},
  number = {8},
  pages = {383--406},
  issn = {2328-4277},
  doi = {10.1002/2014ef000239},
  url = {http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014EF000239},
  urldate = {2020-02-08},
  abstract = {Sea-level rise due to both climate change and non-climatic factors threatens coastal settlements, infrastructure, and ecosystems. Projections of mean global sea-level (GSL) rise provide insufficient information to plan adaptive responses; local decisions require local projections that accommodate different risk tolerances and time frames and that can be linked to storm surge projections. Here we present a global set of local sea-level (LSL) projections to inform decisions on timescales ranging from the coming decades through the 22nd century. We provide complete probability distributions, informed by a combination of expert community assessment, expert elicitation, and process modeling. Between the years 2000 and 2100, we project a very likely (90\% probability) GSL rise of 0.5–1.2 m under representative concentration pathway (RCP) 8.5, 0.4–0.9 m under RCP 4.5, and 0.3–0.8 m under RCP 2.6. Site-to-site differences in LSL projections are due to varying non-climatic background uplift or subsidence, oceanographic effects, and spatially variable responses of the geoid and the lithosphere to shrinking land ice. The Antarctic ice sheet (AIS) constitutes a growing share of variance in GSL and LSL projections. In the global average and at many locations, it is the dominant source of variance in late 21st century projections, though at some sites oceanographic processes contribute the largest share throughout the century. LSL rise dramatically reshapes flood risk, greatly increasing the expected number of “1-in-10” and “1-in-100” year events.}
}

@online{kosmyna_yourbrainonchatgpt:2025,
  title = {Your {{Brain}} on {{ChatGPT}}: {{Accumulation}} of {{Cognitive Debt}} When {{Using}} an {{AI Assistant}} for {{Essay Writing Task}}},
  author = {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye Tong and Situ, Jessica and Liao, Xian-Hao and Beresnitzky, Ashly Vivian and Braunstein, Iris and Maes, Pattie},
  date = {2025-06-10},
  eprint = {2506.08872},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.08872},
  url = {http://arxiv.org/abs/2506.08872},
  urldate = {2025-06-15},
  abstract = {This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.},
  pubstate = {prepublished},
  version = {1}
}

@article{lafferty_downscaling:2023,
  title = {Downscaling and Bias-Correction Contribute Considerable Uncertainty to Local Climate Projections in {{CMIP6}}},
  author = {Lafferty, David C. and Sriver, Ryan L.},
  date = {2023-09-30},
  journaltitle = {npj Climate and Atmospheric Science},
  shortjournal = {npj Clim Atmos Sci},
  volume = {6},
  number = {1},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {2397-3722},
  doi = {10.1038/s41612-023-00486-0},
  url = {https://www.nature.com/articles/s41612-023-00486-0},
  urldate = {2023-10-09},
  abstract = {Efforts to diagnose the risks of a changing climate often rely on downscaled and bias-corrected climate information, making it important to understand the uncertainties and potential biases of this approach. Here, we perform a variance decomposition to partition uncertainty in global climate projections and quantify the relative importance of downscaling and bias-correction. We analyze simple climate metrics such as annual temperature and precipitation averages, as well as several indices of climate extremes. We find that downscaling and bias-correction often contribute substantial uncertainty to local decision-relevant climate outcomes, though our results are strongly heterogeneous across space, time, and climate metrics. Our results can provide guidance to impact modelers and decision-makers regarding the uncertainties associated with downscaling and bias-correction when performing local-scale analyses, as neglecting to account for these uncertainties may risk overconfidence relative to the full range of possible climate futures.},
  issue = {1}
}

@inbook{lall_ncawater:2018,
  title = {Chapter 3: {{Water}}},
  booktitle = {Impacts, {{Risks}}, and {{Adaptation}} in the {{United States}}: {{The Fourth National Climate Assessment}}, {{Volume II}}},
  author = {Lall, Upmanu and Johnson, Thomas and Colohan, Peter and Aghakouchak, Amir and Arumugam, Sankar and Brown, Casey and Mccabe, Gregory J. and Pulwarty, Roger S.},
  date = {2018},
  publisher = {U.S. Global Change Research Program},
  location = {Washington, D.C.},
  doi = {10.7930/NCA4.2018.CH3},
  url = {https://nca2018.globalchange.gov/chapter/3/},
  urldate = {2022-10-30},
  bookauthor = {Reidmiller, David R. and Easterling, David R. and Kunkel, Kenneth E. and Lewis, Kristin L.M. and Maycock, Thomas K. and Stewart, Brooke C.}
}

@article{lamontagne_abatement:2019,
  title = {Robust Abatement Pathways to Tolerable Climate Futures Require Immediate Global Action},
  author = {Lamontagne, J. R. and Reed, P. M. and Marangoni, G. and Keller, K. and Garner, G. G.},
  date = {2019-04},
  journaltitle = {Nature Climate Change},
  shortjournal = {Nat. Clim. Chang.},
  volume = {9},
  number = {4},
  pages = {290--294},
  issn = {1758-6798},
  doi = {10.1038/s41558-019-0426-8},
  url = {http://www.nature.com/articles/s41558-019-0426-8},
  urldate = {2019-09-12},
  abstract = {Uncertainties are often cited as a reason for mitigation inaction. Here, millions of scenarios are evaluated to assess the relative importance of human–earth system uncertainties and policy variables. The growth rate of global abatement is found to be the primary driver of long-term warming.}
}

@article{lanzante_pitfalls:2018,
  title = {Some {{Pitfalls}} in {{Statistical Downscaling}} of {{Future Climate}}},
  author = {Lanzante, John R and Dixon, Keith W and Nath, Mary Jo and Whitlock, Carolyn E and Adams-Smith, Dennis},
  date = {2018-04},
  journaltitle = {Bulletin of the American Meteorological Society},
  volume = {99},
  number = {4},
  pages = {791--803},
  doi = {10.1175/bams-d-17-0046.1},
  url = {http://journals.ametsoc.org/doi/10.1175/BAMS-D-17-0046.1}
}

@book{lauwens_thinkjulia:2019,
  title = {Think {{Julia}}: How to Think like a Computer Scientist},
  author = {Lauwens, Ben and Downey, Allen B},
  date = {2019},
  publisher = {O'Reilly Media},
  url = {https://benlauwens.github.io/ThinkJulia.jl/latest/book.html},
  isbn = {1-4920-4500-4}
}

@article{lee_ercot:2022,
  title = {The {{Impact}} of {{Neglecting Climate Change}} and {{Variability}} on {{ERCOT}}’s {{Forecasts}} of {{Electricity Demand}} in {{Texas}}},
  author = {Lee, Jangho and Dessler, Andrew E.},
  date = {2022-04-01},
  journaltitle = {Weather, Climate, and Society},
  volume = {14},
  number = {2},
  pages = {499--505},
  publisher = {American Meteorological Society},
  issn = {1948-8327, 1948-8335},
  doi = {10.1175/WCAS-D-21-0140.1},
  url = {http://journals.ametsoc.org/view/journals/wcas/14/2/WCAS-D-21-0140.1.xml},
  urldate = {2022-07-03},
  abstract = {Abstract The Electric Reliability Council of Texas (ERCOT) manages the electric power across most of Texas. They make short-term assessments of electricity demand on the basis of historical weather over the last two decades, thereby ignoring the effects of climate change and the possibility of weather variability outside the recent historical range. In this paper, we develop an empirical method to predict the impact of weather on energy demand. We use that with a large ensemble of climate model runs to construct a probability distribution of power demand on the ERCOT grid for summer and winter 2021. We find that the most severe weather events will use 100\% of available power—if anything goes wrong, as it did during the 2021 winter, there will not be sufficient available power. More quantitatively, we estimate a 5\% chance that maximum power demand would be within 4.3 and 7.9 GW of ERCOT’s estimate of best-case available resources during summer and winter 2021, respectively, and a 20\% chance it would be within 7.1 and 17 GW. The shortage of power on the ERCOT grid is partially hidden by the fact that ERCOTs seasonal assessments, which are based entirely on historical weather, are too low. Prior to the 2021 winter blackout, ERCOT forecast an extreme peak load of 67 GW. In reality, we estimate hourly peak demand was 82 GW, 22\% above ERCOT’s most extreme forecast and about equal to the best-case available power. Given the high stakes, ERCOT should develop probabilistic estimates using modern scientific tools to predict the range of power demand more accurately.}
}

@article{lempert_robust:2000,
  title = {Robust Strategies for Abating Climate Change},
  author = {Lempert, Robert J. and Schlesinger, Michael E.},
  date = {2000-06},
  journaltitle = {Climatic Change},
  volume = {45},
  number = {3--4},
  pages = {387--401},
  publisher = {Springer Nature B.V.},
  location = {Dordrecht, Netherlands},
  issn = {01650009},
  doi = {10.1023/A:1005698407365},
  url = {https://www.proquest.com/docview/198492640/abstract/7534863C60614053PQ/1},
  urldate = {2022-09-28},
  abstract = {The need for climate-change decisionmakers to craft strategies that are robust in the face of an unpredictable future is addressed. The climate-change policymaking community could do policymakers a great service by examining signposts that might provide a better basis for building near-term climate-change policies.},
  pagetotal = {387-401}
}

@book{loucks_planning:2017,
  title = {Water Resource Systems Planning and Management: An Introduction to Methods, Models, and Applications},
  author = {Loucks, Daniel P.},
  date = {2017},
  publisher = {Imprint: Springer},
  location = {Cham},
  isbn = {978-3-319-44234-1}
}

@article{lu_spatiotemporal:2025,
  title = {Bayesian Spatiotemporal Nonstationary Model Quantifies Robust Increases in Daily Extreme Rainfall across the {{Western Gulf Coast}}},
  author = {Lu, Yuchen and Seiyon Lee, Benjamin and Doss-Gollin, James},
  date = {2025-08},
  journaltitle = {Environmental Research: Climate},
  shortjournal = {Environ. Res.: Climate},
  volume = {4},
  number = {3},
  pages = {035016},
  publisher = {IOP Publishing},
  issn = {2752-5295},
  doi = {10.1088/2752-5295/adf56e},
  url = {https://dx.doi.org/10.1088/2752-5295/adf56e},
  urldate = {2025-08-23},
  abstract = {Precipitation exceedance probabilities are widely used in engineering design, risk assessment, and floodplain management. While common approaches like National Oceanographic and Atmospheric Administration (NOAA) Atlas 14 assume that extreme precipitation characteristics are stationary over time, this assumption may underestimate current and future hazards due to anthropogenic climate change. However, the incorporation of nonstationarity in statistical modeling of extreme precipitation has faced practical challenges, which have restricted its applications. In particular, random sampling variability challenges the reliable estimation of trends and parameters, especially when observational records are limited. To address this methodological gap, we propose the Spatially Varying Covariates Model, a hierarchical Bayesian spatial framework that integrates nonstationarity and regionalization for robust frequency analysis of extreme precipitation. This model draws from extreme value theory, spatial statistics, and Bayesian statistics, and is validated through cross-validation and multiple performance metrics. Applying this framework to a case study of daily rainfall in the Western Gulf Coast, we identify robustly increasing trends in extreme precipitation intensity and variability throughout the study area, with notable spatial heterogeneity. This flexible model accommodates stations with varying observation records, yields smooth return level estimates, and can be straightforwardly adapted to the analysis of precipitation frequencies at different durations and for other regions.}
}

@book{marchau_decisionmakingunderdeepuncertainty:2019,
  title = {Decision Making under Deep Uncertainty: From Theory to Practice},
  editor = {Marchau, Vincent A. W. J. and Walker, Warren E. and Bloemen, Pieter J. T. M. and Popper, Steven W.},
  date = {2019},
  publisher = {Springer Nature Switzerland AG},
  isbn = {978-3-030-05251-5},
  pagetotal = {314}
}

@incollection{matalas_planning:1977,
  title = {6. {{Water-Resource Systems Planning}}.},
  booktitle = {Climate, {{Climatic Change}}, and {{Water Supply}}},
  author = {Matalas, Nicholas C and Fiering, Myron B},
  date = {1977},
  pages = {99--110},
  publisher = {The National Academies Press},
  location = {Washington, DC},
  url = {https://www.nap.edu/read/185/chapter/11}
}

@book{mcelreath_rethinking2:2020,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  author = {McElreath, Richard},
  date = {2020},
  series = {Texts in Statistical Science Series},
  edition = {Second edition.},
  publisher = {CRC Press, Taylor \& Francis Group},
  location = {Boca Raton ;},
  isbn = {978-0-429-63914-2}
}

@article{mcphail_robustness:2019,
  title = {Robustness Metrics: How Are They Calculated, When Should They Be Used and Why Do They Give Different Results?},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  date = {2019-04-02},
  journaltitle = {Earth's Future},
  shortjournal = {Earth's Future},
  pages = {169--191},
  issn = {2328-4277},
  doi = {10.1002/2017ef000649},
  url = {http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017EF000649%4010.1002/%28ISSN%292328-4277.RESDEC1},
  urldate = {2019-11-20},
  abstract = {Robustness is being used increasingly for decision analysis in relation to deep uncertainty and many metrics have been proposed for its quantification. Recent studies have shown that the application of different robustness metrics can result in different rankings of decision alternatives, but there has been little discussion of what potential causes for this might be. To shed some light on this issue, we present a unifying framework for the calculation of robustness metrics, which assists with understanding how robustness metrics work, when they should be used, and why they sometimes disagree. The framework categorizes the suitability of metrics to a decision-maker based on (1) the decision-context (i.e., the suitability of using absolute performance or regret), (2) the decision-maker's preferred level of risk aversion, and (3) the decision-maker's preference toward maximizing performance, minimizing variance, or some higher-order moment. This article also introduces a conceptual framework describing when relative robustness values of decision alternatives obtained using different metrics are likely to agree and disagree. This is used as a measure of how ?stable? the ranking of decision alternatives is when determined using different robustness metrics. The framework is tested on three case studies, including water supply augmentation in Adelaide, Australia, the operation of a multipurpose regulated lake in Italy, and flood protection for a hypothetical river based on a reach of the river Rhine in the Netherlands. The proposed conceptual framework is confirmed by the case study results, providing insight into the reasons for disagreements between rankings obtained using different robustness metrics.}
}

@article{merz_heavytails:2022,
  title = {Understanding Heavy Tails of Flood Peak Distributions},
  author = {Merz, Bruno and Basso, Stefano and Fischer, Svenja and Lun, David and Blöschl, Günter and Merz, Ralf and Guse, Björn and Viglione, Alberto and Vorogushyn, Sergiy and Macdonald, Elena and Wietzke, Luzie and Schumann, Andreas},
  date = {2022},
  journaltitle = {Water Resources Research},
  volume = {58},
  number = {6},
  pages = {e2021WR030506},
  issn = {1944-7973},
  doi = {10.1029/2021WR030506},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1029/2021WR030506},
  urldate = {2022-06-29},
  abstract = {Statistical distributions of flood peak discharge often show heavy tail behavior, that is, extreme floods are more likely to occur than would be predicted by commonly used distributions that have exponential asymptotic behavior. This heavy tail behavior may surprise flood managers and citizens, as human intuition tends to expect light tail behavior, and the heaviness of the tails is very difficult to predict, which may lead to unnecessarily high flood damage. Despite its high importance, the literature on the heavy tail behavior of flood distributions is rather fragmented. In this review, we provide a coherent overview of the processes causing heavy flood tails and the implications for science and practice. Specifically, we propose nine hypotheses on the mechanisms causing heavy tails in flood peak distributions related to processes in the atmosphere, the catchment, and the river system. We then discuss to which extent the current knowledge supports or contradicts these hypotheses. We also discuss the statistical conditions for the emergence of heavy tail behavior based on derived distribution theory and relate them to the hypotheses and flood generation mechanisms. We review the degree to which the heaviness of the tails can be predicted from process knowledge and data. Finally, we recommend further research toward testing the hypotheses and improving the prediction of heavy tails.}
}

@article{merz_review:2014,
  title = {Floods and Climate: Emerging Perspectives for Flood Risk Assessment and Management},
  author = {Merz, Bruno and Aerts, Jeroen C J H and Arnbjerg-Nielsen, Karsten and Baldi, M and Becker, A and Bichet, A and Blöschl, Günter and Bouwer, Laurens M and Brauer, Achim and Cioffi, F and Delgado, J M and Gocht, M and Guzzetti, F and Harrigan, S and Hirschboeck, K and Kilsby, C and Kron, W and Kwon, H H and Lall, Upmanu and Merz, R and Nissen, K and Salvatti, P and Swierczynski, Tina and Ulbrich, U and Viglione, A and Ward, P J and Weiler, M and Wilhelm, B and Nied, M},
  date = {2014},
  journaltitle = {Natural Hazards and Earth System Science},
  volume = {14},
  number = {7},
  pages = {1921--1942},
  doi = {10/gb9nzm},
  url = {http://www.nat-hazards-earth-syst-sci.net/14/1921/2014/},
  abstract = {Flood estimation and flood management have traditionally been the domain of hydrologists, water resources engineers and statisticians, and disciplinary approaches abound. Dominant views have been shaped; one example is the catchment perspective: floods are formed and influenced by the interaction of local, catchment-specific characteristics, such as meteorology, topography and geology. These traditional views have been beneficial, but they have a narrow framing. In this paper we contrast traditional views with broader perspectives that are emerging from an improved understanding of the climatic context of floods. We come to the following conclusions: (1) extending the traditional system boundaries (local catchment, recent decades, hydrological/hydraulic processes) opens up exciting possibilities for better understanding and improved tools for flood risk assessment and management. (2) Statistical approaches in flood estimation need to be complemented by the search for the causal mechanisms and dominant processes in the atmosphere, catchment and river system that leave their fingerprints on flood characteristics. (3) Natural climate variability leads to time-varying flood characteristics, and this variation may be partially quantifiable and predictable, with the perspective of dynamic, climate-informed flood risk management. (4) Efforts are needed to fully account for factors that contribute to changes in all three risk components (hazard, exposure, vulnerability) and to better understand the interactions between society and floods. (5) Given the global scale and societal importance, we call for the organization of an international multidisciplinary collaboration and data-sharing initiative to further understand the links between climate and flooding and to advance flood research.}
}

@book{mignan_textbook:2024,
  title = {Introduction to {{Catastrophe Risk Modelling}}: {{A Physics-based Approach}}},
  author = {Mignan, Arnaud},
  date = {2024},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/9781009437370},
  url = {https://www.cambridge.org/core/product/A3A5B5FB990921422BFEBB07734BF869},
  abstract = {Focusing on the physics of the~catastrophe process and addressed directly to advanced students, this innovative textbook quantifies dozens of perils, both natural and man-made, and covers the latest developments in catastrophe modelling. Combining basic statistics, applied physics, natural and environmental sciences, civil engineering, and psychology, the text remains at an introductory level, focusing on fundamental concepts for a comprehensive understanding of catastrophe phenomenology and risk quantification. A broad spectrum of perils are covered, including geophysical, hydrological, meteorological, climatological, biological, extraterrestrial, technological and socio-economic, as well as events caused by domino effects and global warming. Following industry standards, the text provides the necessary tools to develop a CAT model from hazard to loss assessment. Online resources include a~CAT risk model starter-kit and a CAT risk modelling 'sandbox' with Python Jupyter tutorial.~Every process, described by equations, (pseudo)codes and illustrations, is fully reproducible, allowing students to solidify knowledge through practice.},
  isbn = {978-1-009-43738-7}
}

@article{milly_stationary:2008,
  title = {Stationarity Is Dead: Whither Water Management?},
  author = {Milly, P C D and Betancourt, Julio and Falkenmark, M and Hirsch, R M and Kundzewicz, Z W and Lettenmaier, D P and Stouffer, R J},
  date = {2008-02},
  journaltitle = {Science},
  volume = {319},
  number = {5863},
  pages = {573--574},
  doi = {10.1126/science.1151915},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1151915},
  abstract = {Climate change undermines a basic assumption that historically has facilitated management of water supplies, demands, and risks.}
}

@incollection{mudelsee_statistical:2020,
  title = {Statistical Analysis of Climate Extremes / {{Manfred Mudelsee}}.},
  booktitle = {Statistical Analysis of Climate Extremes},
  author = {Mudelsee, Manfred},
  date = {2020/2020},
  publisher = {Cambridge University Press},
  location = {Cambridge, United Kingdom ;},
  abstract = {"The big climate question is how climate change affects climate extremes. More hurricanes such as Katrina in 2005? More floods such as that of European river Elbe in 2002? More heatwaves such as in 2003 or 2018? Where to invest resources? All this is not just scientifically challenging. It is also relevant for society and economy to survive. This is the first textbook on statistics and climate extremes. It explains the statistical methods in an accessible language. It provides the necessary software. Case studies on extremes in the three major climate variables (temperature, precipitation and wind speed) show how to use the methods. The book provides the datasets to allow replication of case study calculations. This book is written for students and researchers in climate sciences. It can serve as textbook in university courses. Also risk analysts in the insurance industry benefit from it"– Provided by publisher.},
  isbn = {978-1-108-79146-5},
  lccn = {2019040870}
}

@book{naghettini_stathydro:2017,
  title = {Fundamentals of {{Statistical Hydrology}}},
  editor = {Naghettini, Mauro},
  date = {2017},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-43561-9},
  url = {http://link.springer.com/10.1007/978-3-319-43561-9},
  urldate = {2025-04-28},
  isbn = {978-3-319-43560-2 978-3-319-43561-9}
}

@book{nazarathy_julia:2021,
  title = {Statistics with {{Julia}}: {{Fundamentals}} for {{Data Science}}, {{Machine Learning}} and {{Artificial Intelligence}}},
  author = {Nazarathy, Yoni and Klok, Hayden},
  date = {2021},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-70901-3},
  url = {https://www.springer.com/gp/book/9783030709006},
  urldate = {2021-05-14},
  abstract = {This monograph uses the Julia language to guide the reader through an exploration of the fundamental concepts of probability and statistics, all with a view of mastering machine learning, data science, and artificial intelligence. The text does not require any prior statistical knowledge and only assumes a basic understanding of programming and mathematical notation. It is accessible to practitioners and researchers in data science, machine learning, bio-statistics, finance, or engineering who may wish to solidify their knowledge of probability and statistics. The book progresses through ten independent chapters starting with an introduction of Julia, and moving through basic probability, distributions, statistical inference, regression analysis, machine learning methods, and the use of Monte Carlo simulation for dynamic stochastic models. Ultimately this text introduces the Julia programming language as a computational tool, uniquely addressing end-users rather than developers. It makes heavy use of over 200 code examples to illustrate dozens of key statistical concepts. The Julia code, written in a simple format with parameters that can be easily modified, is also available for download from the book’s associated GitHub repository online.See what co-creators of the Julia language are saying about the book:Professor Alan Edelman, MIT: With “Statistics with Julia”, Yoni and Hayden have written an easy to read, well organized, modern introduction to statistics. The code may be looked at, and understood on the static pages of a book, or even better, when running live on a computer. Everything you need is here in one nicely written self-contained reference. Dr. Viral Shah, CEO of Julia Computing: Yoni and Hayden provide a modern way to learn statistics with the Julia programming language. This book has been perfected through iteration over several semesters in the classroom. It prepares the reader with two complementary skills - statistical reasoning with hands on experience and working with large datasets through training in Julia.},
  isbn = {978-3-030-70900-6}
}

@article{oddo_coastal:2017,
  title = {Deep Uncertainties in Sea-Level Rise and Storm Surge Projections: Implications for Coastal Flood Risk Management},
  author = {Oddo, Perry C. and Lee, Ben S. and Garner, Gregory G. and Srikrishnan, Vivek and Reed, Patrick M. and Forest, Chris E. and Keller, Klaus},
  date = {2017},
  journaltitle = {Risk Analysis},
  volume = {0},
  number = {0},
  issn = {1539-6924},
  doi = {10/ghkp82},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12888},
  urldate = {2019-09-11},
  abstract = {Sea levels are rising in many areas around the world, posing risks to coastal communities and infrastructures. Strategies for managing these flood risks present decision challenges that require a combination of geophysical, economic, and infrastructure models. Previous studies have broken important new ground on the considerable tensions between the costs of upgrading infrastructure and the damages that could result from extreme flood events. However, many risk-based adaptation strategies remain silent on certain potentially important uncertainties, as well as the tradeoffs between competing objectives. Here, we implement and improve on a classic decision-analytical model (Van Dantzig 1956) to: (i) capture tradeoffs across conflicting stakeholder objectives, (ii) demonstrate the consequences of structural uncertainties in the sea-level rise and storm surge models, and (iii) identify the parametric uncertainties that most strongly influence each objective using global sensitivity analysis. We find that the flood adaptation model produces potentially myopic solutions when formulated using traditional mean-centric decision theory. Moving from a single-objective problem formulation to one with multiobjective tradeoffs dramatically expands the decision space, and highlights the need for compromise solutions to address stakeholder preferences. We find deep structural uncertainties that have large effects on the model outcome, with the storm surge parameters accounting for the greatest impacts. Global sensitivity analysis effectively identifies important parameter interactions that local methods overlook, and that could have critical implications for flood adaptation strategies.}
}

@article{oreskes_verification:1994,
  title = {Verification, Validation, and Confirmation of Numerical Models in the {{Earth}} Sciences},
  author = {Oreskes, Naomi and Shrader-Frechette, Kristin and Belitz, Kenneth},
  date = {1994-02-04},
  journaltitle = {Science},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.263.5147.641},
  url = {http://www.science.org/doi/abs/10.1126/science.263.5147.641},
  urldate = {2021-09-02},
  abstract = {Verification and validation of numerical models of natural systems is impossible. This is because natural systems are never closed and because model results are always nonunique. Models can be confirmed by the demonstration of agreement between ...}
}

@report{pblnetherlands_waterchallenges:2019,
  title = {The Geography of Future Water Challenges},
  author = {{PBL Netherlands Environmental Assessment Agency}},
  date = {2019},
  institution = {PBL Netherlands Environmental Assessment Agency},
  location = {The Hague},
  url = {https://www.pbl.nl/en/publications/the-geography-of-future-water-challenges},
  shortauthor = {PBL Netherlands}
}

@article{piironen_comparison:2017,
  title = {Comparison of {{Bayesian}} Predictive Methods for Model Selection},
  author = {Piironen, Juho and Vehtari, Aki},
  date = {2017-05},
  journaltitle = {Statistics and Computing},
  volume = {27},
  number = {3},
  pages = {711--735},
  doi = {10.1007/s11222-016-9649-y},
  url = {http://link.springer.com/10.1007/s11222-016-9649-y},
  abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model. Published in: Statistics and Computing, 2017, Volume 27, Issue 3, 711-735}
}

@article{powell_textbook:2022,
  title = {Sequential {{Decision Analytics}} and {{Modeling}}: {{Modeling}} with {{Python}}},
  author = {Powell, Warren B.},
  date = {2022-11-21},
  publisher = {Now Publishers, Inc.},
  issn = {978-1-63828-082-8, 978-1-63828-083-5},
  url = {https://nowpublishers.com/Article/BookDetails/9781638280828},
  urldate = {2025-03-29},
  abstract = {Publishers of Foundations and Trends, making research accessible}
}

@online{price_correctorgan:2022,
  title = {Increasing the Accuracy and Resolution of Precipitation Forecasts Using Deep Generative Models},
  author = {Price, Ilan and Rasp, Stephan},
  date = {2022-03-23},
  eprint = {2203.12297},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2203.12297},
  url = {http://arxiv.org/abs/2203.12297},
  urldate = {2022-07-03},
  abstract = {Accurately forecasting extreme rainfall is notoriously difficult, but is also ever more crucial for society as climate change increases the frequency of such extremes. Global numerical weather prediction models often fail to capture extremes, and are produced at too low a resolution to be actionable, while regional, high-resolution models are hugely expensive both in computation and labour. In this paper we explore the use of deep generative models to simultaneously correct and downscale (super-resolve) global ensemble forecasts over the Continental US. Specifically, using fine-grained radar observations as our ground truth, we train a conditional Generative Adversarial Network -- coined CorrectorGAN -- via a custom training procedure and augmented loss function, to produce ensembles of high-resolution, bias-corrected forecasts based on coarse, global precipitation forecasts in addition to other relevant meteorological fields. Our model outperforms an interpolation baseline, as well as super-resolution-only and CNN-based univariate methods, and approaches the performance of an operational regional high-resolution model across an array of established probabilistic metrics. Crucially, CorrectorGAN, once trained, produces predictions in seconds on a single machine. These results raise exciting questions about the necessity of regional models, and whether data-driven downscaling and correction methods can be transferred to data-poor regions that so far have had no access to high-resolution forecasts.},
  pubstate = {prepublished}
}

@book{pyrcz_textbook:2024,
  title = {Applied {{Machine Learning}} in {{Python}}: A {{Hands-on Guide}} with {{Code}}},
  author = {Pyrcz, Michael J.},
  date = {2024},
  url = {https://geostatsguy.github.io/MachineLearningDemos_Book}
}

@article{quinn_dps:2017,
  title = {Direct Policy Search for Robust Multi-Objective Management of Deeply Uncertain Socio-Ecological Tipping Points},
  author = {Quinn, Julianne D. and Reed, Patrick M. and Keller, Klaus},
  date = {2017-06-01},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  volume = {92},
  pages = {125--141},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2017.02.017},
  url = {http://www.sciencedirect.com/science/article/pii/S1364815216302250},
  urldate = {2019-09-27},
  abstract = {Managing socio-ecological systems is a challenge wrought by competing societal objectives, deep uncertainties, and potentially irreversible tipping points. A classic, didactic example is the shallow lake problem in which a hypothetical town situated on a lake must develop pollution control strategies to maximize its economic benefits while minimizing the probability of the lake crossing a critical phosphorus (P) threshold, above which it irreversibly transitions into a eutrophic state. Here, we explore the use of direct policy search (DPS) to design robust pollution control rules for the town that account for deeply uncertain system characteristics and conflicting objectives. The closed loop control formulation of DPS improves the quality and robustness of key management tradeoffs, while dramatically reducing the computational complexity of solving the multi-objective pollution control problem relative to open loop control strategies. These insights suggest DPS is a promising tool for managing socio-ecological systems with deeply uncertain tipping points.}
}

@online{rackauckas_sciml:2020,
  title = {Universal {{Differential Equations}} for {{Scientific Machine Learning}}},
  author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  date = {2020},
  doi = {10.48550/ARXIV.2001.04385},
  url = {https://arxiv.org/abs/2001.04385},
  urldate = {2024-09-26},
  abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
  pubstate = {prepublished},
  version = {4}
}

@article{razavi_sensitivity:2020,
  title = {The Future of Sensitivity Analysis: An Essential Discipline for Systems Modeling and Policy Support},
  author = {Razavi, Saman and Jakeman, Anthony and Saltelli, Andrea and Prieur, Clémentine and Iooss, Bertrand and Borgonovo, Emanuele and Plischke, Elmar and Lo Piano, Samuele and Iwanaga, Takuya and Becker, William and Tarantola, Stefano and Guillaume, Joseph H. A. and Jakeman, John and Gupta, Hoshin and Melillo, Nicola and Rabitti, Giovanni and Chabridon, Vincent and Duan, Qingyun and Sun, Xifu and Smith, Stefán and Sheikholeslami, Razi and Hosseini, Nasim and Asadzadeh, Masoud and Puy, Arnald and Kucherenko, Sergei and Maier, Holger R.},
  date = {2020-12-15},
  journaltitle = {Environmental Modelling \& Software},
  shortjournal = {Environmental Modelling \& Software},
  pages = {104954},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2020.104954},
  url = {http://www.sciencedirect.com/science/article/pii/S1364815220310112},
  urldate = {2020-12-20},
  abstract = {Sensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society.}
}

@article{reed_multiobjective:2013,
  title = {Evolutionary Multiobjective Optimization in Water Resources: {{The}} Past, Present, and Future},
  author = {Reed, P. M. and Hadka, D. and Herman, J. D. and Kasprzyk, J. R. and Kollat, J. B.},
  date = {2013-01-01},
  journaltitle = {Advances in Water Resources},
  shortjournal = {Advances in Water Resources},
  series = {35th {{Year Anniversary Issue}}},
  volume = {51},
  pages = {438--456},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2012.01.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0309170812000073},
  urldate = {2019-03-12},
  abstract = {This study contributes a rigorous diagnostic assessment of state-of-the-art multiobjective evolutionary algorithms (MOEAs) and highlights key advances that the water resources field can exploit to better discover the critical tradeoffs constraining our systems. This study provides the most comprehensive diagnostic assessment of MOEAs for water resources to date, exploiting more than 100,000 MOEA runs and trillions of design evaluations. The diagnostic assessment measures the effectiveness, efficiency, reliability, and controllability of ten benchmark MOEAs for a representative suite of water resources applications addressing rainfall–runoff calibration, long-term groundwater monitoring (LTM), and risk-based water supply portfolio planning. The suite of problems encompasses a range of challenging problem properties including (1) many-objective formulations with four or more objectives, (2) multi-modality (or false optima), (3) nonlinearity, (4) discreteness, (5) severe constraints, (6) stochastic objectives, and (7) non-separability (also called epistasis). The applications are representative of the dominant problem classes that have shaped the history of MOEAs in water resources and that will be dominant foci in the future. Recommendations are given for the new algorithms that should serve as the benchmarks for innovations in the water resources literature. The future of MOEAs in water resources needs to emphasize self-adaptive search, new technologies for visualizing tradeoffs, and the next generation of computing technologies.}
}

@article{reed_multisectordynamics:2022,
  title = {Multisector {{Dynamics}}: {{Advancing}} the {{Science}} of {{Complex Adaptive Human-Earth Systems}}},
  author = {Reed, Patrick M. and Hadjimichael, Antonia and Moss, Richard H. and Brelsford, Christa and Burleyson, Casey D. and Cohen, Stuart and Dyreson, Ana and Gold, David F. and Gupta, Rohini S. and Keller, Klaus and Konar, Megan and Monier, Erwan and Morris, Jennifer and Srikrishnan, Vivek and Voisin, Nathalie and Yoon, Jim},
  date = {2022},
  journaltitle = {Earth's Future},
  volume = {10},
  number = {3},
  pages = {e2021EF002621},
  issn = {2328-4277},
  doi = {10.1029/2021EF002621},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1029/2021EF002621},
  urldate = {2022-03-03},
  abstract = {The field of MultiSector Dynamics (MSD) explores the dynamics and co-evolutionary pathways of human and Earth systems with a focus on critical goods, services, and amenities delivered to people through interdependent sectors. This commentary lays out core definitions and concepts, identifies MSD science questions in the context of the current state of knowledge, and describes ongoing activities to expand capacities for open science, leverage revolutions in data and computing, and grow and diversify the MSD workforce. Central to our vision is the ambition of advancing the next generation of complex adaptive human-Earth systems science to better address interconnected risks, increase resilience, and improve sustainability. This will require convergent research and the integration of ideas and methods from multiple disciplines. Understanding the tradeoffs, synergies, and complexities that exist in coupled human-Earth systems is particularly important in the context of energy transitions and increased future shocks.}
}

@article{salas_review:2018,
  title = {Techniques for Assessing Water Infrastructure for Nonstationary Extreme Events: A Review},
  author = {Salas, J D and Obeysekera, J and Vogel, R M},
  date = {2018},
  journaltitle = {Hydrological Sciences Journal},
  volume = {63},
  number = {3},
  pages = {325--352},
  doi = {10.1080/02626667.2018.1426858},
  url = {https://www.tandfonline.com/doi/full/10.1080/02626667.2018.1426858}
}

@book{saltelli_global:2008,
  title = {Global Sensitivity Analysis: The Primer},
  author = {Saltelli, Andrea and Ratto, Marco and Andres, Terry and Campolongo, Francesca and Cariboni, Jessica and Gatelli, Debora and Saisana, Michaela and Tarantola, Stefano},
  date = {2008},
  publisher = {John Wiley \& Sons, Ltd},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/9780470725184.ch1},
  urldate = {2020-04-17},
  isbn = {978-0-470-72518-4}
}

@article{sanchez-lengeling_:2021,
  title = {A {{Gentle Introduction}} to {{Graph Neural Networks}}},
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  date = {2021-09-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {6},
  number = {9},
  pages = {e33},
  issn = {2476-0757},
  doi = {10.23915/distill.00033},
  url = {https://distill.pub/2021/gnn-intro},
  urldate = {2025-07-07},
  abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?}
}

@article{satija_boomtown:2016,
  entrysubtype = {newspaper},
  title = {Boomtown, Flood Town},
  author = {Satija, Nina and Collier, Kiah and Shaw, Al},
  date = {2016-12-07},
  journaltitle = {ProPublica},
  location = {Houston, TX},
  url = {https://projects.propublica.org/houston-cypress/},
  urldate = {2021-05-31},
  abstract = {Climate change will bring more frequent and fierce rainstorms to cities like Houston. But unchecked development remains a priority in the famously un-zoned city, creating short-term economic gains for some while increasing flood risks for everyone.},
  journalsubtitle = {Hell and High Water}
}

@book{savage_foundations:1954,
  title = {Foundations of Statistics.},
  author = {Savage, L.J.},
  date = {1954},
  publisher = {Wiley},
  location = {New York}
}

@article{schlef_idf:2023,
  title = {Incorporating Non-Stationarity from Climate Change into Rainfall Frequency and Intensity-Duration-Frequency ({{IDF}}) Curves},
  author = {Schlef, Katherine E. and Kunkel, Kenneth E. and Brown, Casey and Demissie, Yonas and Lettenmaier, Dennis P. and Wagner, Anna and Wigmosta, Mark S. and Karl, Thomas R. and Easterling, David R. and Wang, Kimberly J. and François, Baptiste and Yan, Eugene},
  date = {2023-01-01},
  journaltitle = {Journal of Hydrology},
  shortjournal = {Journal of Hydrology},
  volume = {616},
  pages = {128757},
  issn = {0022-1694},
  doi = {10.1016/j.jhydrol.2022.128757},
  url = {https://www.sciencedirect.com/science/article/pii/S0022169422013270},
  urldate = {2023-01-26},
  abstract = {Intensity-duration-frequency (IDF) curves – sometimes also called precipitation frequency estimates – are used to design urban drainage systems for stormwater control and, when combined with hydrologic modeling, to design flood control infrastructure and other structures. However, common approaches to estimating IDF curves need to be revised to account for non-stationarity from global climate change. This review synthesizes the current knowledge and on-going research regarding IDF curves under non-stationarity. In particular, this review briefly summarizes known and projected changes in extreme precipitation at the global scale, describes approaches for IDF curve estimation under non-stationarity (focusing on the covariate-based approach and including a brief overview of the specific challenges facing snow-dominated regions), addresses the topic of regionalization under non-stationarity (which has been overlooked in previous reviews), explores the challenges of design values and uncertainty in the context of non-stationarity, provides details on these topics in the context of the United States, and finishes by enumerating needed avenues of future research.}
}

@article{schneider_scenarios:2002,
  title = {Can We Estimate the Likelihood of Climatic Changes at 2100?},
  author = {Schneider, Stephen H.},
  date = {2002-03},
  journaltitle = {Climatic Change},
  volume = {52},
  number = {4},
  pages = {441--451},
  publisher = {Springer Nature B.V.},
  location = {Dordrecht, Netherlands},
  issn = {01650009},
  doi = {http://dx.doi.org/10.1023/A:1014276210717},
  url = {https://www.proquest.com/docview/198493931/abstract/7F4C45C74324C3EPQ/1},
  urldate = {2021-10-26},
  abstract = {Schneider points out some of the consequences of the deliberate choice not to discuss the possibilities of CO2 emissions and how that can--and has--led to confusion on the part of the media and policy makers over the likelihood of potential dangerous anthropogenic climate change in the next century.},
  pagetotal = {441-451}
}

@article{schwetschenau_optimizing:2023,
  title = {Optimizing {{Scale}} for {{Decentralized Wastewater Treatment}}: {{A Tool}} to {{Address Failing Wastewater Infrastructure}} in the {{United States}}},
  author = {Schwetschenau, Sara E. and Kovankaya, Yunus and Elliott, Mark A. and Allaire, Maura and White, Kevin D. and Lall, Upmanu},
  date = {2023-01-13},
  journaltitle = {ACS ES\&T Engineering},
  shortjournal = {ACS EST Eng.},
  volume = {3},
  number = {1},
  pages = {1--14},
  publisher = {American Chemical Society},
  doi = {10.1021/acsestengg.2c00188},
  url = {https://doi.org/10.1021/acsestengg.2c00188},
  urldate = {2023-09-19},
  abstract = {Wastewater systems (sewered or on-site septic tanks) are failing across the U.S. Economically disadvantaged populations, communities of color, tribal lands, and rural/peri-urban areas are especially vulnerable. Efficient deployment of public and private capital to assure appropriate service levels and affordability is a critical need. We present a modeling framework to identify economically optimal wastewater network layout and component sizes. Six system configurations are considered, which include gravity versus pressurized collection system flow and treatment of septic effluent versus raw wastewater. A case study for Uniontown, Alabama─a community that has been in national news for the failure of their wastewater infrastructure─is presented. We find that a decentralized network that separates and stores solids on-site, prior to conveyance to a cluster-scale treatment site, has a fraction of the capital and operating cost of a centralized system that is currently proposed. Broader implications are discussed.}
}

@article{seigerman_operationalizing:2023,
  title = {Operationalizing Equity for Integrated Water Resources Management},
  author = {Seigerman, Cydney K. and McKay, S. Kyle and Basilio, Raul and Biesel, Shelly A. and Hallemeier, Jon and Mansur, Andressa V. and Piercy, Candice and Rowan, Sebastian and Ubiali, Bruno and Yeates, Elissa and Nelson, Donald R.},
  date = {2023},
  journaltitle = {JAWRA Journal of the American Water Resources Association},
  volume = {59},
  number = {2},
  pages = {281--298},
  issn = {1752-1688},
  doi = {10.1111/1752-1688.13086},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1752-1688.13086},
  urldate = {2024-04-01},
  abstract = {Advancing social equity has been implicitly and explicitly central to water resources policy for decades. Yet, equity remains largely outside of standard water resources planning and management practices. Inclusion of equity within water resources infrastructure is inhibited by barriers including an incomplete conceptual understanding of equity, a perceived lack of quantitative and qualitative equity metrics, unclear connections between equity and standard project planning frameworks, and the absence of concrete examples. To facilitate greater practical inclusion of social equity in water resources practices, we describe equity relative to dimensions of distribution, procedure, and recognition and identify metrics associated with each. We then map these dimensions of equity onto different stages of a water resources project life cycle. We discuss how inequities are often perpetuated by current approaches and highlight case studies that promote one or more of the facets of equity. Rather than providing a prescriptive solution to “achieve” equity within water resources practices, we emphasize the need for contextualized approaches that include pragmatic steps toward more equitable practices and outcomes.}
}

@incollection{seneviratne_IPCC:2021,
  type = {Book section},
  title = {Weather and Climate Extreme Events in a Changing Climate},
  booktitle = {Climate Change 2021: {{The}} Physical Science Basis. {{Contribution}} of Working Group {{I}} to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change},
  author = {Seneviratne, S.I. and Zhang, X. and Adnan, M. and Badi, W. and Dereczynski, C. and Di Luca, A. and Ghosh, S. and Iskandar, I. and Kossin, J. and Lewis, S. and Otto, F. and Pinto, I. and Satoh, M. and Vicente-Serrano, S.M. and Wehner, M. and Zhou, B.},
  editor = {Masson-Delmotte, V. and Zhai, P. and Pirani, A. and Connors, S. L. and Péan, C. and Berger, S. and Caud, N. and Chen, Y. and Goldfarb, L. and Gomis, M. I. and Huang, M. and Leitzell, K. and Lonnoy, E. and Matthews, J. B. R. and Maycock, T. K. and Waterfield, T. and Yelekçi, O. and Yu, R. and Zhou, B.},
  date = {2021},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK and New York, NY, USA},
  doi = {10.1017/9781009157896.013},
  url = {https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter11.pdf}
}

@article{smith_pareto:2022,
  title = {Multiobjective Optimization and {{Pareto}} Front Visualization Techniques Applied to Normal Conducting Rf Accelerating Structures},
  author = {Smith, S. and Southerby, M. and Setiniyaz, S. and Apsimon, R. and Burt, G.},
  date = {2022-06-14},
  journaltitle = {Physical Review Accelerators and Beams},
  shortjournal = {Phys. Rev. Accel. Beams},
  volume = {25},
  number = {6},
  pages = {062002},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevAccelBeams.25.062002},
  url = {https://link.aps.org/doi/10.1103/PhysRevAccelBeams.25.062002},
  urldate = {2024-02-26},
  abstract = {There has been a renewed interest in applying multiobjective (MO) optimization methods to a number of problems in the physical sciences, including to rf structure design. The results of these optimizations generate large datasets, which makes visualizing the data and selecting individual solutions difficult. Using the generated results, Pareto fronts can be found giving the trade-off between different objectives, allowing one to utilize this key information in design decisions. Although various visualization techniques exist, it can be difficult to know which technique is appropriate and how to apply them successfully to the problem at hand. First, we present the setup and execution of MO optimizations of one standing wave and one traveling wave accelerating cavity, including constraint handling and an algorithm comparison. In order to understand the generated Pareto frontiers, we discuss several visualization techniques, applying them to the problem, and give the benefits and drawbacks of each. We found that the best techniques involve clustering the resulting data first to narrow down the possible choices and then using multidimensional visualization methods such as parallel coordinate plots and decision maps to view the clustered results and select individual solutions. Finally, we give some examples of the application of these methods and the cavities selected based on arbitrary design requirements.}
}

@article{sobel_biases:2023,
  title = {Near-Term Tropical Cyclone Risk and Coupled {{Earth}} System Model Biases},
  author = {Sobel, Adam H. and Lee, Chia-Ying and Bowen, Steven G. and Camargo, Suzana J. and Cane, Mark A. and Clement, Amy and Fosu, Boniface and Hart, Megan and Reed, Kevin A. and Seager, Richard and Tippett, Michael K.},
  date = {2023-08-15},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {33},
  pages = {e2209631120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2209631120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2209631120},
  urldate = {2023-08-07},
  abstract = {Most current climate models predict that the equatorial Pacific will evolve under greenhouse gas–induced warming to a more El Niño-like state over the next several decades, with a reduced zonal sea surface temperature gradient and weakened atmospheric Walker circulation. Yet, observations over the last 50 y show the opposite trend, toward a more La Niña-like state. Recent research provides evidence that the discrepancy cannot be dismissed as due to internal variability but rather that the models are incorrectly simulating the equatorial Pacific response to greenhouse gas warming. This implies that projections of regional tropical cyclone activity may be incorrect as well, perhaps even in the direction of change, in ways that can be understood by analogy to historical El Niño and La Niña events: North Pacific tropical cyclone projections will be too active, North Atlantic ones not active enough, for example. Other perils, including severe convective storms and droughts, will also be projected erroneously. While it can be argued that these errors are transient, such that the models’ responses to greenhouse gases may be correct in equilibrium, the transient response is relevant for climate adaptation in the next several decades. Given the urgency of understanding regional patterns of climate risk in the near term, it would be desirable to develop projections that represent a broader range of possible future tropical Pacific warming scenarios—including some in which recent historical trends continue—even if such projections cannot currently be produced using existing coupled earth system models.}
}

@article{sommer_rainfall:2022,
  entrysubtype = {newspaper},
  title = {An Unexpected Item Is Blocking Cities' Climate Change Prep: Obsolete Rainfall Records},
  author = {Sommer, Lauren},
  date = {2022-02-09},
  journaltitle = {NPR},
  url = {https://www.npr.org/2022/02/09/1078261183/an-unexpected-item-is-blocking-cities-climate-change-prep-obsolete-rainfall-reco},
  urldate = {2024-01-05},
  abstract = {Cities are experiencing heavier storms and flooding as the climate gets hotter. But due to outdated rainfall records, many are still building infrastructure for the climate of the past.},
  journalsubtitle = {Climate}
}

@article{steinschneider_expanded:2015,
  ids = {Steinschneider:2015kk},
  title = {Expanded Decision-Scaling Framework to Select Robust Long-Term Water-System Plans under Hydroclimatic Uncertainties},
  author = {Steinschneider, Scott and McCrary, Rachel and Wi, Sungwook and Mulligan, Kevin and Mearns, Linda O and Brown, Casey M},
  date = {2015-11},
  journaltitle = {Journal of Water Resources Planning and Management},
  volume = {141},
  number = {11},
  doi = {10.1061/(asce)wr.1943-5452.0000536},
  url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29WR.1943-5452.0000536}
}

@book{sutton_reinforcement:2018,
  title = {Reinforcement Learning: An {{Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  date = {2018},
  edition = {Second Edition},
  publisher = {MIT Press},
  location = {Cambridge, Massachusetts and London, England},
  isbn = {0-262-03924-9}
}

@report{sweet_slr:2022,
  type = {NOAA Technical Report},
  title = {Global and Regional Sea Level Rise Scenarios for the {{United States}}},
  author = {Sweet, W.V. and Hamlington, B.D. and Kopp, R.E. and Weaver, C.P. and Barnard, P.L. and Bekaert, D. and Brooks, W. and Craghan, M. and Dusek, G.},
  date = {2022},
  number = {NOS 01},
  pages = {111},
  institution = {{National Oceanic and Atmospheric Administration, National Ocean Service}},
  location = {Silver Spring, MD},
  url = {https://oceanservice.noaa.gov/hazards/sealevelrise/sealevelrise-tech-report-sections.html}
}

@article{tedesco_exposure:2020,
  title = {Exposure of Real Estate Properties to the 2018 {{Hurricane Florence}} Flooding},
  author = {Tedesco, Marco and McAlpine, Steven and Porter, Jeremy R.},
  date = {2020-04-01},
  journaltitle = {Natural Hazards and Earth System Sciences},
  volume = {20},
  number = {3},
  pages = {907--920},
  publisher = {Copernicus GmbH},
  issn = {1561-8633},
  doi = {10.5194/nhess-20-907-2020},
  url = {https://www.nat-hazards-earth-syst-sci.net/20/907/2020/},
  urldate = {2020-04-11},
  abstract = {Quantifying the potential exposure of property to damages associated with storm surges, extreme weather and hurricanes is fundamental to developing frameworks that can be used to conceive and implement mitigation plans as well as support urban development that accounts for such events. In this study, we aim at quantifying the total value and area of properties exposed to the flooding associated with Hurricane Florence that occurred in September 2018. To this aim, we implement an approach for the identification of affected areas by generating a map of the maximum flood extent obtained from a combination of the flood extent produced by the Federal Emergency Management Agency's (FEMA's) water marks with those obtained from spaceborne radar remote-sensing data. The use of radar in the creation of the flood extent allows for those properties commonly missed by FEMA's interpolation methods, especially from pluvial or non-fluvial sources, and can be used in more accurately estimating the exposure and market value of properties to event-specific flooding. Lastly, we study and quantify how the urban development over the past decades in the regions flooded by Hurricane Florence might have impacted the exposure of properties to present-day storms and floods. This approach is conceptually similar to what experts are addressing as the “expanding bull's eye effect”, in which “targets” of geophysical hazards, such as people and their built environments, enlarge as populations grow and spread. Our results indicate that the total value of property exposed to flooding during Hurricane Florence was USD\&thinsp;52 billion (in 2018 USD), with this value increasing from USD\&thinsp;∼10 billion at the beginning of the past century to the final amount based on the expansion of the number of properties exposed. We also found that, despite the decrease in the number of properties built during the decade before Florence, much of the new construction was in proximity to permanent water bodies, hence increasing exposure to flooding. Ultimately, the results of this paper provide a new tool for shedding light on the relationships between urban development in coastal areas and the flooding of those areas, which is estimated to increase in view of projected increasing sea level rise, storm surges and the strength of storms.}
}

@article{thomson_systemic:2023,
  ids = {thomson_:2022,thomson_financial:2021},
  title = {Systemic {{Financial Risk Arising From Residential Flood Losses}}},
  author = {Thomson, Hope and Zeff, Harrison B. and Kleiman, Rachel and Sebastian, Antonia and Characklis, Gregory W.},
  date = {2023},
  journaltitle = {Earth's Future},
  volume = {11},
  number = {4},
  pages = {e2022EF003206},
  issn = {2328-4277},
  doi = {10.1029/2022EF003206},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022EF003206},
  urldate = {2023-07-07},
  abstract = {Direct damage from flooding at residential properties has typically been categorized as insured, with liabilities accruing to insurers, or uninsured, with costs accruing to property owners. However, residential flooding can also expose lenders and local governments to financial risk, though the distribution of this risk is not well understood. Flood losses are not limited to direct damages, but also include indirect effects such as decreases in property values, which can be substantial, though are rarely well quantified. The combination of direct damage and property value decrease influences rates of mortgage default and property abandonment in the wake of a flood, creating financial risk. In this research, property-level data on sales, mortgages, and insurance claims are used in combination with machine learning techniques and geostatistical methods to provide estimates of flood losses that are then utilized to evaluate the risk of default and abandonment in eastern North Carolina following Hurricane Florence (2018). Within the study area, Hurricane Florence generated \textbackslash 366M in observed insured damages and an estimated \textbackslash 1.77B in combined uninsured damages and property value decreases. Property owners, lenders, and local governments were exposed to an additional \$562M in potential losses due to increased rates of default and abandonment. Areas with lower pre-flood property values were exposed to greater risk than areas with higher valued properties. Results suggest more highly resolved estimates of a flooding event's systemic financial risk may be useful in developing improved flood resilience strategies.}
}

@book{thuerey_pbdl:2024,
  title = {Physics-Based Deep Learning},
  author = {Thuerey, N. and Holzschuh, B. and Holl, P. and Kohl, G. and Lino, M. and Liu, Q. and Schnell, P. and Trost, F.},
  date = {2024},
  url = {https://physicsbaseddeeplearning.org}
}

@book{tye_infrastructure_climate:2021,
  title = {Impacts of Future Weather and Climate Extremes on {{United States}} Infrastructure: Assessing and Prioritizing Adaptation Actions},
  author = {Tye, Mari R. and Giovannettone, Jason P.},
  namea = {AghaKouchak, Amir and Beighley, R. Edward and Capehart, William J. and Fehrenbacher, Noah J. and Fields, Robert E. and Huang, Joshua and Kaatz, Laurna and Lin, Ning and Llewellyn, Dagmar and MacClune, Karen and Olsen, J. Rolf and Pinson, Ariane O. and Shi, Ting and Vahedifard, Farshid},
  nameatype = {collaborator},
  date = {2021-10-13},
  publisher = {American Society of Civil Engineers},
  location = {Reston, VA},
  doi = {10.1061/9780784415863},
  url = {https://ascelibrary.org/doi/book/10.1061/9780784415863},
  urldate = {2022-11-14},
  isbn = {978-0-7844-1586-3 978-0-7844-8372-5}
}

@article{vanberchum_coastal:2019,
  title = {Evaluation of Flood Risk Reduction Strategies through Combinations of Interventions},
  author = {family=Berchum, given=Erik C., prefix=van, useprefix=true and Mobley, William and Jonkman, Sebastiaan N. and Timmermans, Jos S. and Kwakkel, Jan H. and Brody, Samuel D.},
  date = {2019},
  journaltitle = {Journal of Flood Risk Management},
  volume = {12},
  number = {S2},
  pages = {e12506},
  issn = {1753-318X},
  doi = {10.1111/jfr3.12506},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jfr3.12506},
  urldate = {2023-01-17},
  abstract = {Large, complex coastal regions often require a combination of interventions to lower the risk of flooding to an acceptable level. In practice, a limited number of strategies are considered and interdependencies between interventions are often simplified. This paper presents the Multiple Lines of Defence Optimization System (MODOS)-model. This quick, probabilistic model simulates and evaluates the impact of many flood risk reduction strategies while accounting for interdependencies amongst measures. The simulation includes hydraulic calculations, damage calculations, and the effects of measures for various return periods. The application and potential of this model is shown with a conceptual and simplified case study, based on the Houston-Galveston Bay area. The analyses demonstrate how the MODOS-model identifies trade-offs within the system and shows how flood risk, cost, and impact respond to flood management decisions. This improved understanding of the impact of design and planning choices can benefit the discussions in finding the optimal flood risk reduction strategy for coastal regions.}
}

@article{vandantzig_dike:1956,
  title = {Economic {{Decision Problems}} for {{Flood Prevention}}},
  author = {family=Dantzig, given=D., prefix=van, useprefix=true},
  date = {1956},
  journaltitle = {Econometrica},
  volume = {24},
  number = {3},
  pages = {276--287},
  issn = {0012-9682},
  doi = {10.2307/1911632},
  url = {https://doi.org/10.2307/1911632},
  urldate = {2019-09-12}
}

@incollection{walker_deep:2013,
  title = {Deep {{Uncertainty}}},
  booktitle = {Encyclopedia of {{Operations Research}} and {{Management Science}}},
  author = {Walker, Warren E. and Lempert, Robert J. and Kwakkel, Jan H.},
  editor = {Gass, Saul I. and Fu, Michael C.},
  date = {2013},
  pages = {395--402},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4419-1153-7_1140},
  url = {https://doi.org/10.1007/978-1-4419-1153-7_1140},
  urldate = {2019-10-01},
  isbn = {978-1-4419-1153-7}
}

@article{weitzman_stern:2007,
  title = {A Review of the {{Stern}} Review on the Economics of Climate Change},
  author = {Weitzman, Martin L.},
  date = {2007-09},
  journaltitle = {Journal of Economic Literature},
  volume = {45},
  number = {3},
  pages = {703--724},
  issn = {0022-0515},
  doi = {10.1257/jel.45.3.703},
  url = {https://www.aeaweb.org/articles?id=10.1257/jel.45.3.703},
  urldate = {2019-09-21}
}

@article{wing_vulnerability:2020,
  title = {New Insights into {{US}} Flood Vulnerability Revealed from Flood Insurance Big Data},
  author = {Wing, Oliver E. J. and Pinter, Nicholas and Bates, Paul D. and Kousky, Carolyn},
  date = {2020-03-19},
  journaltitle = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {1444},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-15264-2},
  url = {https://www.nature.com/articles/s41467-020-15264-2},
  urldate = {2021-02-16},
  abstract = {Improvements in modelling power and input data have vastly improved the precision of physical flood models, but translation into economic outputs requires depth–damage functions that are inadequately verified. In particular, flood damage is widely assumed to increase monotonically with water depth. Here, we assess flood vulnerability in the US using {$>$}2 million claims from the National Flood Insurance Program (NFIP). NFIP claims data are messy, but the size of the dataset provides powerful empirical tests of damage patterns and modelling approaches. We show that current depth–damage functions consist of disparate relationships that match poorly with observations. Observed flood losses are not monotonic functions of depth, but instead better follow a beta function, with bimodal distributions for different water depths. Uncertainty in flood losses has been called the main bottleneck in flood risk studies, an obstacle that may be remedied using large-scale empirical flood damage data.},
  issue = {1}
}

@article{wutich_madwater:2023,
  title = {{{MAD}} Water: {{Integrating}} Modular, Adaptive, and Decentralized Approaches for Water Security in the Climate Change Era},
  author = {Wutich, Amber and Thomson, Patrick and Jepson, Wendy and Stoler, Justin and Cooperman, Alicia D. and Doss-Gollin, James and Jantrania, Anish and Mayer, Alex and Nelson-Nuñez, Jami and Walker, W. Shane and Westerhoff, Paul},
  date = {2023-07-12},
  journaltitle = {WIREs Water},
  volume = {n/a},
  number = {n/a},
  pages = {e1680},
  issn = {2049-1948},
  doi = {10.1002/wat2.1680},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1680},
  urldate = {2023-07-12},
  abstract = {Centralized water infrastructure has, over the last century, brought safe and reliable drinking water to much of the world. But climate change, combined with aging and underfunded infrastructure, is increasingly testing the limits of—and reversing gains made by—this approach. To address these growing strains and gaps, we must assess and advance alternatives to centralized water provision and sanitation. The water literature is rife with examples of systems that are neither centralized nor networked, yet meet water needs of local communities in important ways, including: informal and hybrid water systems, decentralized water provision, community-based water management, small drinking water systems, point-of-use treatment, small-scale water vendors, and packaged water. Our work builds on these literatures by proposing a convergence approach that can integrate and explore the benefits and challenges of modular, adaptive, and decentralized (“MAD”) water provision and sanitation, often foregrounding important advances in engineering technology. We further provide frameworks to evaluate justice, economic feasibility, governance, human health, and environmental sustainability as key parameters of MAD water system performance.}
}

@article{zarekarizi_suboptimal:2020,
  title = {Neglecting Uncertainties Biases House-Elevation Decisions to Manage Riverine Flood Risks},
  author = {Zarekarizi, Mahkameh and Srikrishnan, Vivek and Keller, Klaus},
  date = {2020-10-26},
  journaltitle = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5361},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19188-9},
  url = {http://www.nature.com/articles/s41467-020-19188-9},
  urldate = {2020-10-26},
  abstract = {Homeowners around the world elevate houses to manage flood risks. Deciding how high to elevate a house poses a nontrivial decision problem. The U.S. Federal Emergency Management Agency (FEMA) recommends elevating existing houses to the Base Flood Elevation (the elevation of the 100-year flood) plus a freeboard. This recommendation neglects many uncertainties. Here we analyze a case-study of riverine flood risk management using a multi-objective robust decision-making framework in the face of deep uncertainties. While the quantitative results are location-specific, the approach and overall insights are generalizable. We find strong interactions between the economic, engineering, and Earth science uncertainties, illustrating the need for expanding on previous integrated analyses to further understand the nature and strength of these connections. Considering deep uncertainties surrounding flood hazards, the discount rate, the house lifetime, and the fragility can increase the economically optimal house elevation to values well above FEMA’s recommendation.},
  issue = {1}
}
