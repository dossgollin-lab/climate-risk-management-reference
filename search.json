[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climate Risk Assessment and Management",
    "section": "",
    "text": "Welcome üéØ\nWelcome to Climate Risk Assessment and Management, an online textbook under construction by James Doss-Gollin.",
    "crumbs": [
      "Welcome üéØ"
    ]
  },
  {
    "objectID": "index.html#motivation-and-scope",
    "href": "index.html#motivation-and-scope",
    "title": "Climate Risk Assessment and Management",
    "section": "Motivation and Scope",
    "text": "Motivation and Scope\n\nHistory\nThis project emerged from two courses taught at Rice University by James Doss-Gollin: CEVE 543 focused on climate hazard and extremes and CEVE 421/521 focused on risk management.\n\n\nAim\nThe book is motivated by questions like\n\nWhat is the probability distribution of wind speeds that a building structure might experience?\nWhat will the probability distribution of extreme rainfall be in 2050, and what drives uncertainty in this estimate?\nWhat is the probability distribution of tropical cyclone losses across a regional portfolio?\nWhen, and how high, should a house be elevated to proactively manage future flood risk?\nWhat are robust, efficient, and equitable strategies for reducing flood risk in an urban area?\n\nThese questions span scales and sectors, yet they share fundamental challenges: characterizing extreme events, quantifying uncertainty, assessing risks, and making robust decisions when probability distributions are unknown or contested. Moreover, there is not a single correct answer to these questions, or a single method that will incontrovertibly answer them.",
    "crumbs": [
      "Welcome üéØ"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "Climate Risk Assessment and Management",
    "section": "How to Use This Resource",
    "text": "How to Use This Resource\nThe book is designed to be useful for practitioners, students, and teachers. Teachers may use individual chapters in their courses. Students may use it as a class text or reference. Practitioners may focus on specific chapters relevant to their work. Each chapter includes learning objectives and can be read independently, though some chapters build on concepts introduced in others.\n\nStructure\n\nThe Preface introduces the book‚Äôs motivation and frames key challenges\nPart 1 introduces key topics in probability, inference, Bayesian methods, optimization, machine learning, and Earth science. Rather than providing a comprehensive treatment, this part focuses on essential concepts and links to further resources.\nPart 2 focuses on hazard assessment, namely modeling climate hazards and extremes. Material is organized around thematic applications and predictive tasks. The foundational idea is integrating information from noisy and/or biased sources to estimate the joint probability distribution of relevant hydroclimatic variables.\nPart 3 risk management, which involves both mapping hazard to risk and designing interventions to manage these risks. Key ideas include the sequential nature of decisions, the pursuit of unclear and/or contested objectives, and the need to account for the sensitivity of estimated probability distributions (of hazard and of other relevant physical, social, and economic variables) to underlying models and assumptions.\nComputational notebooks written in Julia illustrate and complement the methods and concepts discussed in the text. While notebooks are referenced in the text, they are designed as standalone and self-contained resources.\n\n\n\nPrerequisites\nBasic probability and multivariate calculus, along with linear algebra, are sufficient mathematical foundations for this textbook. Some exposure to Earth science, hydrology, water resources, or related topics is strongly encouraged for context, though not strictly necessary for understanding methods. This book builds on a wide range of topics and methods in statistics, machine learning, optimization, and Earth science, and expertise in any of these areas may deepen your understanding, but is not necessary. No programming is required to read the book, but going through computational examples and applying methods to your own problems, which can substantially strengthen your understanding, does require programming.",
    "crumbs": [
      "Welcome üéØ"
    ]
  },
  {
    "objectID": "chapters/about/license.html",
    "href": "chapters/about/license.html",
    "title": "License",
    "section": "",
    "text": "This textbook is licensed under the CC BY-NC 4.0 License. It is free to use, share, and adapt for non-commercial purposes, provided that you give appropriate credit, provide a link to the license, and indicate if changes were made. If you would like to use this content for commercial purposes, please contact me.",
    "crumbs": [
      "**About this book**",
      "License"
    ]
  },
  {
    "objectID": "chapters/about/contributing.html",
    "href": "chapters/about/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "This textbook is a work in progress, and we welcome your contributions. Whether it‚Äôs fixing a typo or proposing a new module, every suggestion helps. The easiest way to contribute is to fork the repository and submit a pull request. If you‚Äôre not comfortable with that workflow, please open an issue on GitHub.",
    "crumbs": [
      "**About this book**",
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/about/citing.html",
    "href": "chapters/about/citing.html",
    "title": "Citing",
    "section": "",
    "text": "Please cite this resource as\n@book{doss-gollin_textbook:2025,\n  author = {Doss-Gollin, James},\n  title = {Climate Risk Management},\n  year = {2025},\n  url = {https://jdossgollin.github.io/climate-risk-book},\n}\nIn the future, we will move to stable releases with numbered versions.",
    "crumbs": [
      "**About this book**",
      "Citing"
    ]
  },
  {
    "objectID": "chapters/about/resources.html",
    "href": "chapters/about/resources.html",
    "title": "Further Reading",
    "section": "",
    "text": "Inspiration\nClimate risk assessment and management are complex and interdisciplinary topics, and we are by no means comprehensive here. This page provides some helpful resources (textbooks, detailed online tutorials, and class websites) for your continued and supplementary study.\nThis textbook draws inspiration and content from several courses and lecture notes, and I am grateful to the instructors who have shared their materials with me.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#inspiration",
    "href": "chapters/about/resources.html#inspiration",
    "title": "Further Reading",
    "section": "",
    "text": "Upmanu Lall‚Äôs Environmental Data Analysis course at Columbia\nVivek Srikrishnan‚Äôs Environmental Systems Analysis and Climate Risk Analysis classes at Cornell\nR. Balaji‚Äôs Advanced Data Analysis Techniques (Statistical Learning Techniques for Engineering and Science) course at CU Boulder\nAlberto Montanari‚Äôs collection of open course notes and lectures\nApplegate and Keller (2015) motivates this project and demonstrates problem-based learning.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#stats-ml-basics",
    "href": "chapters/about/resources.html#stats-ml-basics",
    "title": "Further Reading",
    "section": "Stats + ML basics",
    "text": "Stats + ML basics\nThis book assumes familiarity with these topics, but these resources may be helpful as a refresher.\n\nBlitzstein and Hwang (2019) provides a thorough introduction to key concepts and ideas in probability. The book accompanies a free online course, Stat 110, which is a great resource for learning probability and statistics. Practice problems and solutions, handouts, and lecture videos are all available online.\nDowney (2021) offers an introduction to Bayesian statistics using computational methods. It‚Äôs not environment focused but provides code and a clear explanation of core concepts.\nGelman (2021) is a textbook designed for a first course on applied statistics. Clear and well-worked examples underpin discussion of fundamental ideas in statistical analysis and thinking about data.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#applications",
    "href": "chapters/about/resources.html#applications",
    "title": "Further Reading",
    "section": "Applications",
    "text": "Applications\nThere are lots of related books on catastrophe modeling, water resources research, geostats, statistical hydrology and related topics. Here is an incomplete list of some core references.\n\nNaghettini (2017) is a textbook on statistical hydrology that covers many of the same topics as this course. The statistical hydrology literature often obfuscates key ideas with complex notation and terminology, but this book is a helpful introduction to the field.\nHelsel et al. (2020) is a comprehensive introduction to water resources and hydrology, focusing on statistical methods for analyzing hydrologic data. Its methods are traditional, with less emphasis on machine learning or Bayesian methods and more attention to null hypothesis significance testing, but its case studies are well-worked and thoughtfully described.\nAbernathey (2024) is an excellent resource covering introductory topics in Earth and climate data science using Python, with an emphasis on foundational computations. These core computational concepts serves as a recommended prerequisite for more advanced material in this book.\nPyrcz (2024) is a textbook focused on applied machine learning, with a particular focus on geostatistics. There‚Äôs less focus on extremes, hydroclimate, and decision-making, but it provides very clear and interpretable explanations of many machine learning methods, including some that are not directly covered in this book.\nMignan (2024) is a modern introduction to catastrophe risk modeling that covers a wide range of hazards, including hydroclimatic extremes, from a physics-based perspective. It provides a structured framework for quantifying hazard, exposure, and vulnerability, following industry-standard CAT modeling approaches. While broader in scope and more introductory in level, it complements this book‚Äôs focus by illustrating foundational principles of probabilistic risk modeling in practice.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#more-stats-ml",
    "href": "chapters/about/resources.html#more-stats-ml",
    "title": "Further Reading",
    "section": "More Stats + ML",
    "text": "More Stats + ML\nThis book covers a broad set of topics in statistics, machine learning, and optimization. Most chapters could be a textbook of their own, and in fact many exist.\n\nFriedman, Hastie, and Tibshirani (2001) is a classic introduction to machine learning, which complements the Bayesian perspective nicely.\nJaynes (2003) is a classic text on probability theory that you should read if you‚Äôre interested in questions like ‚Äúwhat is probability?‚Äù\nGelman et al. (2014) and McElreath (2020) are the classic textbooks on Bayesian inference and provide a wealth of insight and detail. The Gelman textbook is a bit more dense while the McElreath book has a more conversational tone, but both cover similar topics.\nCressie and Wikle (2011) provides a detailed exploration of hierarchical space-time models. There have been some computational advances since then that are worth keeping in mind before you apply these models directly, but it‚Äôs a clearly written and overview.\nThuerey et al. (2024) is a new textbook on physics-based deep learning, which is a rapidly growing area of research. It provides a comprehensive overview of the field, including theoretical foundations and practical applications. It covers topics, including neural operators and diffusion models, that are not covered in this course, but which are increasingly used in the climate risk space.\nBishop and Bishop (2024) is a comprehensive, modern, and accessible start-to-finish textbook covering machine learning from basic probability through diffusion models.\nMichael Betancourt‚Äôs writing page has detailed and mathematically rigorous explanations of many topics in Bayesian data analysis and probabilistic modeling.\n\n\n\n\n\nAbernathey, Ryan. 2024. An Introduction to Earth and Environmental Data Science. https://earth-env-data-science.github.io/intro.html.\n\n\nApplegate, Patrick, and Klaus Keller. 2015. Risk Analysis in the Earth Sciences. Leanpub. https://leanpub.next/raes.\n\n\nBishop, Christopher M., and Hugh Bishop. 2024. Deep Learning: Foundations and Concepts. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-45468-4.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability, Second Edition. 2nd Edition. Boca Raton: Chapman and Hall/CRC. http://probabilitybook.net.\n\n\nCressie, Noel A. C., and Christopher K. Wikle. 2011. Statistics for Spatio-Temporal Data. Hoboken, N.J.: Wiley.\n\n\nDowney, Allen B. 2021. Think Bayes. \"O‚ÄôReilly Media, Inc.\". https://allendowney.github.io/ThinkBayes2/.\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. Springer series in statistics Springer, Berlin.\n\n\nGelman, Andrew. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge, United Kingdom ; Cambridge University Press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 2014. Bayesian Data Analysis. 3rd ed. Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nHelsel, Dennis R., Robert M. Hirsch, Karen R. Ryberg, Stacey A. Archfield, and Edward J. Gilroy. 2020. Statistical Methods in Water Resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. New York, NY: Cambridge University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Texts in Statistical Science Series. Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nMignan, Arnaud. 2024. Introduction to Catastrophe Risk Modelling: A Physics-based Approach. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781009437370.\n\n\nNaghettini, Mauro, ed. 2017. Fundamentals of Statistical Hydrology. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-43561-9.\n\n\nPyrcz, Michael J. 2024. Applied Machine Learning in Python: A Hands-on Guide with Code. https://geostatsguy.github.io/MachineLearningDemos_Book.\n\n\nThuerey, N., B. Holzschuh, P. Holl, G. Kohl, M. Lino, Q. Liu, P. Schnell, and F. Trost. 2024. Physics-Based Deep Learning. https://physicsbaseddeeplearning.org.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/preface.html",
    "href": "chapters/preface.html",
    "title": "Preface",
    "section": "",
    "text": "What is climate risk?\nClimate risks arise at the intersection of climate hazards, exposed systems, and vulnerability. They manifest when extreme or changing climate conditions‚Äîfloods, droughts, extreme temperatures, sea-level rise, or shifting precipitation patterns‚Äîimpact human and natural systems that are exposed and vulnerable to these conditions. The financial sector terms these ‚Äúphysical risks‚Äù to distinguish them from transition risks related to policy and market changes.\nClimate risks span scales from the hyperlocal (a single building‚Äôs flood exposure) to the global (climate impacts on agricultural productivity). They encompass immediate acute risks from individual extreme events and longer-term chronic risks from gradual climate changes. Crucially, climate risks are not solely natural phenomena but emerge from the complex interactions between climate hazards and the human systems‚Äîinfrastructure, institutions, communities, and economies‚Äîthat experience their impacts.\nClimate risk is often defined as the product of hazard (probability that something will happen) and consequences (exposure and vulnerability). However, it‚Äôs often helpful to start with the decisions we care about.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#what-is-climate-risk",
    "href": "chapters/preface.html#what-is-climate-risk",
    "title": "Preface",
    "section": "",
    "text": "Risk management\nThe goal of assessing climate risks is to manage them, as is the focus of Part III. We manage climate risks by\n\nbuilding infrastructure, such as seawalls, stormwater pipes, oyster beds, green roofs, dams\ndesigning policy, such as water pricing, land-use regulations, building codes\nresponding to climate disasters through disaster response and recovery. While emergency management is beyond the scope of the book, disaster prevention (through infrastructure, policy, etc) and preparation (planning evacuation routes, assessing resource needs, etc) are problems that the tools of this class can inform.\n\nA key insight from considering these applications is that climate risks are not natural phenomena, but occur at the intersection of natural and human systems. A second insight is that decisions about how to manage climate risks do not depend only on climate hazard, but also on human systems and values.\n\n\nExposure and vulnerability\nHazards do not create consequences by themselves. Hazards affect things that we care about, whether natural ecosystems, human homes, infrastructure systems, or something else. Quantitatively these are often described as exposure and vulnerability. However, this is not always a helpful framing because everything is exposed, to at least some degree, to climate hazards.\n\n\nClimate hazard\nClimate hazards have several key characteristics:\n\nLocation-specific impacts: Specific weather patterns cause different things in different places‚Äîtropical cyclones cause extreme winds on the Gulf Coast, while persistent intense rainfall causes flooding in major rivers\nRequire Earth science and data: Understanding hazards requires both physical process knowledge and empirical data\nVariable focus on extremes: Some applications care about extremes, but others (e.g., water management) care about shifts in the whole distribution\nMulti-scale variability: Characterized by variability across multiple spatial and temporal scales",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#what-are-good-strategies",
    "href": "chapters/preface.html#what-are-good-strategies",
    "title": "Preface",
    "section": "What are good strategies?",
    "text": "What are good strategies?\n\nThe simple story\nIn principle, managing climate risks should be straightforward. If we had clear objectives and well-characterized uncertainty, there are established mathematical formalisms for decision-making under uncertainty. Notably, Bayesian Decision Theory provides an elegant framework: find the action \\(a\\) that maximizes expected utility \\[\n\\mathbb{E}[U(a)] = \\int U(a, s) p(s) ds,\n\\] where \\(U(a, s)\\) is the utility of action \\(a\\) given state of the world \\(s\\), and \\(p(s)\\) is the joint probability distribution over states of the world. The expectation \\(\\mathbb{E}[U(a)]\\) represents the average utility we would expect from action \\(a\\) across all possible future states, weighted by their probabilities (see Chapter on Probability and Statistics for mathematical foundations).\nWith this framework and modern advances in operations research and optimization, we could frame climate risk management as a large-scale optimization problem. This might still be a challenging problem, requiring sophisticated optimization methods, large-ensemble Monte Carlo simulation, high-performance computing, and more, but fundamentally there would be a right answer that we could identify, at least seek to approximate.\n\n\nWhy this isn‚Äôt enough\nIn practice, climate risk management defies this idealized approach for several fundamental reasons:\n\nDeep uncertainty: Unlike textbook optimization problems, we rarely have well-defined probability distributions over future states. Climate risks involve poorly characterized, multiple, and interacting uncertainties spanning physical processes (climate projections), socioeconomic factors (development patterns, institutional capacity, human behavior), and their complex dependencies. The probability distributions we need span climate hazards, exposure patterns, vulnerability functions, and policy effectiveness‚Äîall evolving in ways that resist precise characterization.\nLarge and poorly defined decision spaces: The solution space includes not just individual projects but entire systems: infrastructure networks, policy portfolios, risk transfer arrangements, and adaptive management sequences. These decisions interact across scales, sectors, and time horizons in ways that resist comprehensive optimization.\nContested objectives: Different stakeholders hold different values about what we should optimize for‚Äîeconomic efficiency, equity, robustness, or flexibility. These objectives often conflict, and their relative importance is itself contested and evolving.\n\nThis brings us to a crucial insight: we cannot simply frame climate risk management as a big optimization problem. The field has witnessed an explosion of computational tools‚Äîclimate models with ever-finer resolution, machine learning algorithms for processing vast datasets, and sophisticated visualization platforms for rendering complex projections. While these advances represent genuine progress, their proliferation has created new challenges for practitioners seeking to manage real-world climate risks.\nThe abundance of available tools does not automatically translate to better decisions. Indeed, the sophistication of modern computational approaches can obscure fundamental questions about problem framing, uncertainty characterization, and appropriate methods selection. Without solid conceptual foundations, practitioners may find themselves applying powerful tools inappropriately or mistaking methodological novelty for substantive insight.\n\n\nThe stakes of getting it wrong\nThe consequences of inadequate climate risk management are severe and diverse. Infrastructure failures occur when designs based on historical extremes prove insufficient for future conditions‚Äîleading to flooded neighborhoods when storm drains are undersized, or to costly over-design when extreme projections are treated as certainties. Policy mistakes compound these problems: development policies that ignore flood risks concentrate vulnerable populations in harm‚Äôs way, while overly conservative regulations can stifle economic development without commensurate risk reduction benefits.\nFinancial miscalculations affect both public and private sectors. Insurance companies that underestimate climate risks face catastrophic losses, while those that overestimate risks price themselves out of markets. Infrastructure investors struggle to balance climate resilience against cost constraints, often erring toward solutions that prove either inadequate or prohibitively expensive. These failures cascade across scales: a poorly designed local drainage system contributes to regional flood management challenges, while flawed national climate risk assessments misguide infrastructure investment priorities across entire countries.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#this-book",
    "href": "chapters/preface.html#this-book",
    "title": "Preface",
    "section": "This book",
    "text": "This book\nThis book develops both the technical tools and conceptual frameworks needed for climate risk management:\n\nPart I provides the statistical, optimization, and machine learning foundations that enable rigorous analysis of climate risks and decision alternatives\nPart II focuses on characterizing climate hazards and their uncertainties, emphasizing the integration of multiple imperfect information sources\nPart III addresses the transition from hazard to risk and the design of management strategies under deep uncertainty\n\nThroughout, we emphasize that technical sophistication must be coupled with conceptual clarity about the nature of climate risks and the limits of optimization approaches. The goal is not to abandon quantitative analysis, but to use it more wisely‚Äîfocusing computational power where it adds most value while acknowledging the irreducible uncertainties that require adaptive, robust approaches to climate risk management.\nThis book aims to teach readers how to apply tools from applied mathematics, statistics, and machine learning to answer questions such as\n\nWhat is the probability distribution of some relevant hazards or variables, such as (rainfall, wind, flood, temperature, streamflows) at a specific location?\nHow do these probability distributions change in the next 50 years?\nHow uncertain are these estimates and what specific mechanisms drive these uncertainties?\nWhat is the distribution of annual losses of a portfolio of assets exposed to one or many climate risks?\nWhat are trade-offs between up-front costs and future damages for decisions like how high to elevate a house?\nWhat are robust strategies for sequentially hardening infrastructure against climate risks?\nWhat are trade-offs between flood and drought protection for managing a reservoir?\n\nWhile Part I does provide building blocks, they are intended to be self-contained references rather than a comprehensive overview to applied math, statistics, computer science, machine learning, and operations research. Instead, it aims to give you ‚Äújust enough‚Äù context to think carefully about how to apply tools from these fields to climate risk management challenges.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#what-this-book-is-not",
    "href": "chapters/preface.html#what-this-book-is-not",
    "title": "Preface",
    "section": "What this book is not",
    "text": "What this book is not\nThis book focuses on the technical foundations of climate risk assessment and quantitative decision-making under uncertainty. While we address design requirements, social dimensions, and stakeholder considerations throughout‚Äîrecognizing that technical tools can significantly inform these challenges‚Äîthere are important aspects of climate risk management that require specialized expertise beyond our scope.\nThis book will not primarily teach you how to:\n\nManage reputational and transition risks: While we focus on physical climate risks and their quantitative assessment, organizations also face complex risks from changing policies, markets, and stakeholder expectations that require specialized risk management expertise\nDesign and implement adaptive organizations: While we cover adaptive management strategies and robust decision-making frameworks, the organizational design and management expertise needed to implement these approaches in practice requires additional specialized knowledge\nFacilitate stakeholder processes: While the quantitative tools we teach can strongly support consensus building by clarifying trade-offs and uncertainties, the facilitation, negotiation, and collaborative governance skills needed to lead stakeholder processes require specialized training\nDevelop communication strategies: While we emphasize how to interpret and present quantitative risk assessments, developing effective communication strategies for diverse audiences‚Äîpolicymakers, communities, investors‚Äîrequires specialized expertise in science communication and public engagement\nNavigate implementation challenges: While we address policy design and infrastructure planning from an analytical perspective, the practical challenges of construction management, regulatory processes, and community engagement require domain-specific expertise\n\nThis is an interdisciplinary text that draws insights from multiple fields and acknowledges the social, political, and institutional contexts that shape climate risk management. However, our primary focus remains on the quantitative and analytical foundations that can inform‚Äîbut not replace‚Äîthe broader expertise needed for effective practice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html",
    "href": "chapters/fundamentals/climate-science.html",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#learning-objectives",
    "href": "chapters/fundamentals/climate-science.html#learning-objectives",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "",
    "text": "Understand the three essential questions climate science answers for risk assessment\nIdentify key statistical characteristics of climate hazards that challenge traditional approaches\nRecognize sources of uncertainty in climate information and their implications\nGrasp how physical climate processes drive variability across multiple timescales\nUnderstand the decision-theoretic foundation connecting climate science to risk management",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#why-climate-science-matters-for-risk-assessment",
    "href": "chapters/fundamentals/climate-science.html#why-climate-science-matters-for-risk-assessment",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.1 Why climate science matters for risk assessment",
    "text": "1.1 Why climate science matters for risk assessment\nA hurricane generates storm surge that floods a coastal community. A multi-year drought strains water supplies and agricultural systems. An atmospheric river overwhelms an urban drainage system.\nThese hazardous events are the direct concern of risk management. While they feel like local phenomena, they are not isolated acts of nature. They are the final expression of a complex, interconnected system of climate drivers that operate on scales from planetary to local. Understanding this multi-scale system is the core task of climate science in risk assessment. It allows us to build a causal chain from the large-scale drivers to the local impacts, helping us answer three fundamental questions:\n\nWhat physical processes create the specific weather patterns like hurricanes or droughts that drive hazards?\nHow do these patterns vary naturally over years and decades?\nHow might the statistics of these patterns be shifting?\n\nThis chapter provides the physical foundation for this understanding. We will trace the flow of energy and water through the climate system to see how global patterns shape the regional hazards we must manage.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#global-energy-budget-and-radiative-forcing",
    "href": "chapters/fundamentals/climate-science.html#global-energy-budget-and-radiative-forcing",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.2 Global energy budget and radiative forcing",
    "text": "1.2 Global energy budget and radiative forcing\nEarth‚Äôs climate system is fundamentally driven by the balance between incoming solar (shortwave) radiation and outgoing terrestrial (longwave) radiation. The sun provides essentially all the energy that drives weather and climate. This solar energy heats the surface, which then radiates energy back to space as infrared radiation.\nGreenhouse gases in the atmosphere trap some of this outgoing longwave radiation, warming the surface. This greenhouse effect is essential for making Earth habitable‚Äîwithout it, the planet would be about 33¬∞C cooler. However, human activities have increased atmospheric concentrations of greenhouse gases, creating an energy imbalance.\nRadiative forcing‚Äîan imposed change in the balance between absorbed solar and outgoing terrestrial radiation‚Äîdrives climate change by creating an energy imbalance. That imbalance is gradually reduced through increased outgoing radiation as the planet warms and through heat storage, especially in the oceans. Climate sensitivity, the equilibrium temperature response to doubled atmospheric CO‚ÇÇ, remains uncertain but is estimated at 2.5‚Äì4.0¬∞C. This uncertainty about the magnitude of warming from a given forcing propagates through climate projections.\nThis fundamental warming directly intensifies the global water cycle. The amount of water vapor the atmosphere can hold is governed by the Clausius‚ÄìClapeyron equation, which dictates a roughly 7% increase in moisture per degree Celsius of warming. This thermodynamic effect is a primary mechanism through which the energy balance translates into tangible changes in hydroclimate hazards.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#planetary-circulation-patterns",
    "href": "chapters/fundamentals/climate-science.html#planetary-circulation-patterns",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.3 Planetary circulation patterns",
    "text": "1.3 Planetary circulation patterns\nThe energy imbalance between tropical and polar regions drives large-scale atmospheric and oceanic circulation patterns that distribute heat around the globe. These planetary circulation systems create the framework within which all weather and climate variability occurs.\nAtmospheric circulation includes the Hadley cells that transport heat from the equator toward the poles, creating trade winds and the Intertropical Convergence Zone where much of the world‚Äôs precipitation occurs. The jet streams, driven by temperature gradients between warm tropics and cold poles, guide the paths of storm systems in midlatitudes. Monsoons represent seasonal shifts in these circulation patterns as land masses heat and cool relative to adjacent oceans.\nOceanic circulation redistributes heat through the thermohaline (meridional overturning) circulation driven by temperature and salinity differences (e.g., the Atlantic Meridional Overturning Circulation, AMOC). The Gulf Stream carries warm water northward along the U.S. East Coast before crossing the Atlantic. The Antarctic Circumpolar Current connects all ocean basins. These currents influence regional climates and can change over time as the climate system evolves.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#modes-of-climate-variability",
    "href": "chapters/fundamentals/climate-science.html#modes-of-climate-variability",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.4 Modes of climate variability",
    "text": "1.4 Modes of climate variability\nWithin these overall circulation patterns, the climate system exhibits natural fluctuations that occur on interannual to multidecadal timescales. These modes of variability represent the primary source of year-to-year climate predictability that risk managers must consider.\nEl Ni√±o‚ÄìSouthern Oscillation (ENSO) exemplifies how ocean‚Äìatmosphere interactions create climate variability. Changes in Pacific Ocean temperatures alter atmospheric circulation patterns, affecting precipitation and temperature around the globe. El Ni√±o events typically bring wetter conditions to the southern United States and drier conditions to Australia and Indonesia. La Ni√±a tends to have opposite effects. The phase of ENSO doesn‚Äôt determine the weather, but it shifts the probability distributions of temperature and precipitation across many regions.\nAtlantic Multidecadal Variability (AMV; often called AMO) influences hurricane activity and rainfall patterns around the Atlantic basin over 60‚Äì80-year periods. The Pacific Decadal Oscillation (PDO) affects climate variability along the Pacific rim. The Arctic Oscillation (AO) influences winter weather patterns across the Northern Hemisphere.\nThese natural modes of variability interact with externally forced long-term climate trends. For example, a strong El Ni√±o event superimposed on a warming climate can create record-breaking temperatures. The timing of natural variability can temporarily mask or amplify underlying climate change signals.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#regional-climate-and-local-weather-patterns",
    "href": "chapters/fundamentals/climate-science.html#regional-climate-and-local-weather-patterns",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.5 Regional climate and local weather patterns",
    "text": "1.5 Regional climate and local weather patterns\nThe interplay between large-scale circulation patterns and modes of variability creates the specific weather patterns that lead to local hazards. Understanding these connections requires drawing from several branches of climate science.\nAtmospheric dynamics explains why storms form where they do and how circulation patterns like jet streams influence weather. A meandering jet stream can create atmospheric river events that transport moisture from tropical regions to cause flooding in California. When weather patterns like atmospheric rivers form, they now tap into this thermodynamically enhanced atmospheric moisture, leading to more intense rainfall and a higher flood risk than the same pattern would have produced in a cooler climate. Blocking patterns in the jet stream can create persistent high-pressure systems that lead to heat domes over large regions.\nTropical meteorology focuses on hurricanes, monsoons, and other phenomena that dominate hazards in many regions. Hurricane development requires specific combinations of sea surface temperatures, atmospheric instability, and circulation patterns. Changes in these large-scale conditions affect where storms form, how they track, and how intense they become.\nHydrology examines how precipitation becomes streamflow and how watersheds respond to different rainfall patterns. The same amount of rainfall can produce very different flood responses depending on antecedent soil moisture, snowpack conditions, and the spatial and temporal distribution of precipitation.\nOcean processes affect regional climate through heat transport and moisture sources. Sea surface temperature patterns influence evaporation rates and atmospheric moisture content. Changes in ocean circulation can alter regional temperature and precipitation patterns.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#sources-of-uncertainty-in-climate-projections",
    "href": "chapters/fundamentals/climate-science.html#sources-of-uncertainty-in-climate-projections",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.6 Sources of uncertainty in climate projections",
    "text": "1.6 Sources of uncertainty in climate projections\nAs we move from understanding the past climate to projecting the future, our analysis must account for three distinct sources of uncertainty. These are not interchangeable forms of statistical noise; they arise from different processes and dominate at different time horizons.\n\n1.6.1 Scenario Uncertainty\nThis uncertainty stems from our inability to predict future human activities, primarily greenhouse gas emissions, land-use change, and aerosol concentrations. It is fundamentally a socioeconomic uncertainty, not a physical climate-science uncertainty. Projections are therefore conditioned on a set of plausible futures, or scenarios (e.g., Shared Socioeconomic Pathways, SSPs), each representing a different trajectory of global development. For long-term projections (e.g., post-2050), scenario uncertainty is typically the largest contributor to the total uncertainty in global mean temperature and many regional climate variables.\n\n\n1.6.2 Model Uncertainty\nThis reflects the limitations of our scientific understanding and our ability to represent the climate system numerically. Global Climate Models (GCMs) are based on the fundamental laws of physics, but they must approximate or parameterize processes that are too small or complex to be explicitly resolved, such as cloud formation, ocean turbulence, and vegetation dynamics. Different modeling centers make different, scientifically defensible choices for these parameterizations. The resulting spread in projections from a multi-model ensemble (e.g., CMIP6) represents our structural uncertainty about the true response of the climate system to a given forcing. This is because GCMs must approximate or parameterize processes that are too small or complex to be explicitly resolved, such as individual thunderstorms or cloud formation, leading to scientifically defensible differences between models.\n\n\n1.6.3 Internal Variability\nThis is the natural, unforced variability inherent in the chaotic climate system. It arises from the complex, nonlinear interactions between the atmosphere, oceans, land, and ice. Phenomena like the El Ni√±o‚ÄìSouthern Oscillation (ENSO) or the North Atlantic Oscillation (NAO) are expressions of this internal variability. Even with a perfect model and a perfectly known future emissions scenario, the precise timing of these natural fluctuations is not predictable beyond a few weeks or months. This aleatoric (inherent randomness) uncertainty is the dominant source of uncertainty for projections on short time horizons (e.g., the next 10‚Äì20 years), as natural climate swings can easily mask or amplify the long-term forced trend.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#statistical-characteristics-of-climate-hazards",
    "href": "chapters/fundamentals/climate-science.html#statistical-characteristics-of-climate-hazards",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.7 Statistical characteristics of climate hazards",
    "text": "1.7 Statistical characteristics of climate hazards\nClimate hazards exhibit statistical properties that challenge traditional risk assessment approaches. Understanding why these challenges arise connects the physical climate science we‚Äôve just described to the probabilistic methods needed for risk assessment.\n\n1.7.1 Properties that challenge traditional approaches\nThe physical processes described above create several statistical characteristics that make standard statistical methods problematic.\nExtreme events occur more frequently than a normal distribution predicts. This heavy-tailed (‚Äúfat-tailed‚Äù) behavior emerges from the nonlinear dynamics of atmospheric and oceanic systems, such as the blocking patterns and atmospheric river events described earlier. Small changes in initial conditions or boundary forcing can lead to disproportionately large changes in outcomes. This has profound implications for infrastructure design based on return periods and for insurance mechanisms that assume lighter-tailed distributions.\nClimate change violates the stationarity assumption that underlies most traditional statistical analysis, because, as discussed, radiative forcing from greenhouse gases is altering the fundamental energy balance of the planet. Statistical properties do not remain constant over time. Mean values change as global temperatures rise. Variability patterns shift as circulation patterns evolve. The modes of climate variability we described‚ÄîENSO, AMV‚Äîmay themselves change in a warming climate.\nThe multi-scale nature of climate processes creates challenges for statistical analysis. Interannual variability like ENSO affects weather patterns globally over 2‚Äì7-year cycles. Decadal patterns can dominate regional climate for decades. This means that short observational records may miss important modes of variability that could affect risk assessment. Each source of uncertainty‚Äîinternal variability, model uncertainty, and scenario uncertainty‚Äîcontributes to uncertainty in the posterior predictive distributions used for decisions.\nExtreme events often affect large regions simultaneously, challenging assumptions of spatial independence that underlie many risk models, because planetary-scale circulation patterns like the jet stream and modes of variability like ENSO create spatial coherence in climate anomalies. Heat waves can span entire continents when jet stream patterns create persistent blocking. Droughts affect multiple countries within a river basin when large-scale circulation anomalies reduce precipitation over wide areas. Understanding and modeling these spatial dependencies requires methods covered in Correlation and Dimensionality.\n\n\n1.7.2 Implications for climate information\nThese characteristics, rooted in the physics of the climate system, create requirements for climate information that supports decision-making. Climate characterizations should be consistent with known physical processes rather than purely statistical constructs. They should provide sufficient spatial and temporal resolution for the impact assessment at hand. Large ensembles help quantify the uncertainty sources described above rather than providing single-valued projections. Multiple plausible scenarios acknowledge deep uncertainties about future emissions, policies, and system responses.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#using-climate-models-in-risk-assessment",
    "href": "chapters/fundamentals/climate-science.html#using-climate-models-in-risk-assessment",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "1.8 Using Climate Models in Risk Assessment",
    "text": "1.8 Using Climate Models in Risk Assessment\n\n1.8.1 The uncertainty cascade\nClimate impact projections involve an uncertainty cascade from emissions to impacts. Different socioeconomic pathways lead to different greenhouse gas concentrations. Climate sensitivity‚Äîthe global temperature response to doubled CO‚ÇÇ‚Äîremains uncertain, with current estimates ranging from 2.5-4.0¬∞C. Global temperature changes translate to regional climate patterns through complex atmospheric and oceanic dynamics. Regional climate changes then drive local impacts through sector-specific models for floods, droughts, agriculture, and other consequences.\nUncertainty compounds at each step. For near-term projections (2030), internal climate variability often dominates uncertainty, meaning that natural fluctuations can mask or amplify climate change signals. For long-term projections (2100), scenario uncertainty becomes most important, as different emissions pathways lead to dramatically different climate outcomes. At regional scales and for extreme events, model uncertainty often dominates throughout the century.\nClimate models are evaluated against historical observations, process understanding, and paleoclimate data. A model that reproduces observed climate trends and captures realistic physical mechanisms provides more confidence for future projections. However, good historical performance doesn‚Äôt guarantee accurate future projections, especially for unprecedented conditions or potential tipping point behaviors that lie outside historical experience.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#further-reading",
    "href": "chapters/fundamentals/climate-science.html#further-reading",
    "title": "1¬† Fundamentals of climate science üöß",
    "section": "Further reading",
    "text": "Further reading\nFor statistical approaches to climate extremes, Mudelsee (2020) provides excellent technical background on time series analysis and extreme value methods for climate applications. Merz et al. (2014) offers a comprehensive review of flood risk assessment methods with practical examples that illustrate many concepts covered in this chapter. For a mathematically rigorous treatment grounded in dynamical systems theory, Ghil et al. (2011) connects physical climate processes to statistical extreme value behavior.\n\n\n\n\nGhil, M, P Yiou, S Hallegatte, B D Malamud, P Naveau, A Soloviev, P Friederichs, et al. 2011. ‚ÄúExtreme Events: Dynamics, Statistics and Prediction.‚Äù Nonlinear Processes in Geophysics 18 (3): 295‚Äì350. https://doi.org/10/fvzxvv.\n\n\nMerz, Bruno, Jeroen C J H Aerts, Karsten Arnbjerg-Nielsen, M Baldi, A Becker, A Bichet, G√ºnter Bl√∂schl, et al. 2014. ‚ÄúFloods and Climate: Emerging Perspectives for Flood Risk Assessment and Management.‚Äù Natural Hazards and Earth System Science 14 (7): 1921‚Äì42. https://doi.org/10/gb9nzm.\n\n\nMudelsee, Manfred. 2020. ‚ÄúStatistical Analysis of Climate Extremes / Manfred Mudelsee.‚Äù In Statistical Analysis of Climate Extremes. Cambridge, United Kingdom ; Cambridge University Press.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Fundamentals of climate science üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html",
    "href": "chapters/fundamentals/probability-stats.html",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#learning-objectives",
    "href": "chapters/fundamentals/probability-stats.html#learning-objectives",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "",
    "text": "Understand foundational probability concepts (random variables, distributions, moments).\nApply descriptive and inferential statistical methods to climate-related datasets.\nRecognize when and why certain probability models are appropriate for climate variables.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#three-steps-of-probabilistic-inference",
    "href": "chapters/fundamentals/probability-stats.html#three-steps-of-probabilistic-inference",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "2.1 Three steps of probabilistic inference",
    "text": "2.1 Three steps of probabilistic inference\nBefore diving into the mathematical foundations, let‚Äôs establish what we‚Äôre trying to accomplish with probabilistic methods in climate risk assessment. Probabilistic inference is fundamentally about learning from data to make decisions under uncertainty.\nFollowing Gelman et al. (2014), we can organize all of probabilistic modeling into three essential steps:\n\nSet up a full probability model: Define a joint probability distribution for all observable and unobservable quantities in your problem\nCondition on observed data: Calculate the posterior distribution of unknown quantities given what you‚Äôve observed\nEvaluate the fit and implications: Check how well the model captures reality and what it implies for decisions\n\nThis three-step workflow applies whether you‚Äôre estimating flood return periods, projecting future hurricane intensities, or evaluating adaptation strategies. Let‚Äôs see this in action with a simple example before diving into the mathematical machinery.\n\n2.1.1 A simple example: Coin flipping\nLet‚Äôs start with the simplest possible example to illustrate the three-step workflow. Suppose you have a coin and want to know if it‚Äôs fair. This simple model is analogous to many real-world risk problems: for example, asking if a given year has a ‚Äòhigh‚Äô or ‚Äòlow‚Äô chance of experiencing a major flood based on a complex climate model‚Äôs binary prediction. In both cases, we have a binary outcome and want to infer the underlying probability.\nStep 1 - Set up the model: - Parameter of interest: \\(\\theta\\) = probability the coin lands heads - Prior belief: maybe the coin is fair, so \\(\\theta\\) could be around 0.5 - Data model: each flip follows \\(\\text{Bernoulli}(\\theta)\\)\nStep 2 - Learn from data: You flip the coin 10 times and observe 7 heads. Use Bayes‚Äô theorem to update your belief about \\(\\theta\\).\nStep 3 - Make decisions: Based on your posterior distribution for \\(\\theta\\), decide whether the coin is likely fair or biased.\nThis simple example captures the essence of probabilistic inference. We‚Äôll formalize this logic throughout the chapter, then apply it to much more complex climate risk problems.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#the-language-of-uncertainty",
    "href": "chapters/fundamentals/probability-stats.html#the-language-of-uncertainty",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "2.2 The language of uncertainty",
    "text": "2.2 The language of uncertainty\nWe assume you have taken courses in probability and statistics and are familiar with basic concepts. This section provides a focused review of essential foundations, emphasizing how they serve probabilistic inference for climate risk. For more mathematical details, Michael Betancourt‚Äôs writing is an outstanding resource.\n\n2.2.1 General notation for statistical inference\nBefore developing the theory, let‚Äôs establish clear notation that we‚Äôll use throughout the book. Following Gelman et al. (2014), we distinguish three types of quantities:\n\nParameters (\\(\\theta\\)): Unknown quantities we want to learn about (e.g., the true 100-year flood level, hurricane intensity parameters, or climate sensitivity)\nObserved data (\\(y\\)): Quantities we have measured (e.g., historical flood heights, satellite temperature measurements, or storm track records)\nUnobserved predictions (\\(\\tilde{y}\\)): Unknown but potentially observable quantities (e.g., next year‚Äôs maximum temperature, future sea level, or the outcome of a proposed adaptation strategy)\n\nThe joint probability model \\(p(\\theta, y, \\tilde{y})\\) describes our beliefs about all these quantities and their relationships. This joint distribution is the central object in probabilistic inference‚Äîeverything else flows from asking questions of this distribution.\nFor climate risk assessment, \\(\\theta\\) might represent physical parameters like climate sensitivity or return period parameters, \\(y\\) represents our historical observations and measurements, and \\(\\tilde{y}\\) represents future climate conditions we need to predict for planning.\n\n\n2.2.2 Exchangeability\nA fundamental assumption underlying most statistical models is exchangeability: the idea that we can learn about future observations from past observations because they share some common underlying structure.\nTwo random variables \\(Y_1\\) and \\(Y_2\\) are exchangeable if their joint distribution is unchanged by swapping their labels: \\(p(Y_1, Y_2) = p(Y_2, Y_1)\\). More generally, observations \\(Y_1, \\ldots, Y_n\\) are exchangeable if their joint distribution is invariant under permutations.\nFor climate applications, exchangeability justifies using historical flood records to estimate future flood risk, or using temperature measurements from different years to understand temperature variability. However, exchangeability can break down when the underlying system changes‚Äîfor instance, due to climate change or urbanization affecting flood patterns. Non-stationarity and climate change impacts on statistical properties are discussed in Fundamentals of Climate Science.\n\n\n2.2.3 Axiomatic foundations\nProbability theory rests on three simple axioms that ensure probabilities behave consistently: they‚Äôre non-negative, total probability equals one, and probabilities of mutually exclusive events add up. From these basic rules, we can derive all the mathematical machinery needed for probabilistic inference (Jaynes 2003).\n\n\n2.2.4 Joint distributions\nThe central concept in probabilistic inference is the joint probability distribution. For any problem involving uncertainty, our goal is to specify a joint distribution over all relevant quantities‚Äîboth observed and unobserved.\nThe joint distribution \\(p(\\theta, y, \\tilde{y})\\) completely characterizes our knowledge state about:\n\nWhat we want to learn (parameters \\(\\theta\\))\nWhat we have observed (data \\(y\\))\n\nWhat we want to predict (future observations \\(\\tilde{y}\\))\n\nEverything else in probabilistic inference involves asking questions of this joint distribution:\n\nLearning from data: \\(p(\\theta | y) \\propto p(\\theta, y)\\) (conditioning)\nMaking predictions: \\(p(\\tilde{y} | y) = \\int p(\\tilde{y}, \\theta | y) d\\theta\\) (marginalization)\nQuantifying decision-relevant quantities: \\(\\mathbb{E}[f(\\theta, \\tilde{y}) | y]\\) (expectation of functions)\n\nThis perspective‚Äîthat statistical modeling is about building joint distributions‚Äîprovides the foundation for the computational methods we‚Äôll develop later.\n\n\n2.2.5 Bayes‚Äô theorem\nBayes‚Äô theorem connects the different types of probability statements and provides the foundation for updating beliefs based on evidence. Since joint probability can be decomposed as \\(p(\\theta, y) = p(\\theta) p(y | \\theta) = p(y) p(\\theta | y)\\), we can derive:\n\\[\np(\\theta | y) = \\frac{p(\\theta) p(y | \\theta)}{p(y)}\n\\]\nThis equation has a powerful interpretation: \\(p(\\theta)\\) represents our prior beliefs about parameters \\(\\theta\\), \\(p(y | \\theta)\\) represents the likelihood of observing data \\(y\\) given parameters \\(\\theta\\), and \\(p(\\theta | y)\\) represents our posterior beliefs after seeing the data.\nIn many applications, we work with the proportional form \\(p(\\theta | y) \\propto p(\\theta) p(y | \\theta)\\) since the denominator \\(p(y)\\) doesn‚Äôt depend on \\(\\theta\\). This framework is essential for Bayesian inference in climate science, where we often want to update our understanding of physical processes based on observations.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#using-probability-models-to-learn-from-data",
    "href": "chapters/fundamentals/probability-stats.html#using-probability-models-to-learn-from-data",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "2.3 Using probability models to learn from data",
    "text": "2.3 Using probability models to learn from data\nProbability theory gives us the language for representing uncertainty, but probabilistic inference is about using that language to answer specific questions. Given a joint probability model \\(p(\\theta, y, \\tilde{y})\\), we need computational tools to extract the information relevant for decision-making.\nAll probabilistic inference involves two fundamental operations on the joint distribution:\n\n2.3.1 The two fundamental operations\n\n2.3.1.1 1. Conditioning: Learning from data\nConditioning answers: ‚ÄúHow does observing data \\(y\\) change our beliefs about parameters \\(\\theta\\)?‚Äù\nWe use Bayes‚Äô theorem to compute the posterior distribution: \\[p(\\theta | y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(\\theta) p(y | \\theta)}{\\int p(\\theta) p(y | \\theta) d\\theta}\\]\nThe posterior \\(p(\\theta | y)\\) represents our updated knowledge about \\(\\theta\\) after seeing data \\(y\\). This is the mathematical engine for learning‚Äîit tells us how to rationally update our beliefs based on evidence.\nFor climate risk assessment, conditioning might update our beliefs about:\n\nHurricane intensity parameters after observing a new storm season\nFlood return levels after observing recent extreme events\n\nClimate sensitivity parameters after incorporating new temperature data\n\n\n\n2.3.1.2 2. Marginalization: Focusing on what matters\nMarginalization answers: ‚ÄúWhat do we know about quantity \\(\\theta_1\\) while ignoring nuisance parameters \\(\\theta_2\\)?‚Äù\nWe integrate out unwanted variables to focus on quantities of interest: \\[p(\\theta_1 | y) = \\int p(\\theta_1, \\theta_2 | y) d\\theta_2\\]\nThis allows us to extract the uncertainty in specific parameters while averaging over everything else we don‚Äôt care about for a particular decision.\nFor example, when estimating flood risk, we might marginalize over uncertain rainfall distribution parameters to focus solely on the flood frequency we need for infrastructure planning.\n\n\n\n2.3.2 Computing expectations\nThese two operations ultimately serve one purpose: computing expectations that inform decisions. When we need to know the probability of an event or the expected value of some quantity of interest, we often need to apply both conditioning and marginalization to extract that information from our joint distribution.\nThe goal of probabilistic inference is computing expectations ‚Äî probability-weighted averages of quantities we care about. Most decision-relevant quantities can be expressed as an expectation:\n\nThe mean is \\(\\mathbb{E}[X]\\)\nThe variance is \\(\\mathbb{E}[(X - \\mathbb{E}[X])^2]\\)\nThe probability of an event is \\(\\mathbb{E}[\\mathbf{1}_{\\text{event}}]\\) (expectation of an indicator function)\nExpected annual damages from hurricanes integrate over storm intensity, frequency, and damage functions\nFailure probabilities for infrastructure under extreme heat integrate over temperature distributions and failure thresholds\n\nFormally, we want to compute: \\[\\mathbb{E}[g(\\theta, \\tilde{y}) | y] = \\int g(\\theta, \\tilde{y}) p(\\theta, \\tilde{y} | y) d\\theta d\\tilde{y}\\]\nwhere \\(g(\\theta, \\tilde{y})\\) is some function of our parameters and predictions that captures what we need for decision-making.\n\n2.3.2.1 Analytical vs computational approaches\nFor simple models, we can sometimes compute expectations analytically by solving integrals directly. But for the complex, high-dimensional problems typical in climate risk assessment, we need computational approaches:\nMonte Carlo estimation approximates expectations by: 1. Drawing many samples from the probability model: \\((\\theta^{(1)}, \\tilde{y}^{(1)}), \\ldots, (\\theta^{(S)}, \\tilde{y}^{(S)})\\) 2. Computing the function of interest: \\(g(\\theta^{(s)}, \\tilde{y}^{(s)})\\) for each sample 3. Averaging: \\(\\mathbb{E}[g(\\theta, \\tilde{y}) | y] \\approx \\frac{1}{S} \\sum_{s=1}^S g(\\theta^{(s)}, \\tilde{y}^{(s)})\\)\nThis approach forms the foundation for computational methods we‚Äôll develop in subsequent chapters.\n\n\n\n2.3.3 Interpreting probability\nNow that we‚Äôve seen how expectations drive decision-making, it‚Äôs worth reflecting on what the probabilities in these expectations mean. This philosophical question becomes crucial when studying extreme events and catastrophic risks that cannot be objectively verified through repeated experimentation.\nFrequentist (objective) interpretation views probability as limiting frequencies of repeatable events. Under this view, statements like ‚Äúthe probability of a category 5 hurricane is 0.02‚Äù refer to long-run frequencies we could observe if we repeated the same conditions many times.\nBayesian (subjective/epistemic) interpretation views probability as degrees of belief or states of knowledge. Here, probability statements reflect our uncertainty given available information, even for unique events like ‚Äúthe probability that sea level rise exceeds 1 meter by 2100.‚Äù\nFor extreme events and catastrophic risks, the Bayesian interpretation is often more useful because:\n\nMany catastrophic events are rare or unique (we can‚Äôt repeat the experiment)\nWe often need to reason about future scenarios that have never occurred\nScientific knowledge combines multiple sources of uncertain information\n\n\n\n2.3.4 Computational approaches\nThe Bayesian framework naturally leads us to computational methods for calculating expectations, since the required integrals are often intractable analytically.\n\n2.3.4.1 Law of large numbers\nThe law of large numbers justifies this approach by formalizing that sample averages converge to true expectations. For independent, identically distributed random variables \\(X_1, X_2, \\ldots\\) with mean \\(\\mu\\): \\[\n\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n X_i = \\mu\n\\]\nFor catastrophic risk assessment, this means:\n\nMonte Carlo estimates of expected annual losses become more accurate with more simulations\nHistorical averages of rare event frequencies converge to true rates (given stationarity)\nComplex expectations over joint distributions of multiple hazards can be reliably estimated\n\nSpatial and temporal dependence in climate data requires specialized methods covered in Correlation and Dimensionality.\nHowever, for extreme events, convergence can be slow due to heavy tails and rare events. Estimating expectations of catastrophic losses may require many thousands of simulations to achieve reasonable accuracy.\n\n\n2.3.4.2 Central limit theorem\nThe law of large numbers tells us that Monte Carlo estimates converge, but how uncertain are our estimates? The central limit theorem quantifies this uncertainty: our sample average \\(\\bar{Y}\\) is approximately normally distributed around the true expectation \\(\\mu\\):\n\\[\\bar{Y} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nFor catastrophic risk assessment, this means we can quantify the Monte Carlo uncertainty in our estimates of expected annual losses, failure probabilities, and other risk metrics. This is crucial for decision-making because it distinguishes between uncertainty in our estimates (which decreases with more simulation) and fundamental uncertainty about the underlying extreme event processes.\nStatistical inference for extreme events involves making conclusions about catastrophic risks from limited data. Two fundamentally different philosophical approaches dominate, each with important implications for how we understand and communicate about rare event risks.\n\n\n2.3.4.3 Frequentist vs Bayesian approaches\nFrequentist inference treats the parameters of extreme event distributions (like hurricane intensity parameters or flood return levels) as fixed but unknown quantities. Under this approach, uncertainty comes from sampling variability‚Äîwe‚Äôd get different parameter estimates if we observed different historical samples. Confidence intervals represent the variability we‚Äôd expect across many hypothetical repetitions of the same data collection process.\nBayesian inference treats parameters as random variables representing our knowledge state. Prior distributions encode our beliefs about plausible parameter values (perhaps from physical understanding or regional climate information), which get updated with observed data to produce posterior distributions representing our updated knowledge.\nFor catastrophic risk assessment, the philosophical difference matters:\n\nFrequentist: ‚ÄúGiven these fixed (unknown) hurricane parameters, there‚Äôs a 5% chance our confidence interval won‚Äôt contain the true 100-year return level‚Äù\nBayesian: ‚ÄúGiven our data and prior knowledge, there‚Äôs a 95% probability the 100-year return level lies in this credible interval‚Äù\n\n\n\n2.3.4.4 Why Bayesian methods suit extreme events\nBayesian approaches prove particularly valuable for extreme event analysis because:\n\nLimited data: Extreme events are rare, making prior knowledge from physical processes crucial\nDecision focus: Decisions require acting on current knowledge, not hypothetical repetitions\nUnique events: Many catastrophic scenarios (like worst-case sea level rise) are inherently unique\nUncertainty propagation: Complex models benefit from full uncertainty characterization\n\n\n\n2.3.4.5 Hypothesis testing for extremes\nA common practical question is whether observed data are consistent with specific hypotheses about extreme event behavior‚Äîfor instance, testing whether hurricane intensities show trends or whether flood frequencies have changed. Both frequentist and Bayesian frameworks provide tools for these questions, though they interpret the results differently.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#probability-building-blocks",
    "href": "chapters/fundamentals/probability-stats.html#probability-building-blocks",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "2.4 Probability building blocks",
    "text": "2.4 Probability building blocks\nThe inference framework above relies on several key probability concepts. This section provides the essential building blocks for constructing probability models.\n\n2.4.1 Random variables\nA random variable is a function that assigns numerical values to outcomes of random experiments. We use capital letters (\\(X\\), \\(Y\\), \\(Z\\)) to denote random variables and lowercase letters (\\(x\\), \\(y\\), \\(z\\)) for their specific realizations or values.\nFor catastrophic risk applications, \\(X\\) might represent ‚Äúmaximum storm surge height during a hurricane‚Äù and \\(x = 4.2\\) meters would be a specific observation. Continuous random variables (like storm surge heights, wildfire burned area, or drought duration) can take any value in an interval, while discrete random variables (like the annual number of Category 4+ hurricanes or power outage events) take countable values.\n\n\n2.4.2 CDFs, PDFs, and PMFs\nTo work with random variables, we need functions that describe how probability is distributed across possible values.\nThe cumulative distribution function (CDF) answers ‚Äúwhat is the probability that \\(X\\) is at most \\(x\\)?‚Äù: \\[F_X(x) = \\Pr(X \\leq x)\\]\nFor continuous random variables, we work with probability density functions (PDFs). The density \\(f_X(x)\\) represents probability per unit length, related to the CDF through calculus: \\[F_X(x) = \\int_{-\\infty}^x f_X(u) \\, du \\quad \\text{and} \\quad f_X(x) = \\frac{d}{dx}F_X(x)\\]\nWe calculate probabilities by integrating the PDF over intervals: \\[\\Pr[a \\leq X \\leq b] = \\int_a^b f_X(x) \\, dx\\]\n\n\n\n\n\n\nFor continuous random variables, \\(\\Pr(X = x^*) = 0\\) for any specific value \\(x^*\\). Probability is concentrated over intervals, not points.\n\n\n\nDiscrete random variables use probability mass functions (PMFs) instead: \\(p_X(x) = \\Pr(X = x)\\). Unlike continuous variables, discrete variables can assign non-zero probability to individual values.\n\n\n2.4.3 Marginal, conditional, and joint distributions\nCatastrophic events often involve multiple interacting hazards‚Äîstorm surge and wind speed during hurricanes, temperature and drought severity during heat waves, or rainfall intensity and duration during floods. Understanding these compound extremes requires working with multiple random variables simultaneously. Spatial dependence and correlation structures are explored in detail in Correlation and Dimensionality.\nJoint probability describes multiple extreme events occurring together: \\(p(x, y)\\) for joint densities or \\(\\Pr(X = x, Y = y)\\) for discrete events. This answers questions like ‚Äúwhat‚Äôs the probability of both extreme storm surge AND category 4+ winds?‚Äù ‚Äî critical for compound hurricane risk.\nMarginal probability focuses on one hazard while ignoring others: \\(p(x)\\) for continuous variables or \\(\\Pr(X = x)\\) for discrete events. For instance, the marginal distribution of wildfire burned area ignores weather conditions, even though they strongly influence fire behavior.\nConditional probability describes one extreme given another has occurred: \\(p(x | y)\\) for conditional densities or \\(\\Pr(X = x | Y = y) = \\frac{\\Pr(X = x, Y = y)}{\\Pr(Y = y)}\\) for discrete events. This answers questions like ‚Äúgiven an extreme heat event, what‚Äôs the distribution of drought severity?‚Äù ‚Äî essential for cascading risk assessment.\n\n\n2.4.4 Independence\nTwo events \\(A\\) and \\(B\\) are independent if knowing that one occurred provides no information about the other. Mathematically, independence means: \\[\\Pr(A \\cap B) = \\Pr(A) \\cdot \\Pr(B)\\]\nEquivalently, for independent events: \\(\\Pr(A | B) = \\Pr(A)\\) and \\(\\Pr(B | A) = \\Pr(B)\\).\nFor random variables \\(X\\) and \\(Y\\), independence means the joint density factorizes: \\(p(x, y) = p(x) \\cdot p(y)\\). This is a strong assumption that often doesn‚Äôt hold in climate systems‚Äîtemperature and precipitation are typically dependent, as are wind speed and atmospheric pressure.\nConditional independence is also important: \\(X\\) and \\(Y\\) may be dependent overall but independent given knowledge of a third variable \\(Z\\). This concept is crucial for understanding how climate variables relate through mediating factors.\n\n\n2.4.5 Common probability distributions\nSeveral key distributions appear frequently in catastrophe modeling and extreme event analysis:\n\nThe Poisson distribution \\(Y \\sim \\text{Poisson}(\\lambda)\\) models rare event counts like annual numbers of Category 4+ hurricanes, major earthquakes, or wildfire ignitions over fixed periods.\nThe Negative Binomial distribution handles overdispersed extreme event counts when variance exceeds the mean‚Äîcommon when events cluster in time (like hurricane seasons).\nThe Gamma distribution models positive-valued extremes like maximum daily rainfall amounts, drought durations, and time between major flood events.\nThe Weibull distribution is fundamental for extreme value analysis, modeling phenomena like maximum wind speeds, peak storm surge heights, and infrastructure failure times.\nGeneralized Extreme Value (GEV) and Generalized Pareto (GP) distributions specifically model the statistical behavior of extreme values and exceedances over high thresholds‚Äîfoundational for catastrophe risk assessment.\n\n\n2.4.5.1 Moments\nThe moments of a distribution characterize its shape and properties‚Äîparticularly important for understanding extreme event behavior. The first few moments have direct interpretations for catastrophe modeling:\n\nFirst moment (mean): \\(\\mathbb{E}[X] = \\mu\\) - the center of the distribution\nSecond central moment (variance): \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mu)^2] = \\sigma^2\\) - spread around the mean\n\nThird central moment (skewness): measures asymmetry - positive skew indicates long right tails with rare extreme values\nFourth central moment (kurtosis): measures tail heaviness - higher kurtosis means more frequent extreme values\n\nFor catastrophic events, skewness and kurtosis are crucial. Storm surge heights typically show positive skew (most events are small, rare events are catastrophic), while wildfire burned areas often exhibit heavy tails with extreme kurtosis during exceptional fire seasons. The physical processes driving these statistical characteristics are discussed in Fundamentals of Climate Science.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#further-reading",
    "href": "chapters/fundamentals/probability-stats.html#further-reading",
    "title": "2¬† Probability and Inference ‚úèÔ∏è",
    "section": "Further reading",
    "text": "Further reading\nIf most of this content is unfamiliar to you, reviewing a more comprehensive introduction to probability and statistics before proceeding may be helpful. While introductory statistics is often taught in a dry way, Blitzstein and Hwang (2019), Downey (2021), and Gelman (2021) are three excellent resources that I highly recommend (see recommended reading for more suggestions).\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability, Second Edition. 2nd Edition. Boca Raton: Chapman and Hall/CRC. http://probabilitybook.net.\n\n\nDowney, Allen B. 2021. Think Bayes. \"O‚ÄôReilly Media, Inc.\". https://allendowney.github.io/ThinkBayes2/.\n\n\nGelman, Andrew. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge, United Kingdom ; Cambridge University Press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 2014. Bayesian Data Analysis. 3rd ed. Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. New York, NY: Cambridge University Press.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Probability and Inference ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html",
    "href": "chapters/fundamentals/optimization.html",
    "title": "3¬† Optimization üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html#learning-objectives",
    "href": "chapters/fundamentals/optimization.html#learning-objectives",
    "title": "3¬† Optimization üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the components of an optimization problem: decision variables, objective functions, and constraints.\nLearn how optimization is applied in statistics and policy search.\nExplore the trade-offs between problem complexity and solution accuracy.\n\nOptimization is a field of study unto itself, and forms an essential backbone of statistics, machine learning, and operations research. Here, we briefly summarize key concepts and techniques in optimization, with a focus on their application to statistics and policy search.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html#overview",
    "href": "chapters/fundamentals/optimization.html#overview",
    "title": "3¬† Optimization üöß",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nOptimization is the study of finding the best solution to a problem from a set of feasible solutions. Thus, optimization problems typically consist of\n\nA set of decision variables , \\(x\\), which are the inputs we can control (or the ‚Äúdials‚Äù we can turn).\nOne (or more) objective function(s), \\(f(x)\\), which we want to minimize or maximize.\nA set of constraints which define the feasible region.\n\nOptimization is an incredibly diverse field. Problems can have many constraints or none; can have discrete or continuous decision variables (or both); can be static or dynamic; deterministic or stochastic; linear or nonlinear; or much more. When you encounter an optimization problem in the wild, you should always make sure you understand the decision variables (which define what is being optimized), the objective function(s), which define what makes a solution ‚Äúgood‚Äù, and the constraints, which define what solutions are being considered. When you build an optimization model, you should always make sure to communicate these three components clearly.\nBecause optimization is a widely studied field, exact solutions to some classes of problems are known, and nearly exact solutions to many others are known. However, these exact solutions often require formulating the problem in a very specific way. Thus, there is generally a trade-off between designing an optimization problem that is (relatively) easy to solve vs.¬†one that represents the relevant system dynamics and uncertainties (relatively) accurately. Consequently, converting a decision problem into an optimization problem is somewhat of an art form, rather than an exact science; this is largely the field of operations research.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html#motivation-inference",
    "href": "chapters/fundamentals/optimization.html#motivation-inference",
    "title": "3¬† Optimization üöß",
    "section": "3.2 Motivation: Inference",
    "text": "3.2 Motivation: Inference\nOne common application of optimization is in statistics. Specifically, we often want to evaluate how consistent the data are with different values of the parameters, and to find the values of the parameters that are most consistent with the data.\n\n3.2.1 General Case\n\n3.2.1.1 Likelihood\nThe likelihood is the probability of the observations \\(y\\) given some parameters \\(\\theta\\): \\[\np(y | \\theta)\n\\]\nOften, we want to study how the likelihood changes for different values of \\(\\theta\\), holding \\(y\\) fixed. This is just \\(p(y | \\theta)\\) for a range of \\(\\theta\\).\n\n\n\n\n\n\nYou will sometimes see this referred to as \\(\\mathcal{L}(\\theta)\\). However, since this is a probability distribution, it doesn‚Äôt really need its own notation.\n\n\n\n\n\n3.2.1.2 Likelihood function for multiple observations\nOften, we have multiple observations \\(y_1, y_2, \\ldots, y_n\\).\nIndependent and identically distributed (i.i.d.) assumption: \\[\n\\begin{aligned}\np(y_1, y_2, \\ldots, y_n) &= p(y_1) p(y_2) \\times \\ldots \\times p(y_n)\\\\\n&= \\prod_{i=1}^n p(y_i)\n\\end{aligned}\n\\]\nUsually we have more than one data point. Say we measure \\(y = y_1, y_2, \\ldots, y_n\\): \\[\np(y | \\theta) = \\prod_{i=1}^n p(y_i | \\theta)\n\\]\n\n\n3.2.1.3 Log trick\nRecall: \\(\\log(AB) = \\log(A) + \\log(B)\\) or, more generally, \\[\n\\log \\left( \\prod_{i=1}^n f_i \\right) = \\sum_{i=1}^n \\log(f_i)\n\\]\nThus, we can work with the ‚Äúlog likelihood‚Äù: \\[\n\\log p(y | \\theta) =  \\log \\left( \\prod_{i=1}^n p(y_i | \\theta) \\right) = \\sum_{i=1}^n \\log \\left( p(y_i | \\theta) \\right)\n\\]\nAdding small numbers is also more numerically stable than multiplying them\nCan we find the parameters \\(\\theta^*\\) that maximize the likelihood \\(p(y | \\theta)\\)?\n\n\n3.2.1.4 Log likelihood\nWe can use the log likelihood \\(\\log p(y | \\theta)\\) instead of the likelihood \\(p(y | \\theta)\\).\nThe log likelihood is monotonic with the likelihood, so \\[\n\\arg \\max \\log p(y | \\theta) = \\arg \\max p(y | \\theta)\n\\]\nSolving things analytically takes time up-front, but can be much faster to run because you can avoid the optimization step. Consider the (potentially multivariate) Gaussian example with known covariance matrix \\(\\Sigma\\). We want to maximize the likelihood \\[\n\\sum_{i=1}^n p(y_i | \\mu, \\Sigma)\n\\]\nTo maximize, we set its derivative with respect to \\(\\mu\\), which we‚Äôll denote with \\(\\nabla_\\mu\\), to zero: \\[\n\\sum_{i=1}^n \\nabla_\\mu \\log p(y_i | \\mu, \\Sigma) = 0\n\\]\nSubstituting in the multivariate Gaussian likelihood we get: \\[\n\\begin{aligned}\n0 & =\\sum_{i=1}^n \\nabla_\\mu \\log \\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}} \\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right) \\\\\n& =\\sum_{i=1}^n \\nabla_\\mu\\left(\\log \\left(\\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}}\\right)\\right)+\\log \\left(\\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\right) \\\\\n& =\\sum_{i=1}^n \\nabla_\\mu\\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\\\\n&=\\sum_{i=1}^n \\Sigma^{-1}\\left(x_i-\\mu\\right) \\\\\n0 &= \\sum_{i=1}^n (x_i - \\mu) \\\\\n\\mu &= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{aligned}\n\\]\nYou are not expected to remember the above equations and I won‚Äôt ask you to do this derivation in a time-constrained exam. You should understand the general procedure:\n\nwrite down log likelihood for all data points\n\nwrite down likelihood for one data point\nwrite down log likelihood for one data point\nsum over all data points\n\ntake \\(\\frac{d}{d\\theta}\\) and set equal to zero to maximize\nsolve for \\(\\theta^*\\).\n\n\n\n\n3.2.2 Example: Linear Regression\nLet‚Äôs consider the generic regression probelem where we have paired observations \\(\\left\\{x_i, y_i\\right\\}_{i=1}^n\\). In general, we can write this regression as \\[\ny_i | \\alpha, \\beta, \\epsilon \\sim \\mathcal{N}(\\alpha + x_i \\beta, \\sigma^2)\n\\] where \\(x_i\\) and \\(\\beta\\) may be vectors.\nWe can use the same approach to derive the maximum likelihood estimate for linear regression:\n\nWrite the likelihood for one data point\nWrite the log likelihood for one data point\nWrite the log likelihood for all data points\nTake \\(\\frac{d}{d\\theta}\\) and set equal to zero to maximize\n\nIf you want a walkthrough, see Ryan Adams‚Äôs lecture notes starting at about equation 11.\nIf we work through the math, we can show that the log likelihood for the linear regression problem is \\[\n\\log p(y | X, \\beta, \\sigma) = \\frac{N}{2} \\log (2 \\sigma^2 \\pi)  - \\frac{1}{2 \\sigma^2} \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n\\]\n\n\n\n\n\n\nLinear algebra notation\n\n\n\nThere is no intercept here! This is a common notation and assumes that the first column of \\(X\\) is all ones. That is equivalent to writing down an intercept, but lets us use linear algebra notation and keep track of fewer variable names\n\n\nFrom this, we can show that terms drop out and \\[\n\\beta^\\text{MLE} = \\arg \\min_\\beta \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n\\] which is exactly the least squares problem (minimize squared error): \\[\n\\min_{\\theta} \\sum_{i=1}^n (y_i - y_i^\\text{pred})^2\n\\]\n\n\n\n\n\n\nKey point\n\n\n\n‚ÄúLeast squares can be interpreted as assuming Gaussian noise, and particular choices of likelihood can be interpreted directly as (usually exponentiated) loss functions‚Äù ‚ÄìAdams\n\n\nIf we then want to estimate \\(\\sigma\\), we can estimate the standard deviation of the residuals.\n\n\n\n\n\n\nComputational examples\n\n\n\nFor detailed computational examples of likelihood functions, MLE estimation, and numerical optimization, see the companion file: Maximum Likelihood Estimation: Computational Examples.\nExamples include: - Plotting likelihood functions for single and multiple data points - Poisson likelihood for count data (tropical cyclone example)\n- Multivariate likelihood surfaces - Numerical optimization with Optim.jl - Linear regression MLE equivalence to least squares",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html#motivation-policy-search",
    "href": "chapters/fundamentals/optimization.html#motivation-policy-search",
    "title": "3¬† Optimization üöß",
    "section": "3.3 Motivation: Policy Search",
    "text": "3.3 Motivation: Policy Search",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html#optimization-toolkit",
    "href": "chapters/fundamentals/optimization.html#optimization-toolkit",
    "title": "3¬† Optimization üöß",
    "section": "3.4 Optimization Toolkit",
    "text": "3.4 Optimization Toolkit\n\n3.4.1 Gradient-Based Optimization\nThis is useful for thinking about neural networks, but also for many other optimization problems. The key idea is that we can use the gradient of the objective function to find the direction in which to move to improve our solution. This is often called gradient descent. The basic idea is to take a step in the direction of the gradient, which is the direction of steepest ascent. In the simplest case, \\[\n\\Delta x = -\\alpha \\nabla f(x)\n\\] where \\(\\alpha\\) is the step size or learning rate. This is a hyperparameter that we can tune.\nThe primary limitation of gradient descent is that it can get stuck in local minima. This is especially true for non-convex problems, where there may be many local minima. A vast range of techniques, a treatment of which merits a textbook on its own, have been developed to address this issue, including for example the Adam optimizer (Kingma and Ba 2017).\n\n\n3.4.2 Stochastic Optimization\n\n\n3.4.3 Sequential Decision Problems\n\n\n3.4.4 High-Dimensional Optimization",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/optimization.html#further-reading",
    "href": "chapters/fundamentals/optimization.html#further-reading",
    "title": "3¬† Optimization üöß",
    "section": "Further reading",
    "text": "Further reading\n\nSutton and Barto (2018) provides a comprehensive introduction to reinforcement learning, which is broadly the study of sequential decision-making under uncertainty.\nPowell (2022) provides a comprehensive treatment of sequential decision-making under uncertainty, aiming at a unified framework\n\n\n\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. ‚ÄúAdam: A Method for Stochastic Optimization.‚Äù January 30, 2017. https://doi.org/10.48550/arXiv.1412.6980.\n\n\nPowell, Warren B. 2022. ‚ÄúSequential Decision Analytics and Modeling: Modeling with Python,‚Äù November. https://nowpublishers.com/Article/BookDetails/9781638280828.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. Cambridge, Massachusetts; London, England: MIT Press.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html",
    "href": "chapters/fundamentals/monte-carlo.html",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#learning-objectives",
    "href": "chapters/fundamentals/monte-carlo.html#learning-objectives",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nApply Monte Carlo methods to approximate integrals and expectations\nUse Markov Chain Monte Carlo (MCMC) techniques to estimate posterior distributions\nUnderstand parameter uncertainty propagation in decision-making\nImplement Monte Carlo simulation for climate risk assessment\n\nMonte Carlo is a powerful class of methods used to estimate the properties of a distribution by generating random samples from that distribution. Specifically, Monte Carlo methods are used to estimate the expected value of a function of random variables.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#overview",
    "href": "chapters/fundamentals/monte-carlo.html#overview",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nMonte Carlo methods are used to approximate integrals using sums. If we want to compute an expectation \\(\\mathbb{E}[f(X)]\\) where \\(X\\) has probability density \\(p(x)\\):\n\\[\n\\mathbb{E}[f(X)] = \\int f(x) p(x) dx\n\\]\nwe can approximate this using Monte Carlo: \\[\n\\mathbb{E}[f(X)] \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n\\] where \\(x_1, x_2, \\ldots, x_N\\) are samples drawn from \\(p(x)\\).\nThis is particularly powerful when \\(p(x)\\) is a complex posterior distribution that we can only sample from, not evaluate directly.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#motivation-bayesian-inference",
    "href": "chapters/fundamentals/monte-carlo.html#motivation-bayesian-inference",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.2 Motivation: Bayesian inference",
    "text": "4.2 Motivation: Bayesian inference\nBayesian inference provides a principled framework for updating our beliefs about parameters given observed data. The key insight is that parameters have probability distributions, not single point values.\n\n4.2.1 Bayes‚Äô theorem\nFor continuous parameters, Bayes‚Äô theorem states: \\[\np(\\theta | y) = \\frac{p(y | \\theta) p(\\theta)}{p(y)}\n\\]\nwhere: - \\(p(\\theta | y)\\) is the posterior distribution (what we want) - \\(p(y | \\theta)\\) is the likelihood (probability of data given parameters) - \\(p(\\theta)\\) is the prior distribution (our initial beliefs) - \\(p(y)\\) is the marginal likelihood (normalizing constant)\nSince \\(p(y)\\) doesn‚Äôt depend on \\(\\theta\\), we often work with: \\[\np(\\theta | y) \\propto p(y | \\theta) p(\\theta)\n\\]\n\n\n4.2.2 The challenge\nFor most real-world problems, the posterior distribution \\(p(\\theta | y)\\) cannot be computed analytically. This is where Monte Carlo methods become essential - we use them to draw samples from complex posterior distributions.\n\n\n4.2.3 Markov Chain Monte Carlo (MCMC)\nMCMC methods generate samples from posterior distributions by: 1. Starting at some initial parameter values 2. Proposing new parameter values 3. Accepting or rejecting proposals based on the posterior probability 4. Repeating to create a ‚Äúchain‚Äù of samples\nModern MCMC algorithms like the No-U-Turn Sampler (NUTS) use gradient information to efficiently explore high-dimensional parameter spaces.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#motivation-parameter-uncertainty",
    "href": "chapters/fundamentals/monte-carlo.html#motivation-parameter-uncertainty",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.3 Motivation: parameter uncertainty",
    "text": "4.3 Motivation: parameter uncertainty\nA fundamental challenge in climate risk assessment is that model parameters are uncertain. Consider a simple flood risk assessment problem:\n\nWe have a probability distribution for flood heights: \\(p(h)\\)\nWe have a damage function: \\(d(h)\\) (damages as a function of height)\nWe want the probability distribution of damages: \\(p(d)\\)\n\nMathematically: \\[\np(d) = \\int_h d(h) p(h) \\, dh\n\\]\n\n4.3.1 The challenge\nThis integral is often impossible to solve analytically, especially when: 1. We use realistic (non-analytical) damage functions 2. We want to account for parameter uncertainty in \\(p(h)\\) 3. We have multiple uncertain parameters\n\n\n4.3.2 Monte Carlo Solution\nInstead of solving the integral analytically: 1. Sample flood heights: \\(h_1, h_2, \\ldots, h_N \\sim p(h)\\) 2. Apply damage function: \\(d_i = d(h_i)\\) for each sample 3. Analyze the resulting damage samples: \\(\\{d_1, d_2, \\ldots, d_N\\}\\)\nThis approach generalizes to complex, multi-parameter problems where analytical solutions are impossible.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#monte-carlo-theory",
    "href": "chapters/fundamentals/monte-carlo.html#monte-carlo-theory",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.4 Monte Carlo Theory",
    "text": "4.4 Monte Carlo Theory\n\n4.4.1 Fundamental theorem\nIf \\(\\theta^s \\sim p(\\theta)\\), then by the Law of Large Numbers: \\[\n\\mathbb{E}[f(\\theta)] = \\int_{\\theta} f(\\theta) p(\\theta) d\\theta \\approx \\frac{1}{S} \\sum_{s=1}^S f(\\theta^s)\n\\]\nThis convergence is guaranteed as \\(S \\to \\infty\\), making Monte Carlo a robust approach for complex problems.\n\n\n4.4.2 Advantages over deterministic integration\nDeterministic approach: - Sample \\(\\theta\\) at regular grid points - Compute \\(f(\\theta)\\) at each point and sum - Problems: Curse of dimensionality, choice of grid spacing, boundary handling\nMonte Carlo approach: - Sample \\(\\theta\\) from the actual distribution \\(p(\\theta)\\) - Automatically focuses computational effort where probability is high - Benefits: Scales well to high dimensions, handles complex domains",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#motivation-resilience",
    "href": "chapters/fundamentals/monte-carlo.html#motivation-resilience",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.5 Motivation: resilience",
    "text": "4.5 Motivation: resilience\nMonte Carlo methods are essential for resilience analysis because they can handle: - Multiple uncertain hazards occurring simultaneously - Complex system interactions and cascading failures - Non-linear damage functions and thresholds - Rare but high-impact events",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#motivation-sensitivity-analysis",
    "href": "chapters/fundamentals/monte-carlo.html#motivation-sensitivity-analysis",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.6 Motivation: sensitivity analysis",
    "text": "4.6 Motivation: sensitivity analysis\nMonte Carlo methods excel at sensitivity analysis - understanding how uncertain inputs affect outputs.\n\n4.6.1 Parameter sensitivity\nBy systematically varying parameters and observing changes in Monte Carlo outputs, we can: 1. Identify critical parameters: Which uncertainties matter most? 2. Quantify sensitivity: How much does output uncertainty change with input uncertainty? 3. Guide data collection: Where would additional measurements be most valuable?\n\n\n4.6.2 Global sensitivity analysis\nUnlike local sensitivity (derivatives at a point), Monte Carlo enables global sensitivity analysis across the entire parameter space, capturing: - Non-linear relationships - Parameter interactions - Threshold effects - Tail behavior",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#bayesian-decision-theory-context",
    "href": "chapters/fundamentals/monte-carlo.html#bayesian-decision-theory-context",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.7 Bayesian Decision Theory Context",
    "text": "4.7 Bayesian Decision Theory Context\nRecall from decision theory: \\[\n\\mathbb{E}\\left[L(a, \\theta) \\right] = \\int_\\theta L(a, \\theta) p(\\theta) d\\theta\n\\] Where \\(\\theta\\) is a vector of parameters, \\(a\\) is some action or decision, and \\(L\\) is the loss function.\n\n\n\n\n\n\nWe previously called \\(\\theta\\) a ‚Äústate of the world‚Äù and \\(L\\) a ‚Äúreward function‚Äù.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#practical-problem-flood-risk-assessment",
    "href": "chapters/fundamentals/monte-carlo.html#practical-problem-flood-risk-assessment",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.8 Practical problem: flood risk assessment",
    "text": "4.8 Practical problem: flood risk assessment\nYou have been commissioned by a client to assess their exposure to flooding. Specifically, they want to know the probability distribution of annual flood damages at their property if they do not elevate or floodproof their building.\n\n\\(p(h)\\): probability distribution of annual maximum flood heights at their property\n\\(d(h)\\): flood damages as a deterministic function of flood height\n\\(p(d) = \\int_h d(h) p(h) \\, dh\\)\n\nWith this information, they can compute metrics like the expected annual damage, the 99th percentile annual damage, and the probability of any flood occurring that will help them make a decision.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#functions-of-random-variables-the-challenge",
    "href": "chapters/fundamentals/monte-carlo.html#functions-of-random-variables-the-challenge",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.9 Functions of random variables: the challenge",
    "text": "4.9 Functions of random variables: the challenge\nPlugging in a bounded logistic model for \\(d(h)\\) and our lognormal model for \\(h\\): \\[\n\\begin{align}\np(d) &= \\int_h d(h) p(h) \\, dh \\\\\n&= \\int_{-\\infty}^\\infty \\mathbb{I}[h &gt; 0] \\text{logistic}(h) \\mathcal{N}(h | \\mu, \\sigma^2) \\, dh\\\\\n&= \\int_0^\\infty \\text{logistic}(h) \\mathcal{N}(\\mu, \\sigma^2) \\, dh\\\\\n&= \\int_0^\\infty \\frac{1}{1 + \\exp(-k * (x - x0))} \\frac {1}{\\sigma {\\sqrt {2\\pi }}} \\exp \\left\\{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2} \\right\\} \\, dh\n\\end{align}\n\\]\n\n4.9.1 Limitations: analytic approach\nWe might be able to solve this analytically (Wolfram Alpha can‚Äôt‚Ä¶). But:\n\nNumerous simplifying assumptions and approximations.\nWhat if we want to use a different distribution?\nA different damage model?\n\n\n\n4.9.2 Monte Carlo Solution Strategy\nA deterministic strategy: 1. sample \\(h^s = 0, \\Delta h, 2\\Delta h, \\ldots, (S-1)\\Delta h\\) 2. compute \\(\\text{logistic}(h^s) \\mathcal{N}(h^s | \\mu, \\sigma^2)\\) at each point and sum 3. drawbacks: we have to go to \\(\\infty\\) and select \\(\\Delta h\\).\nA sampling strategy: 1. sample \\(h^1, h^2, \\ldots, h^S \\sim p(h)\\) ‚Äì which we can do because we have a model for \\(p(h)\\) 2. for each value: compute \\(\\mathbb{I}(h^s &gt; 0) \\text{logistic}(h^s)\\) and take the average 3. this converges to the correct expectation!",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#parameter-uncertainty-propagation",
    "href": "chapters/fundamentals/monte-carlo.html#parameter-uncertainty-propagation",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.10 Parameter uncertainty propagation",
    "text": "4.10 Parameter uncertainty propagation\n\n4.10.1 The core problem\nWe have been working with a single probability distribution for flood depths, which we computed by maximum likelihood.\nThese values are not precise. What happens if we consider the lognormal distribution with slightly different, but still plausible, parameters?\nWhat about the depth-damage parameters \\(x_0\\) and \\(k\\)?\n\n\n4.10.2 Parameter uncertainty impact\n\nUncertainties in our model parameters propagate to uncertainties in the things we care about.\nAs we collect more data, fewer combinations of parameters are consistent with observations\nDifferent parameter combinations can lead to substantially different risk assessments",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#bayesian-inference-with-mcmc",
    "href": "chapters/fundamentals/monte-carlo.html#bayesian-inference-with-mcmc",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.11 Bayesian Inference with MCMC",
    "text": "4.11 Bayesian Inference with MCMC\n\n4.11.1 Prior Information: A Simple Example\nEveryone is tested for CEVE543acitis, a rare and deadly disease: 1. It is known that 1 in 1,000 people have CEVE543acitis 2. The test is 99% accurate 3. Your test comes back positive. What is the probability that you have CEVE543acitis?\n\n\n4.11.2 Bayes‚Äô Rule: Discrete Event Version\n\\[\n\\Pr \\left\\{ \\theta | y \\right\\} = \\frac{\\Pr \\left\\{ y | \\theta \\right\\} \\Pr \\left\\{ \\theta \\right\\}}{\\Pr \\left\\{ y \\right\\}}\n\\]\nDefine \\(y\\) is getting a positive test result and \\(\\theta\\) is having the underlying condition. Note that we do not observe \\(\\theta\\) directly! Here \\(y=1\\) and we want to know \\(\\Pr\\left\\{\\theta = 1 \\mid y=1 \\right\\}\\).\nA naive application of maximum likelihood: \\(\\Pr\\left\\{y=1 \\mid \\theta=1 \\right\\} &gt; \\Pr\\left\\{y=1 \\mid \\theta=0 \\right\\}\\) so best estimate is \\(\\theta=1\\)\n\n\n4.11.3 Key Ideas for Bayesian Inference\n\nParameters have probability distributions, not single point values\nStart with some prior distribution for parameters\nGoal: what is the distribution of the parameters given the data?\n\n\n\n4.11.4 Bayes‚Äô Rule for Distributions\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}\n\\]\nIf we are drawing samples from a distribution, we can calculate up to a constant of proportionality and ‚Äì since \\(p(y)\\) doesn‚Äôt depend on \\(\\theta\\) ‚Äì we can usually ignore it.\n\\[\n\\overbrace{p(\\theta \\mid y)}^\\rm{posterior} \\propto \\underbrace{p(y \\mid \\theta)}_\\rm{likelihood} \\overbrace{p(\\theta)}^\\rm{prior}\n\\]\n\n\n4.11.5 Markov Chain Monte Carlo (MCMC)\n\nA class of methods for sampling from a probability distribution\nRandom walkers:\n\nStart at some value\nPropose a new value\nAccept or reject the new value based on some criteria\n\nRepeat to get a ‚Äúchain‚Äù of samples\n\n\n\n4.11.6 MCMC Limitations and Modern Solutions\n\nWorks great for a very simple problem\nComputation blows up in higher dimensions (p_accept gets very small)\nHave to code a new sampler for each problem\n\nModern samplers leverage gradients and clever tricks to draw better samples for harder problems.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/monte-carlo.html#value-of-bayesian-inference",
    "href": "chapters/fundamentals/monte-carlo.html#value-of-bayesian-inference",
    "title": "4¬† Monte Carlo Methods üöß",
    "section": "4.12 Value of Bayesian Inference",
    "text": "4.12 Value of Bayesian Inference\n\n4.12.1 Key Benefits\n\nDraw samples from tricky posteriors to compute expectations \\(\\mathbb{E}[f(\\theta)]\\)\nQuantify parametric uncertainty\n\nIn practice, sometimes this is a big deal and sometimes model structure uncertainties matter more\n\nForce us to specify a data generating process\nComputational methods fail loudly\n\n\n\n4.12.2 The Posterior as Compromise\nThe posterior is a compromise between the prior and the likelihood.\n\nBad priors lead to bad inferences\nThe choice of prior is subjective, which some people hate\nWe approach this in a principled manner\nLots of other steps are also subjective (choice of likelihood model, which data to use, problem framing, etc)\nFalse sense of objectivity is dangerous anyways!\n\n\n\n\n\n\n\nComputational examples\n\n\n\nFor detailed computational examples, see the companion files:\n\nBayesian Inference and MCMC: Computational Examples - MCMC sampling, prior/posterior analysis, Turing.jl usage\nParameter Uncertainty and Risk Assessment: Computational Examples - Monte Carlo simulation for flood risk, parameter uncertainty propagation, sensitivity analysis\n\nExamples include: - Flood risk assessment with uncertain parameters - Depth-damage function analysis - Parameter sensitivity studies - Return period analysis with uncertainty - Bayesian model specification and sampling - Prior predictive checks - MCMC diagnostics and trace plots",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html",
    "href": "chapters/fundamentals/ml-nonparametric.html",
    "title": "5¬† Machine Learning üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics - Optimization",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#learning-objectives",
    "href": "chapters/fundamentals/ml-nonparametric.html#learning-objectives",
    "title": "5¬† Machine Learning üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this chapter, students should be able to:\n\nApply fundamental supervised learning concepts (regression and classification) to frame typical hydroclimate hazard assessment problems, and understand the crucial steps of data preparation and model evaluation.\nUnderstand nonparametric and semiparametric methods for flexible modeling of climate data distributions.\nCritically evaluate the application of machine learning in hydroclimate hazard assessment literature (papers, reports, models, datasets), identifying key assumptions, limitations, and potential biases.\nUnderstand the basic principles and potential of more advanced machine learning techniques (deep learning for sequence data, probabilistic models for scenario generation) in the context of hydroclimate hazards, while also recognizing their inherent complexities and interpretability challenges.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#machine-learning-fundamentals",
    "href": "chapters/fundamentals/ml-nonparametric.html#machine-learning-fundamentals",
    "title": "5¬† Machine Learning üöß",
    "section": "5.1 Machine learning fundamentals",
    "text": "5.1 Machine learning fundamentals\nMachine learning has become an essential tool in hydroclimate hazard assessment, offering powerful methods for pattern recognition, prediction, and data analysis. However, critical engagement with ML-driven products is essential - understanding assumptions, limitations, and appropriate applications.\nThis chapter focuses on fundamental concepts and critical evaluation, with computational notebooks providing practical examples.\n\n5.1.1 Motivating Example\nConsider the fundamental machine learning problem: we want to make predictions about the value of \\(y\\) given some \\(x\\), where the true relationship is unknown.\nSuppose the true function is: \\[\nf(x) = 2x + x \\sin(2 \\pi x)\n\\]\nBut we only observe noisy data and don‚Äôt know the true functional form. How do we approximate \\(f\\) from the data?\nThis illustrates the core challenge of machine learning: learning relationships from limited, noisy observations to make predictions on new data.\n\n\n5.1.2 Function Approximation Approaches\n\n5.1.2.1 Parametric Methods\nParametric methods model the function \\(f\\) using parameters \\(\\theta\\). Finding \\(\\hat{f}\\) becomes equivalent to choosing appropriate \\(\\theta\\).\nLinear Regression Example: \\[\ny | X \\sim \\mathcal{N}(X^T \\beta, \\sigma^2 I)\n\\]\nThis is equivalent to: \\[\ny_i | X_i \\sim \\mathcal{N} \\left(\\sum_{j=1}^J X_{ij} \\beta_j, \\sigma^2 \\right)\n\\]\nPoint Estimation vs.¬†Bayesian Inference:\nWe can obtain point estimates rather than full posterior distributions: - Maximum Likelihood Estimate (MLE): \\(\\arg \\max_\\theta p(y | \\theta)\\) - Maximum a Posteriori Estimate (MAP): \\(\\arg \\max_\\theta p(\\theta | y)\\)\nPoint estimates are appropriate when: 1. We need just a plausible value of the parameters 2. We don‚Äôt need to carefully quantify parametric uncertainty 3. Computational efficiency is prioritized\n\n\n5.1.2.2 Nonparametric Methods\n\\(K\\) nearest neighbors (KNN) exemplifies nonparametric methods: find the \\(K\\) training examples closest to a given input and return the average output.\nImportant Note: ‚ÄúNonparametric‚Äù doesn‚Äôt mean no parameters‚Äî\\(K\\) is still a parameter that must be chosen!\nAdvantages: Flexible, makes few assumptions about functional form Disadvantages: Can be sensitive to local data density, curse of dimensionality\n\n\n\n5.1.3 Loss Functions\nWe need to define what constitutes a ‚Äúbest‚Äù approximation through loss functions that measure differences between predicted and actual values.\nCommon Loss Functions:\n\nMean Squared Error (MSE): \\(L(y, \\hat{y}) = (y - \\hat{y})^2\\)\n\nEmphasizes larger errors but sensitive to outliers\n\nMean Absolute Error (MAE): \\(L(y, \\hat{y}) = |y - \\hat{y}|\\)\n\nLess sensitive to outliers, non-differentiable at zero\n\nHuber Loss: Combines MSE and MAE with threshold parameter \\(\\delta\\): \\[\nL_\\delta(y, \\hat{y}) = \\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta \\left( |y - \\hat{y}| - \\frac{1}{2}\\delta \\right) & \\text{otherwise}\n\\end{cases}\n\\]\nQuantile Loss: Tailored to specific quantiles \\(\\tau\\), useful for asymmetric errors: \\[\nL_\\tau(y, \\hat{y}) = \\begin{cases}\n\\tau(y - \\hat{y}) & \\text{if } (y - \\hat{y}) &gt; 0 \\\\\n(\\tau - 1)(y - \\hat{y}) & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n5.1.4 Bias-Variance Tradeoff\nEvery model error can be decomposed as: \\[\n\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n\\]\nBias: How much on average are predicted values different from actual values? \\[\n\\text{Bias}(\\hat{f}(x)) = E[\\hat{f}(x) - f(x)]\n\\]\nVariance: How much does prediction vary between different model realizations? \\[\n\\text{Variance}(\\hat{f}(x)) = E[\\hat{f}(x)^2] - E[\\hat{f}(x)]^2\n\\]\nIrreducible Error: Noise inherent in real-world data that cannot be removed.\nKey Insight: Bayesian methods address this tradeoff through priors that add bias but often reduce variance.\n\n\n5.1.5 Regularization\nRidge Regression includes an L2 penalty to discourage overly complex models: \\[\nL(\\beta) = \\| Y - X^T \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2\n\\]\nLasso Regression uses an L1 penalty that can set coefficients to exactly zero: \\[\nL(\\beta) = \\| Y - X^T \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1\n\\]\nwhere \\(\\lambda\\) controls the regularization strength.\n\n\n5.1.6 Supervised Learning\nSupervised learning involves learning from labeled data (inputs and desired outputs). Given paired data \\(\\{(X_i, y_i) \\mid i = 1, 2, \\ldots, n\\}\\) where \\(X_i\\) are predictors and \\(y_i\\) are targets, the goal is to approximate a function \\(f\\) that provides good predictions on new data.\n\n5.1.6.1 Regression\nPredicting continuous hazard variables (e.g., flood depth, streamflow, precipitation amounts). The challenge is modeling relationships between predictors and continuous outcomes while handling uncertainty and avoiding overfitting.\n\n\n5.1.6.2 Classification\nPredicting categorical hazard outcomes (e.g., landslide occurrence, drought severity class, flood/no-flood). Often involves probability estimation rather than just binary decisions, which is crucial for risk assessment applications.\n\n\n\n5.1.7 Unsupervised Learning\nLearning patterns from data without explicit target variables. Common applications include dimensionality reduction, clustering similar climate patterns, and exploratory data analysis.\n\n\n5.1.8 Model Evaluation and Validation\n\n5.1.8.1 Hyperparameters\nMany ML models have nested parameters called ‚Äúhyperparameters‚Äù:\n\nParameters: Learned during model fitting (e.g., where to partition regions in trees)\nHyperparameters: Must be chosen before training (e.g., number of trees in a forest)\n\nHyperparameters are not optimized during model training and require separate tuning strategies.\n\n\n5.1.8.2 Cross-Validation\nKey idea: Evaluate models on data not used for fitting (out of sample).\nK-fold Cross-Validation Process: 1. Split data into \\(K\\) folds 2. For each fold \\(k = 1, \\ldots, K\\): - Fit model on all folds except the \\(k\\)-th - Evaluate model on the \\(k\\)-th fold 3. Average performance across all folds\nCross-validation reduces variance in performance estimates, but cross-validated estimates can still be biased due to hyperparameter overfitting.\n\n\n5.1.8.3 Train-Test Split\nFor unbiased performance assessment: 1. Split data into training (70-80%) and test (20-30%) sets - For spatially/temporally structured data, use structured splits 2. Fit model on training set (including cross-validation for hyperparameter tuning) 3. Final evaluation on test set as last step\nThis provides an honest estimate of model performance on truly unseen data.\n\n\n5.1.8.4 Grid Search\nSimple hyperparameter optimization approach: 1. Predefine a set of \\(S\\) hyperparameter combinations 2. For each combination, fit the model using cross-validation 3. Choose the best-performing model\nMore sophisticated approaches include random search and Bayesian optimization.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#nonparametric-methods-1",
    "href": "chapters/fundamentals/ml-nonparametric.html#nonparametric-methods-1",
    "title": "5¬† Machine Learning üöß",
    "section": "5.2 Nonparametric methods",
    "text": "5.2 Nonparametric methods\nFrom Bayes‚Äô rule, \\[\nf(y | \\vec{x}) = \\frac{f(y, \\vec{x})}{f(\\vec{x})}\n\\] if we can build reliable multivariate probability distributions, we can build a general framework for conditional distributions.\n\n5.2.1 Density Estimation\n\n\n\n\n\n\nCredit\n\n\n\nPulling some bits from https://vita.had.co.nz/papers/density-estimation.pdf, be sure to cite correctly.\n\n\nDensity estimation builds an estimate of some underlying probability density function using an observed data sample. Density estimation can either be parametric, where the data is from a known family, or nonparametric, which attempts to flexibly estimate an unknown distribution.\nA simple approach is a histogram (refer to grad class notes). The histogram requires two parameters: bin width and starting position of the first bin.\n\n\n\n\n\n\nREFER TO Ricardo Gutierrez-Osuna notes\n\n\n\nAnother approach is kernel density estimation (KDE), which uses a kernel function \\[\n\\hat{f}_{\\text{KDE}} (x) = \\frac{1}{n} \\sum_{i=1}^n K(\\frac{x - x_i}{h})\n\\] where \\(K\\) is the kernel function, \\(h\\) is the bandwidth, and \\(x_i\\) are the data points.\nThe main challenge to the kde approach is varying data density: regions of high data density could have small bandwidths, but regions with sparse data need large bandwidths.\n\n\n5.2.2 Nonparametric Regression\n\n\n5.2.3 Neighborhood Methods\n\n\n5.2.4 Bootstrapping\n\n\n5.2.5 Semi-parametric Methods",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#generalized-linear-models-glms",
    "href": "chapters/fundamentals/ml-nonparametric.html#generalized-linear-models-glms",
    "title": "5¬† Machine Learning üöß",
    "section": "5.3 Generalized linear models (GLMs)",
    "text": "5.3 Generalized linear models (GLMs)\nLinear regression assumes normally distributed errors and a linear relationship between predictors and the response. Generalized Linear Models extend this framework to other distributions and link functions, making them particularly useful for climate applications.\n\n5.3.1 Why Linear Models?\nThe linear relationship \\(y = ax + b\\) is often a strong assumption, not always physically justifiable, though frequently useful. A helpful way to think about linear models is as Taylor series representations of functions - they provide local approximations to more complex relationships.\n\n\n5.3.2 The GLM Framework\nGLMs consist of three components: 1. Random component: The probability distribution of the response variable 2. Systematic component: Linear combination of predictors \\(\\alpha + \\beta x_i\\) 3. Link function: Connects the systematic component to the expected response\n\n\n5.3.3 Binomial Regression: Forest Fire Example\nConsider modeling forest fire occurrence based on average summertime temperature. We have binary outcomes (fire occurred or not) and want to model the probability of occurrence.\nFor each data point, we use a Bernoulli distribution: \\[\ny_i \\sim \\mathrm{Bernoulli}(p_i)\n\\]\nThe challenge: we need \\(p_i \\in (0, 1)\\) but linear predictors \\(\\alpha + \\beta x_i \\in (-\\infty, \\infty)\\).\n\n5.3.3.1 Logit Link Function\nThe canonical link function is the logit: \\[\n\\begin{align}\n\\textrm{logit}(p_i) &= \\alpha + \\beta x_i \\\\\n\\log \\frac{p_i}{1 - p_i} &= \\alpha + \\beta x_i \\\\\np_i &= \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)}\n\\end{align}\n\\]\nThis maps the linear space \\((-\\infty, \\infty)\\) onto the probability space \\((0, 1)\\).\n\n\n5.3.3.2 Alternative Link Functions\nOther link functions exist, such as the probit link (inverse of standard normal CDF), popular in economics. The choice can affect tail behavior and interpretation.\n\n\n\n5.3.4 Poisson Regression: Wildlife Sightings Example\nFor count data (e.g., number of wildlife sightings, flood events), Poisson regression is appropriate: \\[\ny_i \\sim \\mathrm{Poisson}(\\lambda_i)\n\\]\nThe canonical link function is logarithmic: \\[\n\\log(\\lambda_i) = \\alpha + \\beta x_i\n\\]\nThis ensures \\(\\lambda_i &gt; 0\\) (required for Poisson) while allowing linear relationships on the log scale.\n\n\n5.3.5 Implementation in Julia\nGLMs can be implemented using Bayesian frameworks like Turing.jl:\n@model function logistic_regression(y::AbstractVector, x::AbstractVector)\n    Œ± ~ Normal(0, 1)\n    Œ≤ ~ Normal(0, 1)\n    for i in eachindex(y)\n        p = logistic(Œ± + Œ≤ * x[i])\n        y[i] ~ Bernoulli(p)\n    end\nend\n\n\n5.3.6 Other GLM Families\n\nNegative Binomial regression: For overdispersed count data\nRobust regression: Using t-distributions for heavy-tailed errors\nGamma regression: For positive continuous data with non-constant variance\n\n\n\n5.3.7 Semi-parametric Methods",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#machine-learning-practice",
    "href": "chapters/fundamentals/ml-nonparametric.html#machine-learning-practice",
    "title": "5¬† Machine Learning üöß",
    "section": "5.4 Machine learning practice",
    "text": "5.4 Machine learning practice\n\n5.4.1 Input Data and Preprocessing\n\nThe quality and preparation of input data is crucial for model performance.\nKnown issues with common hydroclimate datasets (e.g., biases, resolution limitations).\nHandling common data issues: Missing values, outliers, scaling.\n\n\n\n5.4.2 Feature Engineering\n\nCreating informative predictors from raw data.\n\n\n\n5.4.3 Feature Selection\n\nChoosing the most relevant variables for the model.\n\n\n\n5.4.4 Loss Functions\n\nExplain the concept of a loss function as a measure of model error during training.\nDiscuss common loss functions for regression (e.g., MSE, MAE) and their sensitivity to different error types.\nIntroduce loss functions for classification (e.g., binary cross-entropy).\nBriefly mention the importance of selecting a loss function aligned with the hazard prediction goal.\n\n\n5.4.4.1 Common Regression Metrics\n\nRMSE, MAE, \\(R^2\\), and their interpretation in a hazard context.\n\n\n\n5.4.4.2 Common Classification Metrics\n\nAccuracy, Precision, Recall, F1-score, Confusion Matrix, and their relevance to hazard prediction accuracy and reliability.\n\n\n\n\n5.4.5 Training and Evaluating Models for Generalization\n\nThe primary goal of model training and evaluation is to achieve robust predictive performance on unseen data, avoiding overfitting or underfitting (the bias-variance tradeoff).\nTraining, Validation, and Testing Sets:\n\nThe standard evaluation framework to assess generalization.\nThe validation set‚Äôs role in hyperparameter tuning and model selection.\nRobustly assessing out-of-sample performance using the test set.\n\nCross-Validation: A technique for more reliable estimation of generalization performance.\nRegularization: Techniques (e.g., L1, L2) to prevent overfitting by penalizing model complexity.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#tree-based-methods-and-ensemble-learning",
    "href": "chapters/fundamentals/ml-nonparametric.html#tree-based-methods-and-ensemble-learning",
    "title": "5¬† Machine Learning üöß",
    "section": "5.5 Tree-based methods and ensemble learning",
    "text": "5.5 Tree-based methods and ensemble learning\n\n5.5.1 Decision Trees\nDecision trees partition the predictor space into distinct regions and make constant predictions within each region. They are intuitive, interpretable, and handle both regression and classification tasks.\n\n5.5.1.1 Tree Construction\nThe algorithm uses recursive binary splitting: 1. Select a predictor \\(X_j\\) and cutpoint \\(s\\) that minimizes the loss function 2. Split the space into \\(\\{X | X_j &lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) 3. Repeat for each resulting region\nFor regression, the loss function is typically residual sum of squares (RSS): \\[\n\\sum_{j=1}^J \\sum_{i \\in R_j} \\left(y_i - \\hat{y}_j \\right)^2\n\\]\nFor classification, common loss functions include cross-entropy: \\[\nD = - \\sum_{k=1}^K p_{mk} \\log \\hat{p}_{mk}\n\\] where \\(\\hat{p}_{mk}\\) is the proportion of observations in region \\(m\\) that are in class \\(k\\).\n\n\n5.5.1.2 Bias-Variance Tradeoff in Trees\n\nDeep trees: Low bias, high variance (overfitting risk)\nShallow trees: High bias, low variance (underfitting risk)\n\n\n\n5.5.1.3 Pruning\nTo control overfitting, use cost complexity pruning: \\[\n\\text{Loss} = \\sum_{m=1}^{|T|} \\sum_{i: X_i \\in R_m} \\left(y_i - \\hat{y}_{R_m} \\right)^2 + \\alpha |T|\n\\]\nwhere \\(|T|\\) is the number of terminal nodes and \\(\\alpha\\) is the complexity parameter.\n\n\n\n5.5.2 Ensemble Methods\nThe key insight: combine many ‚Äúweak‚Äù learners into a ‚Äústrong‚Äù learner. Ensemble methods work better when the weak learners are less correlated.\n\n5.5.2.1 Bagging (Bootstrap Aggregating)\nProblem: Decision trees have high variance.\nSolution: Average predictions from multiple trees trained on bootstrap samples: \\[\n\\hat{f}_\\text{bag} = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{*b}(x)\n\\]\n\n\n5.5.2.2 Random Forests\nProblem: Bagged trees are highly correlated if there‚Äôs one dominant predictor.\nSolution: At each split, consider only a random subset of \\(m\\) predictors (typically \\(m \\approx \\sqrt{p}\\)).\nAlgorithm: 1. For each bootstrap sample, grow a tree 2. At each split, randomly select \\(m\\) predictors from \\(p\\) total 3. Choose the best split among these \\(m\\) predictors 4. Average predictions across all trees\n\n\n5.5.2.3 Boosting\nIdea: Sequentially fit trees to residuals from previous iterations.\nAlgorithm: 1. Initialize \\(\\hat{f}(x) = 0\\) and residuals \\(r_i = y_i\\) 2. For \\(b = 1, 2, \\ldots, B\\): - Fit tree \\(\\hat{f}^b\\) to \\((X, r)\\) with \\(d\\) splits - Update: \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\) - Update residuals 3. Output: \\(\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x)\\)\nKey parameters: number of trees \\(B\\), shrinkage rate \\(\\lambda\\), tree depth \\(d\\).\n\n\n\n5.5.3 Implementation in Julia\nUsing the DecisionTree.jl package:\n# Random Forest Regressor\nm = Int(ceil(sqrt(size(features, 2))))  # number of features per split\nmodel = RandomForestRegressor(n_subfeatures=m, n_trees=250)\nfit!(model, features, labels)\npredictions = DecisionTree.predict(model, features)",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#principal-component-analysis-pca",
    "href": "chapters/fundamentals/ml-nonparametric.html#principal-component-analysis-pca",
    "title": "5¬† Machine Learning üöß",
    "section": "5.6 Principal component analysis (PCA)",
    "text": "5.6 Principal component analysis (PCA)\n\n5.6.1 Motivation for Dimensionality Reduction\nHigh-dimensional data presents several challenges: 1. Difficult to visualize and interpret 2. Computational challenges (curse of dimensionality) 3. Redundant or correlated dimensions 4. Need to identify meaningful patterns\nClimate data is particularly high-dimensional: indexed by location and time, often with strong spatial correlation.\n\n\n5.6.2 PCA Theory\nGiven \\(n\\) observations and \\(p\\) features \\(X_1, X_2, \\ldots, X_p\\):\n\nFind a low-dimensional representation that captures maximum variation\nThe first principal component is the linear combination that maximizes variance: \\[Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\\] with constraint \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\nThe loading vector \\(\\phi_1 = (\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1})^T\\) defines the direction of maximum variation\n\n\n\n5.6.3 PCA as Optimization\nConsider representing data \\(X\\) with a linear model: \\[\nf(Z) = \\mu + \\phi_q Z\n\\]\nMinimize reconstruction error: \\[\n\\min \\sum_{i=1}^n \\| X_i - \\phi_q Z \\|_2^2\n\\]\nThe solution involves the singular value decomposition (SVD) of the empirical covariance matrix.\n\n\n5.6.4 Climate Data Applications\nFor space-time climate data, PCA provides: - Spatial patterns (EOFs - Empirical Orthogonal Functions) - Time series (principal component loadings) - Data compression and noise reduction\n\n\n5.6.5 Preprocessing Considerations\n\n5.6.5.1 Centering\nVariables should have mean zero. For climate data, work with anomalies: \\[\nx(t) = \\overline{x}(t) + x'(t)\n\\] where \\(\\overline{x}(t)\\) is climatology and \\(x'(t)\\) is the anomaly.\n\n\n5.6.5.2 Standardization\nOptional: standardize variables to unit variance when variables have different scales or units.\n\n\n5.6.5.3 Weighting\nFor spatial data, consider area weighting: \\(\\sqrt{\\cos(\\phi)}\\) where \\(\\phi\\) is latitude.\n\n\n\n5.6.6 Implementation\nJulia packages for PCA: - EmpiricalOrthogonalFunctions.jl: Specialized for climate data - MultivariateStats.jl: General-purpose PCA implementation\n\n\n5.6.7 Choosing Number of Components\nUse a scree plot to identify natural breaks in explained variance. Consider practical trade-offs between dimension reduction and information retention.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#deep-learning-and-advanced-methods",
    "href": "chapters/fundamentals/ml-nonparametric.html#deep-learning-and-advanced-methods",
    "title": "5¬† Machine Learning üöß",
    "section": "5.7 Deep learning and advanced methods",
    "text": "5.7 Deep learning and advanced methods\n\nThis section introduces more advanced machine learning techniques that have shown promise in complex data analysis.\n\n\n5.7.1 Deep Learning Background\n\nBriefly introduce the core idea of deep learning: hierarchical feature learning through multi-layered neural networks.\n\n\n\n5.7.2 Modeling Temporal Dependencies: Recurrent Neural Networks (RNNs) and LSTMs\n\nIntroduce the concept of processing sequential data, crucial for time-series hazard prediction.\nExplain the basic idea and utility of LSTMs for capturing long-range dependencies in temporal data (e.g., streamflow forecasting).\nBriefly discuss their complexity and data requirements.\n\n\n\n5.7.3 Probabilistic Models\n\nIntroduce the concept of models that can generate synthetic hazard data and quantify uncertainty.\nBriefly discuss GANs and diffusion models and their potential for creating realistic hazard scenarios and exploring uncertainty.\nAcknowledge the challenges in evaluating and interpreting these models.\n\n\n\n5.7.4 Image (Spatial) Models\n\nBriefly introduce Convolutional Neural Networks (CNNs) for analyzing spatial patterns in hazard-related data (e.g., satellite imagery for flood extent mapping).\n\n\n\n5.7.5 Spatiotemporal Models\n\nBriefly mention models that combine spatial and temporal analysis (e.g., using combinations of CNNs and RNNs for dynamic hazard prediction).\n\n\n\n5.7.6 Scientific Machine Learning\nThe field of scientific machine learning (SciML) typically refers to using ML to solve systems of differential equations (Rackauckas et al. 2020)",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#conclusion",
    "href": "chapters/fundamentals/ml-nonparametric.html#conclusion",
    "title": "5¬† Machine Learning üöß",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize the fundamental ML concepts and their application to hazard assessment.\nRe-emphasize the importance of critical thinking when engaging with ML-driven hazard products.\nPoint to the computational notebooks for practical examples and deeper exploration.\nBriefly preview the application of these concepts in subsequent chapters.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#further-reading",
    "href": "chapters/fundamentals/ml-nonparametric.html#further-reading",
    "title": "5¬† Machine Learning üöß",
    "section": "Further reading",
    "text": "Further reading\n\nThere are numerous books on modern deep learning approaches, but Bishop and Bishop (2024) is an accessible and comprehensive introduction\nJames et al. (2013) provides a clear overview of traditional statistical learning methods, mostly predating the recent deep learning boom, that provides a conceptual link between statistics and machine learning\nThuerey et al. (2024) covers modern techniques for physics-based deep learning, covering topics like neural operators, diffusion models, physics-informed neural networks, and related topics. These topics would nicely supplement the material in this book, but are not covered here except to note their potential relevance and applications.\n\n\n\n\n\nBishop, Christopher M., and Hugh Bishop. 2024. Deep Learning: Foundations and Concepts. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-45468-4.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York.\n\n\nRackauckas, Christopher, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. 2020. ‚ÄúUniversal Differential Equations for Scientific Machine Learning.‚Äù 2020. https://doi.org/10.48550/ARXIV.2001.04385.\n\n\nThuerey, N., B. Holzschuh, P. Holl, G. Kohl, M. Lino, Q. Liu, P. Schnell, and F. Trost. 2024. Physics-Based Deep Learning. https://physicsbaseddeeplearning.org.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Machine Learning üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html",
    "href": "chapters/fundamentals/correlation-dimensionality.html",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics - Machine Learning and Nonparametric Methods",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#learning-objectives",
    "href": "chapters/fundamentals/correlation-dimensionality.html#learning-objectives",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nModel and interpret spatial dependence in climate fields\nPerform kriging and other geostatistical interpolations for mapping hazards\nUnderstand the fundamentals of spatial autocorrelation and variogram analysis\nApply time series analysis methods to detect trends and patterns in climate data\nUse dimension reduction techniques for high-dimensional climate datasets\nIntegrate spatial and temporal methods for spatiotemporal climate analysis",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#spatial-statistics",
    "href": "chapters/fundamentals/correlation-dimensionality.html#spatial-statistics",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "6.1 Spatial Statistics",
    "text": "6.1 Spatial Statistics\n\n6.1.1 Spatial Autocorrelation and Variograms\n\n\n6.1.2 Interpolation Methods\n\n\n6.1.3 Kriging\n\n(ordinary, universal)\n\n\n\n6.1.4 Gaussian Processes in 1D\n\n\n6.1.5 Gaussian Processes in Multiple Dimensions",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#time-series-analysis",
    "href": "chapters/fundamentals/correlation-dimensionality.html#time-series-analysis",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "6.2 Time Series Analysis",
    "text": "6.2 Time Series Analysis\n\n6.2.1 Autocorrelation Functions\n\n\n6.2.2 Frequency Analysis\n\n\n6.2.3 Wavelets\n\n\n6.2.4 Autoregressive Models\nSometimes used in practice, but most useful as a didactic tool.\n\n\n6.2.5 Trends\nWe are often interested in the following sorts of trends:\n\nChanging Mean\n\nShifts\nSmooth Changes\n\nMonotonic\nCyclical\n\n\nChanging Variance\nChanging Event Frequency\nChanging Seasonality\n\nProbability Distribution of the Process changes slowly with time: due to identifiable or unknown causes\nKey questions:\n\nis change ‚Äúnatural‚Äù or can it be attributed to specific human activity?\ndid change in recording method or station location change the ‚Äúrecord‚Äù\nare policies working?\nif trend is removed, are underlying causal factors revealed?\nare time series models valid if process has trends?\n\n\n6.2.5.1 Example\nTODO: add the Folosm River data with a vertical line at 1945\nThis is the annual maximum flood time series from the Fair Oaks station on the American River above Sacramento. Folsom Dam was built as a multi-purpose structure in 1945. The system of dam and dikes was designed to provide Sacramento with flood protection at a level between the 200- to 500-year return period flood as estimated from the 1912 to 1942 annual maximum flood data. There have now been 6 annual maximum floods larger than the largest observed in the pre-dam construction period. As a result, if the recent data is used, the estimated flood protection is below FEMA‚Äôs nominal level of 100 years, which makes the property in Sacramento uninsurable for flood risk. Moving windows of two different lengths are used to estimate the 100-year flood using the Log-Normal distribution. The estimate is reported at the center of each moving window. The 21-year moving window shows nearly a four-fold increase in the magnitude of the 100-year flood over time, with most of the increase coming in the period after 1940. The 51-year window shows a 1.8-fold increase in the estimated 100-year flood magnitude, with a nearly monotonic increase starting post-dam construction. Are the changes just due to chance? Do they reflect changes in the basin, leading to higher floods with the same rainfall? Do they reflect changes in global climate?\n\n\n6.2.5.2 Mann-Kendall\nThere‚Äôs an example in Helsel et al. (2020) for the Potomac River that we can work through\n\n\n\n6.2.6 Other Time Series Models\n\nARIMA\nGeneralized Additive Models\nState Space Models\nLSTMs\nand beyond",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#high-dimensional-methods",
    "href": "chapters/fundamentals/correlation-dimensionality.html#high-dimensional-methods",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "6.3 High-Dimensional Methods",
    "text": "6.3 High-Dimensional Methods\n\n6.3.1 Covariance and Correlation\n\n\n6.3.2 Structured Variability\n\ne.g., model grid\n\n\n\n6.3.3 Dimension Reduction\n\n\n\n\n\n\nMotivate with some example scatterplots\n\n\n\nWhat is the true dimensionality of this data?\n\n\n\nGoal: summarize data with many (\\(p\\)) variables by a smaller set of \\(k\\) derived (synthetic, composite) variables\nStart with \\(A_{n \\times p}\\), \\(n\\) samples of \\(p\\) variables. Get \\(X_{n \\times k}\\), \\(k &lt; p\\).\nYou will lose some information in \\(A\\) that is not in \\(X\\)\nbalancing act betwen clarity of represenation (ease of understanding) vs loss of relevant information\n\n\n6.3.3.1 Principal Component Analysis (PCA)\nOverview and Motivation:\nHigh-dimensional data presents several challenges: - Difficult to visualize and interpret - Contains redundant or irrelevant dimensions - Computational challenges in high dimensions (‚Äúcurse of dimensionality‚Äù) - Need to identify meaningful patterns in complex datasets\nPCA addresses these challenges by finding low-dimensional representations that preserve as much variation as possible in the original data.\nMathematical Foundation:\nFor \\(n\\) observations and \\(p\\) features \\(X_1, X_2, \\ldots, X_p\\), PCA finds a low-dimensional representation that captures maximum variance.\nThe first principal component is the linear combination of features that maximizes variance: \\[Z_1  = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\\]\nwith normalization constraint: \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\nThe loading vector \\(\\phi_1 = (\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1})^T\\) defines a direction in feature space along which the data vary the most.\nPCA as an Optimization Problem:\nWe can formulate PCA as finding the best linear approximation to the data: \\[f(Z) = \\mu + \\phi_q Z\\]\nwhere: - \\(\\mu\\) is a location vector (typically zero for centered data) - \\(\\phi_q\\) is a \\(p \\times q\\) matrix with \\(q\\) orthogonal unit vectors as columns - \\(Z\\) is a \\(q\\)-dimensional vector of coefficients or scores\nThe optimization problem becomes: \\[\\min \\sum_{i=1}^n \\| X_i - \\phi_q Z_i \\|_2^2\\]\nConnection to Singular Value Decomposition (SVD):\nThe solution can be written as a singular value decomposition of the empirical covariance matrix. This assumes that variance is an appropriate measure of variability in the data.\nImportant caveat: This assumption may be poor when: - Variables have very different scales - Outliers are present - Non-Gaussian distributions where variance doesn‚Äôt capture the relevant structure\nUniqueness and Interpretation:\nEach principal component loading vector is unique up to a sign flip. The choice of sign is arbitrary and doesn‚Äôt affect the interpretation.\nClimate Science Applications:\nIn climate science, we commonly work with space-time data where PCA components can be interpreted as:\n\nSpatial patterns (\\(Z\\)): The ‚ÄúEOFs‚Äù (Empirical Orthogonal Functions) or principal components represent dominant spatial patterns of variability\nTime series (\\(\\phi\\)): The loadings represent time evolution of each spatial pattern\nData reconstruction: Can reconstruct the full field at time \\(t\\) from the EOFs and their time series\n\nPreprocessing Considerations:\nCentering: - Variables should have mean zero for standard PCA - Variance is the average squared deviation from the mean - Centered and non-centered data will have different covariance matrices\nStandardization: - Optional: standardize variables to unit variance - Important when variables have different units or scales - Equivalent to weighting variables by inverse of their variances\nClimate Anomalies: In climate science, it‚Äôs common to decompose time series into climatology and anomalies: \\[x(t) = \\overline{x}(t) + x'(t)\\] where \\(\\overline{x}(t)\\) is the climatology and \\(x'(t)\\) is the anomaly.\nCommon approaches for defining climatology: 1. Time-mean over a reference period 2. Seasonal climatology computed separately for each month or season (DJF, MAM, JJA, SON) 3. Harmonic climatology with seasonal cycle removed using sinusoidal terms\nSpatial Data Considerations:\nWeighting: - Area weighting: Use \\(\\sqrt{\\cos(\\phi)}\\) where \\(\\phi\\) is latitude to weight by grid cell area - Variance weighting: Weight by inverse variance (equivalent to standardization) - Physical considerations: Weight variables based on measurement uncertainty or physical importance\nModel Selection:\nHow Many Components to Retain? - Scree plot: Look for ‚Äúelbow‚Äù where eigenvalues level off - Cumulative variance: Retain components explaining desired percentage (e.g., 90%) - Cross-validation: Choose number based on out-of-sample performance - Physical interpretation: Retain components with clear physical meaning\nComputational Implementation:\nJulia Packages: - EmpiricalOrthogonalFunctions.jl: Climate-focused PCA/EOF analysis, based on Python‚Äôs EOFS library - MultivariateStats.jl: General-purpose multivariate statistics including PCA\nPractical Workflow:\nusing EmpiricalOrthogonalFunctions\n\n# Assume data matrix with time √ó space structure\nsolver = EmpiricalOrthogonalFunctions.Eof(data)\n\n# Get EOFs (spatial patterns)\neofs = solver.eofs()\n\n# Get principal components (time series)  \npcs = solver.pcs()\n\n# Get explained variance\nvariance = solver.varianceFraction()\nExtensions and Advanced Methods:\nProbabilistic PCA: Provides probabilistic framework with uncertainty quantification Robust PCA: Less sensitive to outliers and non-Gaussian distributions\nSparse PCA: Produces interpretable loadings with many zero entries Canonical Correlation Analysis: Analyzes relationships between two sets of variables\nClimate Applications:\nMode Analysis: Identify dominant climate variability patterns (ENSO, NAO, etc.) Model Evaluation: Compare spatial patterns between observations and climate models Dimension Reduction: Reduce high-dimensional climate model output for analysis Data Compression: Efficiently store and transmit large climate datasets\nFurther Reading:\n\nSVD Mathematical Overview by Steven Brunton provides conceptual SVD overview\nMIT Computational Thinking PCA Lecture\nChapter 10.2 of James et al. (2013) for statistical learning perspective\nNCAR EOF Tutorial for climate applications\n\n\n\n\n6.3.4 Copulas\n\n\n6.3.5 Deep Models\nMany generative AI models are fundamentally sampling from high-dimensional conditional distributions. Handling the high dimensionality of these distributions is a key challenge.\n\nGAN\ncGAN\nDiffusion Models",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#spatiotemporal-integration",
    "href": "chapters/fundamentals/correlation-dimensionality.html#spatiotemporal-integration",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "6.4 Spatiotemporal Integration",
    "text": "6.4 Spatiotemporal Integration\n\n6.4.1 Spatiotemporal Covariance Structures\n\n\n6.4.2 Spatiotemporal Kriging\n\n\n6.4.3 State-Space Models for Climate Data\n\n\n6.4.4 Climate Field Reconstruction\n\n\n6.4.5 Empirical Orthogonal Functions (EOFs)\n\n\n6.4.6 Machine Learning for Spatiotemporal Climate Data",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#applications-to-climate-data",
    "href": "chapters/fundamentals/correlation-dimensionality.html#applications-to-climate-data",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "6.5 Applications to Climate Data",
    "text": "6.5 Applications to Climate Data\n\n6.5.1 Gridded Climate Data Analysis\nAlmost all the high-dimensional data we deal with in climate science are spatial (e.g., gridded climate model output, reanalysis products, satellite observations).\n\n\n6.5.2 Climate Mode Analysis\n\n\n6.5.3 Extreme Event Detection in Spatiotemporal Fields\n\n\n6.5.4 Model Evaluation and Intercomparison",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#further-reading",
    "href": "chapters/fundamentals/correlation-dimensionality.html#further-reading",
    "title": "6¬† Correlation and Dimensionality üöß",
    "section": "Further reading",
    "text": "Further reading\n\n\n\n\nHelsel, Dennis R., Robert M. Hirsch, Karen R. Ryberg, Stacey A. Archfield, and Edward J. Gilroy. 2020. Statistical Methods in Water Resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Correlation and Dimensionality üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html",
    "href": "chapters/fundamentals/model-comparison.html",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics - Optimization (maximum likelihood estimation) - Monte Carlo Methods (Bayesian inference)",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#learning-objectives",
    "href": "chapters/fundamentals/model-comparison.html#learning-objectives",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:\n\nApply graphical diagnostic methods to assess model fit quality\nUse information criteria (AIC, DIC, BIC) for quantitative model comparison\nUnderstand the relationship between model selection and predictive accuracy\nRecognize the subjective nature of model selection and make transparent choices",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#overview",
    "href": "chapters/fundamentals/model-comparison.html#overview",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nModel validation and comparison are critical steps in any statistical analysis. We need methods to:\n\nAssess model fit: Does our model adequately capture the patterns in the data?\nCompare alternatives: Which model should we choose among competing options?\nPredict future observations: How well will our model perform on new data?\n\nThese challenges are particularly acute in climate science because Earth systems are high-dimensional, multi-scale, and nonlinear.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#graphical-diagnostic-methods",
    "href": "chapters/fundamentals/model-comparison.html#graphical-diagnostic-methods",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "7.2 Graphical diagnostic methods",
    "text": "7.2 Graphical diagnostic methods\n\n7.2.1 The Power of Visualization\nBefore diving into numerical criteria, graphical methods provide intuitive ways to assess model performance. ‚ÄúVibe checks with plots‚Äù - as they say - can be more informative than numerical summaries alone.\nKey diagnostic plots include:\n\n7.2.1.1 Histogram and Density Plot\n\nWhat we plot: Histogram of the data and the probability density function\nIdeal case: The histogram and the PDF appear to show the same distribution\nWarnings: Systematic deviations between empirical and fitted distributions\nLimitations: Hard to learn much about the tails of the distribution from this plot\n\n\n\n7.2.1.2 Probability Plot (P-P Plot)\n\nWhat we plot: Empirical CDF (1 - AEP) against the fitted distribution‚Äôs CDF\nIdeal case: A straight line, indicating perfect agreement between the empirical CDF and the fitted CDF\nWarnings: Curvature or systematic deviations from the line, especially in the tails\nLimitations: Sampling uncertainty affects interpretation\n\n\n\n7.2.1.3 Q-Q Plot\n\nWhat we plot: Quantiles (i.e., return levels) of the data against quantiles of the fitted distribution\nIdeal case: A straight line through the data points\nWarnings: Curvature or systematic deviations from the line, especially in the tails\nLimitations: Sampling uncertainty, especially for extreme quantiles\n\n\n\n7.2.1.4 Calibration Histogram\nFor spatial analyses with many locations: - What we plot: Histogram where each observation is the observed quantile of the data, given the conditional model at that location/year - Ideal case: A uniform distribution - Warnings: Systematic deviations from uniformity - Limitations: Aggregating over sites can hide local issues\n\n\n\n7.2.2 Model Adequacy vs Perfect Fit\nThe goal is not to find a ‚Äúperfect‚Äù model (which doesn‚Äôt exist), but rather a model that is: - Adequate for the intended purpose - Interpretable given the context - Robust to reasonable changes in assumptions - Predictively useful for decision-making\nRemember: You cannot look at a single criterion and decide whether a model is ‚Äúgood‚Äù or not.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#information-criteria",
    "href": "chapters/fundamentals/model-comparison.html#information-criteria",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "7.3 Information criteria",
    "text": "7.3 Information criteria\n\n7.3.1 The Fundamental Problem\nWe want to make probabilistic predictions about unobserved data \\(\\tilde{y}\\). The challenge is balancing: - Complexity: More parameters can better fit observed data - Overfitting: Complex models may perform poorly on new data - Parsimony: Simpler models are more interpretable and robust\n\n\n7.3.2 Predictive Accuracy Framework\nWe define predictive performance using a utility function, commonly the log predictive density: \\[\n\\overline{u}(M) = \\mathbb{E}[\\log p(\\tilde{y} | D, M)] = \\int p_t(\\tilde{y}) \\log p(\\tilde{y} | D, M) d\\tilde{y}\n\\] where \\(p_t(\\tilde{y})\\) is the true data generating distribution (unknown!).\nMaximizing this expected utility is equivalent to minimizing the Kullback-Leibler divergence between our model and the true data-generating process.\n\n7.3.2.1 Kullback-Leibler Divergence\nThe KL divergence measures how similar two distributions are: \\[\nD_\\text{KL} (P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left[ \\frac{P(x)}{Q(x)} \\right]\n\\]\nOne interpretation of \\(D_\\text{KL} (P \\parallel Q)\\) is the measure of information gained by revising one‚Äôs beliefs from the prior distribution \\(Q\\) to the posterior distribution \\(P\\). Another interpretation is the amount of information lost when \\(Q\\) is used to approximate \\(P\\).\n\n\n7.3.2.2 Working with Estimates\nIn practice, we don‚Äôt know the true distribution, so we approximate the log pointwise predictive density (lppd): \\[\n\\begin{align}\n\\text{lppd} &= \\log \\prod_{i=1}^N p_\\text{post}(y_i) = \\sum_{i=1}^N \\log \\int p(y_i | \\theta) p_\\text{post} (\\theta) d \\theta \\\\\n&\\approx \\sum_{i=1}^N \\log \\left[ \\frac{1}{S} \\sum_{s=1}^S p(y_i | \\theta^s) \\right]\n\\end{align}\n\\] where we have approximated the posterior with \\(S\\) simulations from the posterior (e.g., using MCMC).\nThe LPPD of observed data \\(y\\) is an overestimate of the expected LPPD for future data, which is why information criteria include bias corrections.\n\n\n\n7.3.3 Akaike Information Criterion (AIC)\nFor models with \\(k\\) parameters estimated by maximum likelihood: \\[\n\\text{AIC} = 2k - 2\\ln\\hat{\\mathcal{L}}\n\\]\nwhere \\(\\hat{\\mathcal{L}}\\) is the maximized likelihood.\nKey assumptions: - Parameters are asymptotically normal - Residuals are independent given parameter estimates - Sample size is large relative to number of parameters\n\n\n7.3.4 Deviance Information Criterion (DIC)\nExtends AIC to Bayesian settings by: 1. Replacing MLE with posterior mean: \\(\\hat{\\theta}_{\\text{Bayes}} = \\mathbb{E}[\\theta | y]\\) 2. Using data-based bias correction for effective parameters\n\\[\n\\text{DIC} = -2\\log p(y | \\hat{\\theta}_{\\text{Bayes}}) + 2p_{\\text{DIC}}\n\\]\n\n\n7.3.5 Bayesian Information Criterion (BIC)\nTakes a different approach, approximating the marginal likelihood: \\[\n\\text{BIC} = k\\ln(n) - 2\\ln\\hat{\\mathcal{L}}\n\\]\nAssuming a ‚Äútrue model‚Äù exists, BIC tends to select simpler models than AIC.\n\n\n7.3.6 Significance Criteria\nUse Null Hypothesis Significance Testing (NHST) to decide whether to include a variable. For example, should we add a trend term in our regression?\n\nForm a null hypothesis: \\(\\beta = 0\\)\nTest statistics ‚áí \\(p\\)-value\nIf \\(p &lt; \\alpha\\) then use \\(M_2\\) else use \\(M_1\\)\n\nNote that: - This is equivalent to Bayes factor in certain contexts - Still assumes existence of a true model (hence the many problems with NHST) - This is widely used in practice, often without justification",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#model-selection-philosophy",
    "href": "chapters/fundamentals/model-comparison.html#model-selection-philosophy",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "7.4 Model selection philosophy",
    "text": "7.4 Model selection philosophy\n\n7.4.1 No Magic Numbers\nCritical insight: You cannot look at a single criterion and decide whether a model is ‚Äúgood‚Äù or not. Model selection involves subjective choices about: - Which criteria to prioritize - How to balance fit vs complexity - What constitutes ‚Äúadequate‚Äù performance - Which aspects of the data are most important to capture\n\n\n7.4.2 Transparency Over False Objectivity\nRather than pretending model selection is purely objective: 1. Make assumptions explicit so others can follow and critique 2. Consider multiple criteria and understand their trade-offs 3. Use domain knowledge to inform choices 4. Test sensitivity to key assumptions",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#model-combination-and-ensemble-methods",
    "href": "chapters/fundamentals/model-comparison.html#model-combination-and-ensemble-methods",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "7.5 Model combination and ensemble methods",
    "text": "7.5 Model combination and ensemble methods\n\n7.5.1 Bayesian Model Averaging\nInstead of selecting a single ‚Äúbest‚Äù model, we can average across models: \\[\np(\\tilde{y} | D) = \\sum_{\\ell=1}^L p(\\tilde{y} | D, M_\\ell) p(M_\\ell | D)\n\\]\nThis approach: - Accounts for model uncertainty - Often improves predictive performance - Provides more realistic uncertainty estimates\n\n\n7.5.2 Model Stacking\nAlternative ensemble approach that optimizes predictive performance by finding optimal weights for combining models without assuming any single model is ‚Äútrue.‚Äù\n\n\n\n\n\n\nComputational examples\n\n\n\nFor detailed computational examples of model validation and comparison, see the companion file: Model Validation Examples: Diagnostic Plots and Information Criteria.\nExamples include: - Extreme value model fitting with diagnostic plots (using Extremes.jl) - Extremes.histplot(), Extremes.probplot(), Extremes.qqplot(), Extremes.qqplotci(), and Extremes.diagnosticplots() functions - Information criteria comparison across model families - Bayesian model averaging implementation - Model validation for Houston precipitation data and HOUSTON HOBBY AP station",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#further-reading",
    "href": "chapters/fundamentals/model-comparison.html#further-reading",
    "title": "7¬† Model Validation and Comparison üöß",
    "section": "Further reading",
    "text": "Further reading\nFor more accessible discussion, see Chapter 7 of McElreath (2020). For more technical treatment, see Piironen and Vehtari (2017). For practical guidance on extreme value model selection, see Coles (2001).\n\n\n\n\nColes, Stuart. 2001. An Introduction to Statistical Modeling of Extreme Values. Springer Series in Statistics. London: Springer.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Texts in Statistical Science Series. Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nPiironen, Juho, and Aki Vehtari. 2017. ‚ÄúComparison of Bayesian Predictive Methods for Model Selection.‚Äù Statistics and Computing 27 (3): 711‚Äì35. https://doi.org/10.1007/s11222-016-9649-y.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Model Validation and Comparison üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html",
    "href": "chapters/hazard/extremes.html",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics - Fundamentals of Climate Science",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#learning-objectives",
    "href": "chapters/hazard/extremes.html#learning-objectives",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:\n\nUnderstand the motivation for extreme value theory in climate risk assessment\nDistinguish between block maxima and peak-over-threshold approaches\n\nSelect and fit appropriate extreme value distributions (GEV, GPD)\nCalculate return periods, return levels, and exceedance probabilities\nQuantify and interpret different sources of uncertainty in extreme value analysis\nRecognize the challenges posed by non-stationarity and climate change\nApply extreme value methods to real-world case studies",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#overview",
    "href": "chapters/hazard/extremes.html#overview",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nExtreme value theory (EVT) provides the statistical foundation for analyzing rare, high-impact events.\n\n8.1.1 Applications\nKey applications include:\n\nEngineering design: Infrastructure sizing for extreme loads\nEmergency management: Planning for rare but catastrophic events\n\nRegulation: Setting safety standards and building codes\nInsurance: Pricing catastrophic risk and managing tail exposures\nFinancial risk: Managing extreme market movements and operational risks\n\n\n\n8.1.2 Variables of Interest\nCommon applications in climate risk include: - Streamflow extremes for flood risk assessment - Precipitation rates and totals for stormwater design - Wind speeds for structural engineering - Temperature extremes for energy planning and public health\n\n\n8.1.3 Why Extremes Are Challenging\nExtremes present unique statistical challenges:\n\nRarity: By definition, we have limited data on extreme events\nExtrapolation: Need to estimate probabilities beyond observed range\n\nHigh stakes: Errors in extreme value estimates have large consequences\nFundamental challenge: Extrapolation is inherently difficult\n\nKey sources of uncertainty:\n\nParametric uncertainty: Multiple parameter values consistent with data\nModel structure uncertainty: Different distributional assumptions\nSampling uncertainty: Finite data for rare event estimation\nNon-stationarity: Climate change affects extreme value statistics",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#theoretical-frameworks",
    "href": "chapters/hazard/extremes.html#theoretical-frameworks",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.2 Theoretical Frameworks",
    "text": "8.2 Theoretical Frameworks\nTwo primary approaches exist for extreme value analysis:",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#block-maxima-approach",
    "href": "chapters/hazard/extremes.html#block-maxima-approach",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.3 Block Maxima Approach",
    "text": "8.3 Block Maxima Approach\n\n8.3.1 Methodology\nBlock maxima divides data into blocks (typically years) and models the maximum in each block:\n\nDefine blocks: Usually annual (but definition of ‚Äúyear‚Äù matters)\nExtract maxima: One extreme value per block\nModel distribution: Fit theoretical distribution to block maxima\n\n\n\n8.3.2 Advantages and Disadvantages\nAdvantages: - Easier to communicate and implement - Flexible modeling frameworks available - Direct connection to return periods - Well-established theoretical foundation\nDisadvantages: - Timing of extremes within blocks not captured - Multiple extremes in one block ignored - Sometimes block maximum not particularly ‚Äúextreme‚Äù - Potential waste of information\n\n\n8.3.3 Generalized Extreme Value (GEV) Distribution\nThe GEV distribution is the natural choice for block maxima. For properly standardized maxima, the distribution has cumulative distribution function:\n\\[F(x) = \\exp\\left\\{-\\left[1 + \\xi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\right]^{-1/\\xi}\\right\\}\\]\nwhere: - \\(\\mu\\): location parameter (center of distribution) - \\(\\sigma &gt; 0\\): scale parameter (spread of distribution) - \\(\\xi\\): shape parameter (tail behavior)\n\n\n8.3.4 Shape Parameter Interpretation\nThe shape parameter \\(\\xi\\) determines tail behavior:\n\n\\(\\xi &gt; 0\\) (Fr√©chet): Heavy tails, no upper bound\n\\(\\xi = 0\\) (Gumbel): Light tails, unbounded but exponentially decreasing\n\\(\\xi &lt; 0\\) (Weibull): Light tails, finite upper bound at \\(\\mu - \\sigma/\\xi\\)\n\nCritical insight: Shape parameter estimation is challenging but crucial for extrapolation.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#peak-over-threshold-pot-approach",
    "href": "chapters/hazard/extremes.html#peak-over-threshold-pot-approach",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.4 Peak-Over-Threshold (POT) Approach",
    "text": "8.4 Peak-Over-Threshold (POT) Approach\n\n8.4.1 Methodology\nPeak-over-threshold models all events exceeding a chosen threshold:\n\nDefine threshold \\(u\\): Values above this level considered ‚Äúextreme‚Äù\nExtract exceedances: All values \\(x_i &gt; u\\)\nModel excesses: Fit distribution to \\(y_i = x_i - u\\)\nModel arrivals: Separately model frequency of threshold exceedances\n\n\n\n8.4.2 Advantages and Disadvantages\nAdvantages: - Focuses on meaningful events regardless of timing - Uses more extreme data than block maxima approach - Can capture multiple extremes per time period - More efficient use of available extreme value information\nDisadvantages:\n- Threshold selection is subjective and critical - Arrival process modeling adds complexity - Potential dependence between threshold exceedances\n\n\n8.4.3 Generalized Pareto Distribution (GPD)\nFor threshold excesses \\(Y = X - u | X &gt; u\\), the GPD is the natural choice:\n\\[F(y) = 1 - \\left(1 + \\xi \\frac{y}{\\sigma}\\right)^{-1/\\xi}\\]\nwhere \\(y \\geq 0\\), \\(\\sigma &gt; 0\\), and: - \\(\\sigma\\): scale parameter\n- \\(\\xi\\): shape parameter (same interpretation as GEV)\nConnection to GEV: GPD and GEV have the same shape parameter by theoretical construction.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#terminology-and-key-concepts",
    "href": "chapters/hazard/extremes.html#terminology-and-key-concepts",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.5 Terminology and Key Concepts",
    "text": "8.5 Terminology and Key Concepts\n\n8.5.1 Return Periods and Levels\nFundamental concepts for communicating extreme value results:\n\nExceedance probability: \\(p\\) = probability of exceeding a level in any given year\nReturn period/recurrence interval: \\(T = 1/p\\) = average time between exceedances\n\nReturn level: Value exceeded with probability \\(1/T\\) = quantile corresponding to return period \\(T\\)\n\nExample: 100-year flood has: - Exceedance probability: \\(p = 0.01\\) (1% chance per year) - Return period: \\(T = 100\\) years - Return level: Discharge exceeded on average once per century\nImportant: Return period is an average - a 100-year event can occur multiple times in a decade or not at all for centuries.\n\n\n8.5.2 Plotting Positions\nFor visualization, need to assign return periods to observed data:\n\nRanks: Largest observation = rank 1, second largest = rank 2, etc.\nWeibull plotting position: \\(p = m/(N+1)\\) where \\(m\\) = rank, \\(N\\) = sample size\nReturn period: \\(T = 1/p = (N+1)/m\\)\n\nNote: Various plotting position formulas exist in literature - choice affects visualization but not fitted model parameters.\n\n\n8.5.3 Threshold Selection\nCentral challenge: Balance bias vs.¬†variance - Too low: Model not asymptotically valid ‚Üí bias - Too high: Few exceedances ‚Üí large uncertainty\n\n\n8.5.4 Mean Residual Life Plot\nBased on GPD mean excess property:\nIf \\(Y_i \\sim \\text{GPD}(\\sigma, \\xi)\\), then the mean is given by \\[\n\\mathbb{E}[Y] = \\frac{\\sigma}{1 - \\xi}, \\quad \\xi &lt; 1.\n\\tag{8.1}\\]\nFor threshold exceedances: \\[\n\\mathbb{E}[X - u | X &gt; u] = \\frac{\\sigma_u}{1 - \\xi}, \\quad \\xi &lt; 1\n\\tag{8.2}\\]\nIf GPD is appropriate above threshold \\(u_0\\), then mean excess should be approximately linear in \\(u\\) for \\(u &gt; u_0\\). This forms the basis of the ‚Äúmean residual life plot‚Äù - subjective but useful diagnostic.\n\n\n8.5.5 Parameter Stability Plot\nAlternative approach: estimate GPD parameters for range of thresholds. Shape parameter \\(\\xi\\) should be approximately constant above appropriate threshold.\n\n\n8.5.6 Declustering\nProblem: Successive threshold exceedances may be dependent (e.g., multi-day storms) Solution: ‚ÄúDecluster‚Äù by combining nearby exceedances or imposing minimum separation\nMethods: - Runs declustering: Combine exceedances separated by &lt; threshold for &lt; specified duration - Peak identification: Extract local maxima with minimum separation\nPractical consideration: Balance between removing dependence and preserving extreme values.\n\n\n\n\n\n\n\n\n\nFigure¬†8.1: Mean Residual Life Plot for extreme rainfall data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.2: Return Period with Uncertainty\n\n\n\n\n\n\n\n\n8.5.7 Parameter Estimation\nSeveral approaches exist for fitting extreme value distributions:\n\n8.5.7.1 Maximum Likelihood Estimation (MLE)\nMost common approach: Find parameters that maximize likelihood of observed data\nAdvantages: - Asymptotically efficient and unbiased - Provides uncertainty estimates via Fisher information - Standard statistical inference procedures apply\nDisadvantages: - Can be unstable for small samples - May not exist for some parameter combinations - Sensitive to outliers\n\n\n8.5.7.2 Probability Weighted Moments (PWM)\nAlternative approach: Match theoretical and sample probability weighted moments\nAdvantages: - Often more robust than MLE for small samples - Computationally simpler - Less sensitive to outliers\nDisadvantages: - Less efficient than MLE asymptotically - Uncertainty quantification more complex\n\n\n8.5.7.3 Bayesian Methods\nModern approach: Specify prior distributions and compute posterior\nAdvantages: - Natural uncertainty quantification - Can incorporate prior knowledge - Regularizes parameter estimates - Handles model selection coherently\nDisadvantages: - Requires prior specification - Computationally intensive - Results depend on prior choice",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#challenges-in-extreme-value-analysis",
    "href": "chapters/hazard/extremes.html#challenges-in-extreme-value-analysis",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.6 Challenges in Extreme Value Analysis",
    "text": "8.6 Challenges in Extreme Value Analysis\n\n8.6.1 Parametric Uncertainty\nProblem: Many parameter values consistent with limited extreme data Consequence: Very different conclusions about rare event probabilities Example: Shape parameter uncertainty leads to large confidence intervals for return levels\nManagement approaches: - Bayesian methods for uncertainty quantification - Regional information to stabilize estimates - Informative priors based on physical understanding\n\n\n8.6.2 Model Structure Uncertainty\nProblem: Different distributional assumptions yield different results Common comparisons: - GEV vs.¬†alternative distributions (Log-Pearson III, etc.) - Stationary vs.¬†non-stationary models - Block maxima vs.¬†POT approaches\nManagement approaches: - Information criteria for model selection - Model averaging across plausible alternatives - Sensitivity analysis across model choices\n\n\n8.6.3 Sampling Uncertainty\nProblem: Finite samples for rare event estimation Key insight: If Hurricane Harvey had never occurred, 100-year rainfall estimates would be very different\nImplications: - Confidence intervals grow rapidly with return period - Need for longer records or supplementary information - Regional pooling to increase effective sample size",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#non-stationarity-and-climate-change",
    "href": "chapters/hazard/extremes.html#non-stationarity-and-climate-change",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.7 Non-Stationarity and Climate Change",
    "text": "8.7 Non-Stationarity and Climate Change\nTraditional assumption: Statistical properties constant over time Climate reality: Extreme value characteristics changing due to: - Rising temperatures affecting heat extremes - Changing precipitation patterns - Shifting storm tracks and intensity - Sea level rise affecting coastal extremes\n\n8.7.1 Covariate Methods\nAllow extreme value parameters to depend on covariates:\n\\[\\mu(t) = \\mu_0 + \\mu_1 \\cdot \\text{covariate}(t)\\]\nCommon covariates: - Time trends (linear, polynomial) - Climate indices (ENSO, AMO) - Global temperature anomalies - Physical process variables\nBenefits: Called ‚Äúprocess-informed‚Äù approaches (Schlef et al. 2023) Reviews: See Salas, Obeysekera, and Vogel (2018) for comprehensive treatment\n\n\n8.7.2 Implementation Challenges\n\nModel selection: Which covariates to include?\nFunctional form: Linear vs.¬†nonlinear relationships?\nParameter dependence: Which parameters vary with covariates?\nPrediction: How to project covariates into future?\n\n\n\n8.7.3 Practical Considerations\nDesign implications: Traditional return period concept breaks down Risk assessment: Need for time-varying risk measures Decision-making: Robust strategies under non-stationarity",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#non-stationary-extreme-value-analysis-theory-and-practice",
    "href": "chapters/hazard/extremes.html#non-stationary-extreme-value-analysis-theory-and-practice",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.8 Non-Stationary Extreme Value Analysis: Theory and Practice",
    "text": "8.8 Non-Stationary Extreme Value Analysis: Theory and Practice\n\n8.8.1 The Stationarity Assumption and Its Breakdown\nExtreme value theory is based on the assumption that the data are independent and identically distributed (iid): - Each draw comes from the same distribution - Statistical properties remain constant over time\nThis assumption is violated by: - Climate change effects on temperature and precipitation extremes - Low-frequency variability (e.g., Pacific Decadal Oscillation) - Memory processes in the climate system - Urbanization and land use changes\n\n\n8.8.2 What is Stationarity?\nA stationary process is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. A stochastic process is a model for a sequence of random variables (e.g., random walk, MCMC).\nAs famously stated by Milly et al. (2008): ‚ÄúStationarity is dead.‚Äù\n\n\n8.8.3 Climate Change Impacts on Extremes\nThermodynamic Effects: - Clausius-Clapeyron relation: \\(e_s(T) = e_0 \\exp\\left(\\frac{L_v}{R_v T}\\right)\\) - Approximately 7% increase in atmospheric moisture per degree K of warming - Direct impacts on precipitation intensity\nDynamic Effects: - Longer, hotter summers due to slower seasonal transitions - Poleward expansion of tropical circulation patterns - Changes in storm structure and intensity - Shifts in jet stream patterns affecting extreme weather\nThe following content draws from Seneviratne et al. (2021) executive summary:\nClimate Change Impacts on Precipitation: - The frequency and intensity of heavy precipitation events have likely increased at the global scale over a majority of land regions with good observational coverage - Heavy precipitation has likely increased on the continental scale over three continents: North America, Europe, and Asia - Heavy precipitation will generally become more frequent and more intense with additional global warming - At a global warming level of 4¬∞C relative to the pre-industrial level, very rare (e.g., one in 10 or more years) heavy precipitation events would become more frequent and more intense than in the recent past, on the global scale (virtually certain) and in all continents and AR6 regions - The projected increase in the intensity of extreme precipitation translates to an increase in the frequency and magnitude of pluvial floods ‚Äì surface water and flash floods ‚Äì (high confidence)\nClimate Change Impacts on River Floods: - Significant trends in peak streamflow have been observed in some regions over the past decades (high confidence) - The seasonality of river floods has changed in cold regions where snow-melt is involved, with an earlier occurrence of peak streamflow (high confidence) - Global hydrological models project a larger fraction of land areas to be affected by an increase in river floods than by a decrease in river floods (medium confidence)\nClimate Change Impacts on Extreme Temperatures: - The frequency and intensity of hot extremes (including heatwaves) have increased, and those of cold extremes have decreased on the global scale since 1950 (virtually certain) - Human-induced greenhouse gas forcing is the main driver of the observed changes in hot and cold extremes on the global scale (virtually certain) and on most continents (very likely) - The frequency and intensity of hot extremes will continue to increase and those of cold extremes will continue to decrease, at global and continental scales and in nearly all inhabited regions with increasing global warming levels\nClimate Change Impacts on Tropical Cyclones: - The average and maximum rain rates associated with tropical cyclones (TCs), extratropical cyclones and atmospheric rivers across the globe, and severe convective storms in some regions, increase in a warming world (high confidence) - It is likely that the global proportion of Category 3‚Äì5 tropical cyclone instances has increased over the past four decades - The proportion of intense TCs, average peak TC wind speeds, and peak wind speeds of the most intense TCs will increase on the global scale with increasing global warming (high confidence) - Future wind speed changes are expected to be small, although poleward shifts in the storm tracks could lead to substantial changes in extreme wind speeds in some regions (medium confidence)\nEl Ni√±o-Southern Oscillation Effects: El Ni√±o-Southern Oscillation remains a major driver of interannual climate variability, affecting extreme value statistics through teleconnections that modify regional precipitation and temperature patterns.\n\n\n8.8.4 Non-Stationary Modeling Approaches\nRolling Window Approach: - Simple method that estimates parameters using moving time windows - Advantages: Simple and interpretable - Disadvantages: Noisy estimates, potential loss of extreme events at window boundaries - Less bias but more variance compared to stationary models\nRegression Models for Parameters: In linear regression and GLMs, every data point is drawn from its own distribution that depends on parameters and covariates. We can apply this approach to extreme value models by allowing GEV or GPD parameters to vary with covariates.\nTypes of Parameter Variation: What can vary with time/covariates? 1. Location parameter: \\(\\mu(t) = f(X(t))\\) 2. Scale parameter: \\(\\sigma(t) = f(X(t))\\) 3. Both location and scale parameters 4. Scale and coefficient of variation: \\(\\mu(t) = \\phi \\sigma(t)\\) 5. Varying shape parameter (impractical but theoretically allowed)\nFunctional Forms for Parameter Variation: How parameters vary with covariates: 1. Linear relationships: \\(\\theta(t) = \\alpha + \\beta_1 X_1(t) + \\beta_2 X_2(t) + \\cdots\\) 2. Nonlinear relationships using splines, GAMs, or other flexible approaches 3. Anything is theoretically allowed, but not everything is practical for extreme value analysis\n\n\n8.8.5 Covariate Selection\nGeneral Guidance: - Physical theory and domain knowledge are invaluable for covariate selection - For precipitation extremes, logarithm of CO2 concentration is often a useful covariate because: - It isolates the global warming signal from natural variability like ENSO - It provides a monotonic trend that matches expected thermodynamic responses\nCommon Covariates: - Time (linear or polynomial trends) - Global mean temperature anomalies - Logarithm of atmospheric CO2 concentration - Climate oscillation indices (ENSO, AMO, PDO) - Local environmental variables (sea surface temperatures, soil moisture)\n\n\n8.8.6 Case Study: Houston Hobby Airport Precipitation Analysis\nMotivation: Analysis of trends in extreme precipitation at Houston Hobby Airport demonstrates practical implementation of non-stationary extreme value methods.\nData Considerations: - Annual maximum daily precipitation from NOAA GHCND - Quality control: keep only years with at least 350 days of data - Need to address potential non-stationarity due to climate change\nTrend Analysis: The Mann-Kendall test is commonly used to assess the presence of trends in time series data. Rank correlation between precipitation ranks and years provides initial evidence of trends.\nModel Formulations:\nLocation Trend Model: \\[\n\\begin{aligned}\ny_t &\\sim \\text{GEV} \\left( \\mu_t, \\sigma, \\xi \\right) \\\\\n\\mu_t &= \\alpha + \\beta X_t\n\\end{aligned}\n\\]\nScale Trend Model: \\[\n\\begin{aligned}\ny_t &\\sim \\text{GEV} \\left( \\mu, \\sigma_t, \\xi \\right) \\\\\n\\log(\\sigma_t) &= \\alpha + \\beta X_t\n\\end{aligned}\n\\]\nCombined Location and Scale Trend: \\[\n\\begin{aligned}\ny_t &\\sim \\text{GEV} \\left( \\mu_t, \\sigma_t, \\xi \\right) \\\\\n\\mu_t &= \\alpha_\\mu + \\beta_\\mu X_t \\\\\n\\log(\\sigma_t) &= \\alpha_\\sigma + \\beta_\\sigma X_t\n\\end{aligned}\n\\]\n\n\n8.8.7 Implementation Considerations\nComputational Tools: - Extremes.jl package provides gevfitbayes() function with covariate support - Can specify locationcovid and logscalecovid parameters for regression modeling - Default uniform priors may not be ideal - custom priors often needed\nModel Comparison: Comparison between stationary and various non-stationary models helps identify: - Which parameters are most sensitive to climate change - How return level estimates change with different trend assumptions - Uncertainty in future projections under continued warming\n\n\n8.8.8 Key Insights from Non-Stationary Analysis\nBias-Variance Trade-off: - Non-stationary models reduce bias by accounting for trends - But increase variance due to additional parameters to estimate - Physical process knowledge helps guide model selection\nParametric Uncertainty: - Non-stationary models typically have larger parametric uncertainty - Future projections require assumptions about covariate evolution - Model comparison becomes even more critical\nPractical Implications: - Traditional design standards may need updating - Infrastructure planning must account for changing risk profiles - Adaptive management strategies become more important",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#regionalization-and-spatial-methods",
    "href": "chapters/hazard/extremes.html#regionalization-and-spatial-methods",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.9 Regionalization and Spatial Methods",
    "text": "8.9 Regionalization and Spatial Methods\nMotivation: Single-site records often too short for reliable extreme value analysis\nNearby stations should (usually) have similar precipitation, flood, or other extreme event probabilities. Regionalization methods reduce estimation error by pooling information while reducing sampling error from random variation between nearby stations. However, regionalization does NOT reduce sampling error from major regional events that affect all stations simultaneously.\n\n8.9.1 Classical Regional Frequency Analysis\nL-Moment Estimators: L-moments are linear combinations of order statistics that can be used to match theoretical and empirical distribution moments.\nAdvantages: - Computationally efficient - Work well in practice - Robust parameter estimation\nDisadvantages: - Less flexible than likelihood-based methods - Difficult to quantify parametric uncertainty - Limited ability to incorporate covariates\nRegional Frequency Analysis Process: 1. Assign sites to regions based on climate, geography, or other similarity measures 2. Estimate L-moments for each site using observed data 3. Check for regional homogeneity using statistical tests 4. Take regional L-moments as the weighted mean of site L-moments 5. Apply scaling factors (e.g., average annual maximum flood for each site)\nThis approach is best implemented using the R lmomRFA package, which can be called from Julia using RCall.jl.\n\n\n8.9.2 Region of Influence Approach\nMotivation: RFA assumes all sites are assigned to a single region, but often regions are not distinct.\nMethodology: 1. Define similarity measures between each pair of sites (e.g., distance, land use, elevation, climate indices) 2. For estimates at site i, define its ‚Äúregion of influence‚Äù as the most similar sites (analogous to k-nearest neighbors) 3. Estimate L-moments for each site and compute weighted average as in RFA 4. Allow flexible, site-specific regions rather than rigid regional boundaries\nAdvantages: - More flexible than traditional RFA - Can adapt region definition to local characteristics - Better handles sites near regional boundaries\n\n\n8.9.3 Hierarchical Bayesian Models\nModern spatial approaches use hierarchical models to balance between ‚Äúfull pooling‚Äù (all sites identical) and ‚Äúno pooling‚Äù (each site independent).\n\n\n8.9.4 Full Pooling Approach\nConcept: Assume within a region, all sites have the same distribution. Estimate a single distribution for the entire region. This is analogous to classical regional frequency analysis.\nBayesian Implementation:\n@model function gev_fully_pooled(y::AbstractMatrix)\n    N_yr, N_stn = size(y)\n    Œº ~ Normal(5, 5)\n    œÉ ~ LogNormal(0, 2)\n    Œæ ~ Uniform(-0.5, 0.5)\n    for s in 1:N_stn\n        for t in 1:N_yr\n            if !ismissing(y[t, s])\n                y[t, s] ~ GeneralizedExtremeValue(Œº, œÉ, Œæ)\n            end\n        end\n    end\nend\nAdvantages: - Fast sampling due to high data-to-parameter ratio - Simple to implement and interpret - Maximizes information sharing across sites\nImportant Limitation: This approach weights each observation equally, regardless of site or year. If some years have more observations than others, those years are implicitly weighted more heavily. A better model would weight each year equally.\n\n\n8.9.5 Partial Pooling Approach\nConcept: Model parameters at each site as being drawn from a common distribution. This balances between full pooling and no pooling by sharing information while allowing site-specific variation.\nMathematical Framework: \\[\n\\begin{aligned}\n    y_{s,t} &\\sim \\text{GEV}(\\mu_s, \\sigma_s, \\xi_s) \\\\\n    \\mu_s &\\sim \\text{Normal}(\\mu^0, \\tau^\\mu) \\\\\n    \\sigma_s &\\sim \\text{LogNormal}(\\sigma^0, \\tau^\\sigma) \\\\\n    \\xi &\\sim \\text{Uniform}(-0.5, 0.5) \\text{ (fully pooled)}\n\\end{aligned}\n\\]\nwhere \\(s\\) is the site index and \\(t\\) is the year index.\nHyperparameters: In Bayesian statistics, hyperparameters like \\(\\mu^0\\) and \\(\\tau^\\mu\\) are learned as part of the model. These describe the distribution from which site-specific parameters are drawn.\nImplementation Example:\n@model function gev_partial_pool(y::AbstractMatrix)\n    N_yr, N_stn = size(y)\n\n    # Define hyperparameters with informative priors\n    Œº‚ÇÄ ~ Normal(5, 3)\n    œÑŒº ~ LogNormal(0, 0.5)\n    œÉ‚ÇÄ ~ LogNormal(0.5, 0.5)\n    œÑœÉ ~ LogNormal(0, 0.5)\n\n    # Site-specific parameters depend on hyperparameters\n    Œº ~ filldist(Normal(Œº‚ÇÄ, œÑŒº), N_stn)\n    œÉ ~ filldist(truncated(Normal(œÉ‚ÇÄ, œÑœÉ), 0, Inf), N_stn)\n\n    # Fully pooled shape parameter\n    Œæ ~ Uniform(-0.5, 0.5)\n\n    # Likelihood\n    for s in 1:N_stn\n        for t in 1:N_yr\n            y[t, s] ~ GeneralizedExtremeValue(Œº[s], œÉ[s], Œæ)\n        end\n    end\nend\nComputational Considerations: With \\(N\\) stations, we have \\(4 + N + N + 1 = 5 + 2N\\) parameters to estimate, making sampling slower than full pooling. Careful prior specification becomes more important with increased model complexity.\n\n\n8.9.6 Spatial Regression Models\nAlternative Approach: Model parameters as functions of geographical location and environmental covariates. This can simplify the model by reducing the number of parameters to estimate.\nSimple Spatial Model Example: \\[\n\\begin{aligned}\n    \\mu(s) &= \\alpha^\\mu + \\beta^\\mu_1 \\cdot \\text{lat}(s) + \\beta^\\mu_2 \\cdot \\text{lon}(s) \\\\\n    \\sigma(s) &= \\alpha^\\sigma + \\beta^\\sigma_1 \\cdot \\text{lat}(s) + \\beta^\\sigma_2 \\cdot \\text{lon}(s) \\\\\n    y_{s,t} &\\sim \\text{GEV}(\\mu(s), \\sigma(s), \\xi)\n\\end{aligned}\n\\]\nExtensions: - Include elevation, distance to coast, climate indices as covariates - Use flexible relationships (splines, GAMs) instead of linear functions - Incorporate spatial correlation through Gaussian process priors - Combine with temporal trends for spatiotemporal modeling\n\n\n8.9.7 Practical Implementation Considerations\nData Challenges: - Missing data requires careful treatment in likelihood calculations - Different record lengths across sites - Need for quality control and homogenization\nModel Selection: - Compare full pooling, partial pooling, and no pooling approaches - Use cross-validation or information criteria - Consider out-of-sample prediction performance\nUncertainty Quantification: - Hierarchical models naturally provide uncertainty estimates - Can propagate parameter uncertainty to return level calculations - Important for decision-making under uncertainty\n\n\n8.9.8 Regional Frequency Analysis\nApproach: Pool information across ‚Äúsimilar‚Äù sites\n\nIdentify regions: Group sites with similar extreme value behavior\nNormalize data: Scale to common distribution\nFit regional model: Estimate shape parameter regionally\nScale back: Apply regional parameters to individual sites\n\nAdvantages: - Increased effective sample size - More stable parameter estimates - Better extrapolation to rare events\nChallenges: - Defining hydrologically similar regions - Testing regional homogeneity - Accounting for cross-site dependence\n\n\n8.9.9 Modern Spatial Approaches\nHierarchical models: Borrow strength across space while allowing local variation Spatial random effects: Model spatial correlation in extreme value parameters Machine learning: Use environmental covariates to predict extreme value parameters",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#case-studies",
    "href": "chapters/hazard/extremes.html#case-studies",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.10 Case Studies",
    "text": "8.10 Case Studies\n\n8.10.1 Hurricane Harvey and the Addicks/Barker Reservoirs\nContext: Legal case requiring extreme precipitation frequency analysis\nChallenges: - Interacting drivers of non-stationarity - Short observational records - High stakes for affected communities\nKey insights: - Plausible assumptions led to vastly different estimates - No single ‚Äúobjective‚Äù answer exists - Uncertainty quantification crucial for decision-making\n\n\n8.10.2 Texas Precipitation Frequency Analysis\nProject: Joint TWDB/TAMU/Rice effort to update Atlas 14\nMotivation: - NOAA Atlas 14 doesn‚Äôt account for climate change - Need state-wide consistent methodology - Multiple durations and return periods required\nApproach: - More stations than federal analysis - Climate change considerations - Advanced uncertainty quantification\n\n\n8.10.3 Winter Storm Uri Analysis\nQuestions: How likely was this event? Should we have been prepared?\nVariables studied: - Temperature at individual grid cells - Population-weighted temperature indices - Duration and spatial extent\nFindings: Illustrated challenges of: - Compound extremes (cold + widespread) - Infrastructure vulnerability to rare events - Need for robust planning under uncertainty",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#computational-tools",
    "href": "chapters/hazard/extremes.html#computational-tools",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "8.11 Computational Tools",
    "text": "8.11 Computational Tools\nR packages: - ismev: Classical extreme value methods - evd: Extended extreme value distributions - POT: Peak-over-threshold methods\nJulia packages: - Extremes.jl: Comprehensive extreme value toolkit - Well-documented with practical examples\nPython packages: - scipy.stats: Basic extreme value distributions - pyextremes: Specialized extreme value analysis",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#further-reading",
    "href": "chapters/hazard/extremes.html#further-reading",
    "title": "8¬† Extreme Value Theory üöß",
    "section": "Further reading",
    "text": "Further reading\nEssential texts: - Coles (2001): Canonical extreme value textbook with mathematical rigor and practical examples\nCurrent research directions: - Sampling uncertainty: Lu, Seiyon Lee, and Doss-Gollin (2025) discusses spatial approaches - Non-independence: Accounting for temporal and spatial dependence - Climate change: Non-stationary extreme value models - Machine learning: Neural networks for extreme value analysis - Multivariate extremes: Joint behavior of multiple variables\n\n\n\n\nColes, Stuart. 2001. An Introduction to Statistical Modeling of Extreme Values. Springer Series in Statistics. London: Springer.\n\n\nLu, Yuchen, Benjamin Seiyon Lee, and James Doss-Gollin. 2025. ‚ÄúBayesian Spatiotemporal Nonstationary Model Quantifies Robust Increases in Daily Extreme Rainfall Across the Western Gulf Coast.‚Äù Environmental Research: Climate 4 (3): 035016. https://doi.org/10.1088/2752-5295/adf56e.\n\n\nMilly, P C D, Julio Betancourt, M Falkenmark, R M Hirsch, Z W Kundzewicz, D P Lettenmaier, and R J Stouffer. 2008. ‚ÄúStationarity Is Dead: Whither Water Management?‚Äù Science 319 (5863): 573‚Äì74. https://doi.org/10.1126/science.1151915.\n\n\nSalas, J D, J Obeysekera, and R M Vogel. 2018. ‚ÄúTechniques for Assessing Water Infrastructure for Nonstationary Extreme Events: A Review.‚Äù Hydrological Sciences Journal 63 (3): 325‚Äì52. https://doi.org/10.1080/02626667.2018.1426858.\n\n\nSchlef, Katherine E., Kenneth E. Kunkel, Casey Brown, Yonas Demissie, Dennis P. Lettenmaier, Anna Wagner, Mark S. Wigmosta, et al. 2023. ‚ÄúIncorporating Non-Stationarity from Climate Change into Rainfall Frequency and Intensity-Duration-Frequency (IDF) Curves.‚Äù Journal of Hydrology 616 (January): 128757. https://doi.org/10.1016/j.jhydrol.2022.128757.\n\n\nSeneviratne, S. I., X. Zhang, M. Adnan, W. Badi, C. Dereczynski, A. Di Luca, S. Ghosh, et al. 2021. ‚ÄúWeather and Climate Extreme Events in a Changing Climate.‚Äù Book section. In Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change, edited by V. Masson-Delmotte, P. Zhai, A. Pirani, S. L. Connors, C. P√©an, S. Berger, N. Caud, et al. Cambridge, UK; New York, NY, USA: Cambridge University Press. https://doi.org/10.1017/9781009157896.013.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Extreme Value Theory üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html",
    "href": "chapters/hazard/downscaling-bias-correction.html",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Fundamentals of Climate Science - Correlation and Dimensionality",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#learning-objectives",
    "href": "chapters/hazard/downscaling-bias-correction.html#learning-objectives",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:\n\nDistinguish between supervised and distributional downscaling approaches\nUnderstand the motivation for downscaling climate model outputs\nApply bias correction and quantile-quantile mapping techniques\nRecognize the stationarity assumption and its implications\nEvaluate different downscaling methods for specific applications\nUnderstand modern machine learning approaches to climate downscaling",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#motivation",
    "href": "chapters/hazard/downscaling-bias-correction.html#motivation",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.1 Motivation",
    "text": "9.1 Motivation\nClimate models operate at coarse spatial and temporal resolutions, but many applications require high-resolution climate information:\n\nStormwater management: Long-term design requires hourly precipitation at city scales\nWater resources management: Subseasonal to multi-year planning needs basin-specific information\nFire propagation: Hourly to weekly meteorological conditions at landscape scales\nAgriculture: Daily temperature and precipitation at field scales\n\n\n9.1.1 Objectives of Downscaling\nDownscaling aims to achieve three primary objectives (Lanzante et al. 2018):\n\nEnhanced spatial detail: Increase resolution from ~100-200 km to ~1-10 km\nMitigation of systematic ESM biases: Correct known model biases\nGeneration of variables not explicitly rendered by GCMs: Derive additional variables\n\n\n\n9.1.2 Challenges with Earth System Models\nEarth System Models (ESMs) face inherent limitations for local applications:\n\nScale mismatch: ESMs are tuned for energy balance and large-scale circulation, not local extremes\nSpatial averaging: Grid cells represent averages over large areas\nTemporal averaging: Time steps may miss sub-daily variability\nProcess representation: Local-scale processes may be parameterized or missing\n\nTwo specific challenges illustrate these limitations:\n\n‚ÄúDreary‚Äù problem: Models produce too many days with light precipitation\n‚ÄúDrizzling‚Äù problem: Models fail to capture intense precipitation events\n\nThese systematic biases require correction for practical applications.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#supervised-methods",
    "href": "chapters/hazard/downscaling-bias-correction.html#supervised-methods",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.2 Supervised Methods",
    "text": "9.2 Supervised Methods\nSupervised downscaling is especially common in weather forecasting, where we have pairs of (observed, forecasted) data.\n\n9.2.1 Framework\nSupervised downscaling treats the problem as a statistical learning task:\n\nInput: Pairs \\((X_i, y_i)\\) where:\n\n\\(X_i\\): Predictors (e.g., gridded model output)\n\\(y_i\\): Predictand (e.g., station observations)\n\nGoal: Learn function \\(f\\) such that \\(f(X_i) \\approx y_i\\)\nKey requirement: \\(X_i\\) and \\(y_i\\) observed at the same time\n\nQuality is measured through loss functions (e.g., mean squared error, likelihood).\n\n\n9.2.2 Applications\nSupervised methods work well when: - Historical model-observation pairs exist - Relationship between predictors and predictand is stable - Focus is on weather forecasting or hindcasting\nExamples: - Mapping satellite to radar precipitation data - Post-processing numerical weather predictions - Downscaling reanalysis to station observations\n\n\n9.2.3 Example: Linear Regression\nSimplest approach relates large-scale predictors to local observations:\n\\[y = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j + \\epsilon\\]\nwhere \\(X_j\\) might include: - Temperature at multiple pressure levels - Geopotential height gradients - Humidity measures - Previous day‚Äôs local weather\nAdvantages: Simple, interpretable, computationally fast Limitations: Assumes linear relationships, may miss complex interactions\n\n\n9.2.4 Example: Model Output Statistics (MOS)\nOperational approach used by weather services:\n\nPreprocessing: Standardize model outputs and observations\nPredictor selection: Choose relevant large-scale variables\nModel fitting: Often multiple linear regression with categorical predictors\nPost-processing: Apply adjustments for known biases\n\nKey insight: MOS exploits systematic model biases to improve forecasts.\n\n\n9.2.5 Example: Generative ML\nModern approach using deep learning:\n\nGenerative Adversarial Networks (GANs): Learn to generate realistic high-resolution fields\nDiffusion models: Model the data generation process through noise injection and removal\nConditional models: Generate outputs conditional on large-scale inputs\n\nGoal: Sample from \\(p(y|X)\\) rather than just predict \\(\\mathbb{E}[y|X]\\)\nAdvantages: Can capture complex nonlinear relationships and full probability distributions Limitations: Computationally intensive, requires large training datasets, difficult to interpret",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#distribution-based-methods",
    "href": "chapters/hazard/downscaling-bias-correction.html#distribution-based-methods",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.3 Distribution-Based Methods",
    "text": "9.3 Distribution-Based Methods\n\n9.3.1 The Climate Model Challenge\nClimate models present a unique challenge different from weather forecasting. ESMs simulate from the distribution of weather given climate boundary conditions:\n\nRun 100 ESM ensemble members over historical conditions\nStudy December 1, 1980 across all ensemble members\nSome realizations will be rainy, others dry; some cool, others warm\nStatistically meaningful but not deterministic forecasts\n\n\n\n9.3.2 No Paired Data Problem\nUnlike weather forecasting, climate models don‚Äôt provide paired observations: - We have samples \\({X_1, \\ldots, X_N}\\) from climate model - We have samples \\({y_1, \\ldots, y_K}\\) from observations - No correspondence between \\(X_i\\) and \\(y_j\\) at specific times\nKey insight: April 15, 1995 in model ‚â† April 15, 1995 in observations\nBoth are samples from ‚Äúdistribution of late April weather in 1990s conditions‚Äù\n\n\n9.3.3 Distributional Approach\nSince we cannot use supervised methods, we compare distributions:\n\\[p_{\\text{model}}(X) \\neq p_{\\text{obs}}(y)\\]\nGoal: Transform model outputs to match observational distribution characteristics.\n\n\n9.3.4 Example: Bias Correction\nSimplest distributional method corrects the mean:\n\\[\\begin{aligned}\n\\text{bias} &= \\mathbb{E}[X] - \\mathbb{E}[y] \\\\\n\\hat{y} &= X - \\text{bias}\n\\end{aligned}\\]\nQuestion: Is this distributional or supervised? Answer: Distributional - uses distributional moments, not paired data.\n\n\n9.3.5 Example: Quantile-Quantile Mapping\nMore sophisticated approach matching full distributions:\n\nCalculate quantiles: For probabilities \\(p \\in [0,1]\\):\n\nModel quantile: \\(q_m(p) = F_m^{-1}(p)\\)\nObserved quantile: \\(q_o(p) = F_o^{-1}(p)\\)\n\nCreate mapping function: \\(h(x) = F_o^{-1}(F_m(x))\\)\nApply correction: \\(\\hat{y} = h(X)\\)\n\nInterpretation: Transform model value to its quantile, then map to corresponding observed quantile.\nAdvantages: - Corrects entire distribution, not just mean - Preserves temporal correlations - Handles non-Gaussian distributions\nLimitations: - Assumes stationarity of correction - Cannot add variability not present in model - May create artifacts at distribution tails\n\n\n9.3.6 Example: CDF Matching\nAlternative approach directly matches cumulative distribution functions:\n\nEstimate CDFs: \\(\\hat{F}_m(x)\\) and \\(\\hat{F}_o(x)\\)\nDefine mapping: Minimize \\(\\int |\\hat{F}_m(h(x)) - \\hat{F}_o(x)| dx\\)\nApply transformation: Use optimal \\(h(\\cdot)\\) to correct future data\n\nThis is closely related to quantile mapping but may use different estimation techniques.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#advanced-machine-learning-approaches",
    "href": "chapters/hazard/downscaling-bias-correction.html#advanced-machine-learning-approaches",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.4 Advanced Machine Learning Approaches",
    "text": "9.4 Advanced Machine Learning Approaches\n\n9.4.1 CorrectorGAN\nRecent work applies Generative Adversarial Networks to bias correction (Price and Rasp 2022):\nArchitecture: - Generator: Takes coarse model output, produces high-resolution corrected field - Discriminator: Learns to distinguish real observations from generated corrections\nAdvantages: - Learns complex spatial patterns - Can generate multiple plausible realizations - Captures spatial correlations better than pointwise methods\nTraining process: 1. Generator learns mapping from model to observations 2. Discriminator learns to identify ‚Äúrealistic‚Äù weather patterns 3. Adversarial training improves both networks\n\n\n9.4.2 Other Modern Approaches\nSuper-resolution methods: Increase spatial resolution while correcting biases Conditional diffusion models: Generate high-resolution weather conditioned on large-scale patterns Physics-informed neural networks: Incorporate physical constraints into ML models",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#dynamical-downscaling",
    "href": "chapters/hazard/downscaling-bias-correction.html#dynamical-downscaling",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.5 Dynamical Downscaling",
    "text": "9.5 Dynamical Downscaling\n\n9.5.1 Physics-Based Dynamical Downscaling\nApproach: Run high-resolution regional climate model (RCM) nested within global model:\n\nGlobal model provides boundary conditions\nRegional model simulates detailed physics at ~10-50 km resolution\nOutput includes all meteorological variables at high resolution\n\nAdvantages: - Physically consistent - Captures local topographic effects - Generates all meteorological variables simultaneously\nLimitations: - Computationally expensive - May inherit global model biases - Still requires bias correction for many applications\n\n\n9.5.2 AI-Based Dynamical Downscaling\nEmerging approach using machine learning weather models:\n\nTraining: Learn atmospheric dynamics from high-resolution observations/reanalysis\nApplication: Use ML model to generate high-resolution fields from coarse inputs\nExamples: FourCastNet, GraphCast, DLWP\n\nPotential advantages: - Much faster than physics-based models - Can be trained on observational targets - May avoid some systematic model biases\nCurrent limitations: - Limited to variables in training data - May not conserve physical quantities - Shorter stable integration times",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#the-stationarity-assumption",
    "href": "chapters/hazard/downscaling-bias-correction.html#the-stationarity-assumption",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.6 The Stationarity Assumption",
    "text": "9.6 The Stationarity Assumption\n\n9.6.1 Critical Assumption\nAll downscaling methods assume stationarity: the relationship between large-scale and local climate does not change over time.\nFor supervised methods: \\(p(y|X)\\) or \\(y = f(X)\\) constant over time For distributional methods: Distributional corrections constant over time\n\n\n9.6.2 Why Stationarity Matters\nDownscaling is trained on historical relationships but applied to future conditions: - Climate change may alter precipitation-temperature relationships - Atmospheric circulation patterns may shift - Extreme event characteristics may change\nThis assumption is never perfect but is necessary for practical applications.\n\n\n9.6.3 Implications for Practice\n\nMethod selection: Choose approaches robust to modest non-stationarity\nValidation: Test performance across different time periods\nUncertainty: Acknowledge stationarity as source of uncertainty\nUpdating: Regularly retrain models as new observations become available",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#common-datasets-for-downscaling",
    "href": "chapters/hazard/downscaling-bias-correction.html#common-datasets-for-downscaling",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.7 Common Datasets for Downscaling",
    "text": "9.7 Common Datasets for Downscaling\n\n9.7.1 Observational Data\n\nGauge data: Point measurements with high temporal resolution\nGridded observational products: Interpolated from station networks\nRadar/satellite products: Remote sensing observations processed to grids\n\n\n\n9.7.2 Model Data\n\nReanalysis products: Gridded reconstructions combining observations and models\n\nExample: ERA5 (0.25¬∞ resolution, hourly, 1940-present)\n\nESM outputs: Climate model simulations\n\nHistorical runs (observed forcing)\nFuture projections (scenario forcing)\nCMIP archives provide standardized multi-model ensembles\n\n\n\n\n9.7.3 Key Characteristics\nESMs simulate weather distributions conditional on boundary conditions: - Not deterministic forecasts - Multiple ensemble members show range of possible weather - Focus on statistics, not individual events",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#practical-considerations",
    "href": "chapters/hazard/downscaling-bias-correction.html#practical-considerations",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "9.8 Practical Considerations",
    "text": "9.8 Practical Considerations\n\n9.8.1 Method Selection\nChoose downscaling approach based on:\n\nData availability: Supervised requires paired data; distributional does not\nApplication needs: Point vs.¬†spatial; single vs.¬†multiple variables\nComputational resources: Statistical vs.¬†dynamical methods\nPhysical consistency: Importance of conserving physical relationships\n\n\n\n9.8.2 Evaluation Strategies\nStatistical metrics: - Bias in mean, variance, quantiles - Correlation with observations - Skill at extreme events\nPhysical consistency: - Energy and water balance - Spatial and temporal correlations - Frequency of extremes\nDecision-relevant metrics: - Performance for specific applications - Economic value for decision-making\n\n\n9.8.3 Common Pitfalls\n\nOver-reliance on stationarity: Assume relationships never change\nIgnoring physical constraints: Focus only on statistical fit\nInadequate validation: Test only on training period\nSingle-model dependence: Rely on one downscaling approach",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#further-reading",
    "href": "chapters/hazard/downscaling-bias-correction.html#further-reading",
    "title": "9¬† Downscaling and Bias Correction üöß",
    "section": "Further reading",
    "text": "Further reading\n\nLanzante et al. (2018) for comprehensive review of downscaling challenges\nWeather generators can be used for downscaling\nExtreme value statistics for downscaling extremes\nCorrelation and Dimensionality for advanced statistical techniques\n\n\n\n\n\nLanzante, John R, Keith W Dixon, Mary Jo Nath, Carolyn E Whitlock, and Dennis Adams-Smith. 2018. ‚ÄúSome Pitfalls in Statistical Downscaling of Future Climate.‚Äù Bulletin of the American Meteorological Society 99 (4): 791‚Äì803. https://doi.org/10.1175/bams-d-17-0046.1.\n\n\nPrice, Ilan, and Stephan Rasp. 2022. ‚ÄúIncreasing the Accuracy and Resolution of Precipitation Forecasts Using Deep Generative Models.‚Äù March 23, 2022. https://doi.org/10.48550/arXiv.2203.12297.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Downscaling and Bias Correction üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html",
    "href": "chapters/hazard/generators.html",
    "title": "10¬† Stochastic Weather Generators üöß",
    "section": "",
    "text": "Learning objectives",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stochastic Weather Generators üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html#example-hidden-markov-models-hmms",
    "href": "chapters/hazard/generators.html#example-hidden-markov-models-hmms",
    "title": "10¬† Stochastic Weather Generators üöß",
    "section": "10.1 Example: Hidden Markov Models (HMMs)",
    "text": "10.1 Example: Hidden Markov Models (HMMs)\nWe can pull one of Andy‚Äôs examples into a notebook",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stochastic Weather Generators üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html#generators-for-downscaling",
    "href": "chapters/hazard/generators.html#generators-for-downscaling",
    "title": "10¬† Stochastic Weather Generators üöß",
    "section": "10.2 Generators for Downscaling",
    "text": "10.2 Generators for Downscaling\nGenerators can be used as a form of downscaling, where some credibly simulated variables from a climate model are used as input to a stattistical generator.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stochastic Weather Generators üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html#further-reading",
    "href": "chapters/hazard/generators.html#further-reading",
    "title": "10¬† Stochastic Weather Generators üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Stochastic Weather Generators üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html",
    "href": "chapters/hazard/physics-models.html",
    "title": "11¬† Physics-Based Models and Calibration üöß",
    "section": "",
    "text": "11.1 Physics-based and ML-based models\nWe often want to combine models. For example, to assess flood hazard from a tropical cyclone, we might\nWhile statistical models are well-suited to components (1) and (2), we might want to use physics-based models for tasks (2) and (3). Similarly, in water resources planning we might be interested in modeling drought risks for a watershed; while we might want to use a stochastic weather generator to generate synthetic multi-site time series of variables like precipitation, temperature, and potential evapotranspiration, we might want to use a hydrologic model to simulate the watershed response to these variables. While the field of hydrological, hydraulic, and hydrodynamic modeling is extensive, and the subject of numerous textbooks (and thus beyond our scope), here we will focus on\nThis is a false dichotomy. Physics-based models all have parameterizations, for example of sub-grid turbulence. Rather, models exist on a spectrum ranging from fully data-driven to fully physics-based. Moreover, ML techniques are increasingly being applied to speed and enhance solutions to PDEs, further blurring this distinction (Rackauckas et al. 2020).",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Physics-Based Models and Calibration üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html#uncertainty-in-model-chains",
    "href": "chapters/hazard/physics-models.html#uncertainty-in-model-chains",
    "title": "11¬† Physics-Based Models and Calibration üöß",
    "section": "11.2 Uncertainty in model chains",
    "text": "11.2 Uncertainty in model chains\n\nDittes et al. (2018) quantifies the contribution of various drivers (scenario, GCM, downscaling, hydro modeling)\nLafferty and Sriver (2023) shows that model structure uncertainty in downscaling choice matters a lot for risk assessment",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Physics-Based Models and Calibration üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html#calibration",
    "href": "chapters/hazard/physics-models.html#calibration",
    "title": "11¬† Physics-Based Models and Calibration üöß",
    "section": "11.3 Calibration",
    "text": "11.3 Calibration",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Physics-Based Models and Calibration üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html#further-reading",
    "href": "chapters/hazard/physics-models.html#further-reading",
    "title": "11¬† Physics-Based Models and Calibration üöß",
    "section": "Further reading",
    "text": "Further reading\n\n\n\n\nDittes, Beatrice, Olga ≈†paƒçkov√°, Lukas Schoppa, and Daniel Straub. 2018. ‚ÄúManaging Uncertainty in Flood Protection Planning with Climate Projections.‚Äù Hydrology and Earth System Sciences 22 (4): 2511‚Äì26. https://doi.org/10.5194/hess-22-2511-2018.\n\n\nLafferty, David C., and Ryan L. Sriver. 2023. ‚ÄúDownscaling and Bias-Correction Contribute Considerable Uncertainty to Local Climate Projections in CMIP6.‚Äù Npj Climate and Atmospheric Science 6 (1, 1): 1‚Äì13. https://doi.org/10.1038/s41612-023-00486-0.\n\n\nRackauckas, Christopher, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. 2020. ‚ÄúUniversal Differential Equations for Scientific Machine Learning.‚Äù 2020. https://doi.org/10.48550/ARXIV.2001.04385.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Physics-Based Models and Calibration üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html",
    "href": "chapters/hazard/sampling.html",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Monte Carlo Methods - Optimization",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#learning-objectives",
    "href": "chapters/hazard/sampling.html#learning-objectives",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nApply sampling techniques to generate synthetic event sets (e.g., hurricanes, floods).\nUse importance and stratified sampling to improve efficiency in hazard modeling.\nEvaluate how sampling choices affect estimates of extreme risk.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#monte-carlo-sampling-and-variance-reduction",
    "href": "chapters/hazard/sampling.html#monte-carlo-sampling-and-variance-reduction",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "12.1 Monte Carlo sampling and variance reduction",
    "text": "12.1 Monte Carlo sampling and variance reduction",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#importance-sampling-stratified-sampling",
    "href": "chapters/hazard/sampling.html#importance-sampling-stratified-sampling",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "12.2 Importance sampling, stratified sampling",
    "text": "12.2 Importance sampling, stratified sampling",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#synthetic-event-generation-hurricane-tracks-extreme-rainfall-patterns",
    "href": "chapters/hazard/sampling.html#synthetic-event-generation-hurricane-tracks-extreme-rainfall-patterns",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "12.3 Synthetic event generation (hurricane tracks, extreme rainfall patterns)",
    "text": "12.3 Synthetic event generation (hurricane tracks, extreme rainfall patterns)",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#balancing-computational-cost-vs.-accuracy-in-climate-risk-estimation",
    "href": "chapters/hazard/sampling.html#balancing-computational-cost-vs.-accuracy-in-climate-risk-estimation",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "12.4 Balancing computational cost vs.¬†accuracy in climate risk estimation",
    "text": "12.4 Balancing computational cost vs.¬†accuracy in climate risk estimation",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#further-reading",
    "href": "chapters/hazard/sampling.html#further-reading",
    "title": "12¬† Optimal Sampling Methods üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Optimal Sampling Methods üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html",
    "href": "chapters/hazard/sensitivity-analysis.html",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Monte Carlo Methods - Physics-Based Models and Calibration",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#learning-objectives",
    "href": "chapters/hazard/sensitivity-analysis.html#learning-objectives",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the role of sensitivity analysis in climate risk modeling\nApply variance-based sensitivity methods (Sobol indices) to identify key model parameters\nUse Morris screening methods for initial parameter importance ranking\nInterpret sensitivity analysis results for model simplification and uncertainty reduction\nApply sensitivity analysis to complex model chains and multi-model ensembles",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#introduction",
    "href": "chapters/hazard/sensitivity-analysis.html#introduction",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nGlobal sensitivity analysis (GSA) is essential for understanding which parameters, inputs, or model components contribute most to uncertainty in climate risk assessments. Unlike local sensitivity analysis that examines parameter effects around a single point, GSA explores the entire parameter space to provide robust insights into model behavior.\n\n13.1.1 Why Sensitivity Analysis Matters\nIn climate risk modeling, we often work with:\n\nHigh-dimensional parameter spaces: Climate models may have dozens to hundreds of parameters\nComplex model chains: From global climate models to local impact models\nComputational constraints: Limited budget for model evaluations\nDecision-making needs: Must identify which uncertainties matter most for risk management\n\nKey questions GSA helps answer:\n\nWhich parameters contribute most to output uncertainty?\nWhich parameters can be fixed without significantly affecting results?\nHow do parameter interactions affect model behavior?\nWhere should we focus calibration and uncertainty reduction efforts?",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#variance-based-sensitivity-analysis",
    "href": "chapters/hazard/sensitivity-analysis.html#variance-based-sensitivity-analysis",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.2 Variance-Based Sensitivity Analysis",
    "text": "13.2 Variance-Based Sensitivity Analysis\n\n13.2.1 Sobol Indices\nThe most widely used approach for GSA decomposes output variance based on input contributions:\n\\[\nV(Y) = \\sum_{i} V_i + \\sum_{i&lt;j} V_{ij} + \\sum_{i&lt;j&lt;k} V_{ijk} + \\ldots + V_{1,2,\\ldots,k}\n\\]\nwhere \\(V_i\\) represents the variance contribution from parameter \\(i\\) alone, \\(V_{ij}\\) from the interaction between parameters \\(i\\) and \\(j\\), etc.\nFirst-order Sobol index: \\[\nS_i = \\frac{V_i}{V(Y)} = \\frac{V[\\mathbb{E}[Y|X_i]]}{V(Y)}\n\\]\nTotal-effect index: \\[\nS_T^i = \\frac{V_i + \\sum_{j \\neq i} V_{ij} + \\sum_{j \\neq i, k \\neq i, j \\neq k} V_{ijk} + \\ldots}{V(Y)}\n\\]\n\n\n13.2.2 Computational Methods\nSaltelli sampling scheme efficiently estimates Sobol indices using structured sampling matrices.\nSample size requirements: For \\(k\\) parameters, need \\((k+2) \\times N\\) model evaluations where \\(N\\) is the base sample size.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#morris-elementary-effects",
    "href": "chapters/hazard/sensitivity-analysis.html#morris-elementary-effects",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.3 Morris Elementary Effects",
    "text": "13.3 Morris Elementary Effects\nFor computationally expensive models, Morris method provides screening at lower cost:\n\\[\nEE_i^{(j)} = \\frac{f(x_1, \\ldots, x_i + \\Delta, \\ldots, x_k) - f(x_1, \\ldots, x_i, \\ldots, x_k)}{\\Delta}\n\\]\nMorris measures: - \\(\\mu^*_i\\): Mean of absolute elementary effects (overall importance) - \\(\\sigma_i\\): Standard deviation of elementary effects (interactions/non-linearity)",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#applications-in-climate-risk",
    "href": "chapters/hazard/sensitivity-analysis.html#applications-in-climate-risk",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.4 Applications in Climate Risk",
    "text": "13.4 Applications in Climate Risk\n\n13.4.1 Case Study 1: Hydrologic Model Calibration\nProblem: Identify most important parameters in distributed watershed model\nApproach: 1. Define parameter ranges based on physical constraints 2. Apply Morris screening to eliminate unimportant parameters\n3. Use Sobol analysis on reduced parameter set 4. Focus calibration on high-sensitivity parameters\nResults: Typically find 5-10 parameters explain 80%+ of output variance\n\n\n13.4.2 Case Study 2: Storm Surge Model Chain\nModel chain components: 1. Tropical cyclone track generator 2. Wind field model\n3. Storm surge model 4. Sea level rise projections\nGSA insights: - Storm intensity parameters dominate surge height uncertainty - Track parameters most important for timing - Sea level rise becomes dominant for multi-decadal planning\n\n\n13.4.3 Case Study 3: Drought Risk Assessment\nIntegrated modeling system: - Climate model projections - Hydrologic model - Water system operations model - Economic impact model\nKey findings: - Climate model uncertainty dominates at seasonal scales - Hydrologic parameters important for extreme events - Operations parameters critical for economic impacts",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#advanced-methods",
    "href": "chapters/hazard/sensitivity-analysis.html#advanced-methods",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.5 Advanced Methods",
    "text": "13.5 Advanced Methods\n\n13.5.1 Multi-model Sensitivity Analysis\nWhen working with ensemble of models:\n\\[\nV(Y) = V[\\mathbb{E}[Y|M]] + \\mathbb{E}[V[Y|M]]\n\\]\nDecomposes uncertainty into: - Between-model variance: Structural uncertainty - Within-model variance: Parameter uncertainty\n\n\n13.5.2 Time-Varying Sensitivity\nFor dynamic systems, sensitivity indices may change over time: - Use rolling window analysis - Apply functional data analysis methods - Consider time-integrated sensitivity measures",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#implementation-guidelines",
    "href": "chapters/hazard/sensitivity-analysis.html#implementation-guidelines",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.6 Implementation Guidelines",
    "text": "13.6 Implementation Guidelines\n\n13.6.1 Sampling Strategy\n\nParameter space definition: Use physical bounds and expert knowledge\nCorrelation handling: Account for parameter dependencies using copulas\nHierarchical sampling: For multi-scale problems, use nested sampling designs\n\n\n\n13.6.2 Computational Efficiency\n\nSurrogate modeling: Build emulators for expensive models\nAdaptive sampling: Focus sampling in high-sensitivity regions\nMulti-fidelity methods: Combine high and low-fidelity model evaluations\n\n\n\n13.6.3 Interpretation and Communication\n\nThreshold values: \\(S_i &gt; 0.1\\) often considered ‚Äúimportant‚Äù\nInteraction effects: \\(S_T^i - S_i\\) indicates interaction strength\nVisualization: Use radar plots, heatmaps for multi-output problems",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#software-and-tools",
    "href": "chapters/hazard/sensitivity-analysis.html#software-and-tools",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "13.7 Software and Tools",
    "text": "13.7 Software and Tools\n\n13.7.1 R Packages\n\nsensitivity: Comprehensive GSA methods\nSALib (Python): Popular sensitivity analysis library\n\n\n\n13.7.2 Specialized Tools\n\nSAFE Toolbox (MATLAB/Python): Advanced methods\nOpenTURNS: Uncertainty quantification platform",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#further-reading",
    "href": "chapters/hazard/sensitivity-analysis.html#further-reading",
    "title": "13¬† Global Sensitivity Analysis üöß",
    "section": "Further reading",
    "text": "Further reading\nKey References: - Saltelli et al. (2008): Comprehensive introduction to GSA - Herman and Usher (2017): Practical implementation guide - Razavi et al. (2020): Review of GSA in environmental modeling\nClimate Applications:\n\n\n\n\nHerman, Jon, and Will Usher. 2017. ‚ÄúSALib: An Open-Source Python Library for Sensitivity Analysis.‚Äù Journal of Open Source Software 2 (9): 97. https://doi.org/10.21105/joss.00097.\n\n\nRazavi, Saman, Anthony Jakeman, Andrea Saltelli, Cl√©mentine Prieur, Bertrand Iooss, Emanuele Borgonovo, Elmar Plischke, et al. 2020. ‚ÄúThe Future of Sensitivity Analysis: An Essential Discipline for Systems Modeling and Policy Support.‚Äù Environmental Modelling & Software, December, 104954. https://doi.org/10.1016/j.envsoft.2020.104954.\n\n\nSaltelli, Andrea, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. 2008. Global Sensitivity Analysis: The Primer. John Wiley & Sons, Ltd. http://onlinelibrary.wiley.com/doi/abs/10.1002/9780470725184.ch1.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Global Sensitivity Analysis üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html",
    "href": "chapters/risk/exposure-vulnerability.html",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "",
    "text": "14.1 Learning objectives\nBy the end of this chapter, you should be able to:",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#learning-objectives",
    "href": "chapters/risk/exposure-vulnerability.html#learning-objectives",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "",
    "text": "Define exposure and vulnerability in the context of climate risk assessment\nDistinguish between different types of vulnerability (physical, social, economic)\nUnderstand methods for quantifying and mapping exposure\nApply vulnerability assessment frameworks to real-world scenarios\nIntegrate exposure and vulnerability data with hazard information for risk assessment",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#introduction",
    "href": "chapters/risk/exposure-vulnerability.html#introduction",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.2 Introduction",
    "text": "14.2 Introduction\nGiven some hazard, how do we assess the damages or impacts?\nWe can quantify this, for example as \\[\n\\textrm{Damage} = \\textrm{Vulnerability} \\times \\textrm{Exposure} \\times \\textrm{Hazard}\n\\]\nLet‚Äôs understand these terms through some examples!\nClimate risk emerges from the intersection of three components: hazard, exposure, and vulnerability. While Part II focused on characterizing climate hazards, this chapter addresses the other two critical components that determine how hazards translate into actual risks and impacts.\nExposure refers to the people, livelihoods, species, ecosystems, environmental functions, services, resources, infrastructure, or economic, social, or cultural assets in places that could be adversely affected by climate hazards.\nVulnerability encompasses the conditions determined by physical, social, economic, and environmental factors that increase the susceptibility of an individual, community, assets, or systems to the impacts of hazards.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#illustrative-examples",
    "href": "chapters/risk/exposure-vulnerability.html#illustrative-examples",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.3 Illustrative examples",
    "text": "14.3 Illustrative examples\n\n14.3.1 Structural fragility curve\n\\[\n\\Pr(\\textrm{failure}) = f(\\textrm{hazard}, \\theta)\n\\]\n\n\n\n\n\n\nFigure¬†14.1: Parameterized fragilities as a function of surge and wave height for MSSS concrete bridges in South Carolina (a). (b): fragility curves for bridges subjected to tsunami hazard for low (solid lines) and moderate (dashed lines) flow rates (Gidaris et al. 2017).\n\n\n\n\n\n14.3.2 Flood depth-damage curve\n\n\n\n\n\n\nFigure¬†14.2: Boxplots show Wing et al. (2020) analysis of insurance claims. Lines show USACE depth-damage curves.\n\n\n\n\n\n14.3.3 Seawall cost-benefit analysis\nFor a given storm, the total damages can be estimated by summing the damages for each property. \\[\n\\textrm{Total damages} = \\sum_{i \\in \\, \\textrm{exposure}} \\textrm{Hazard}_i \\times \\textrm{Vulnerability}_i\n\\]\n\n\n\n\n\n\nFigure¬†14.3: Proposed ‚ÄúIke Dike‚Äù\n\n\n\n\n\n14.3.4 Insurance portfolio risk\nThis is the most straightforward example of the exposure-vulnerability-hazard framework\n\nHazard: a real or synthetic storm\nExposure: all the assets that are insured\nVulnerability: predicted losses as a function of the hazard",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#types-of-exposure",
    "href": "chapters/risk/exposure-vulnerability.html#types-of-exposure",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.4 Types of Exposure",
    "text": "14.4 Types of Exposure\n\n14.4.1 Trends in exposure\n\n\n\n\n\n\nFigure¬†14.4: Global asset exposure to river and coastal flooding using the population and land-use methods (Jongman, Ward, and Aerts 2012).\n\n\n\n\n\n14.4.2 Example: North Carolina\n\n\n\n\n\n\nFigure¬†14.5: Spatial distribution of the properties within our database that were built during the (a) 1800‚Äì1900, (b) 1900‚Äì1950, (c) 1950‚Äì2000 and (d) 2000‚Äì2018 periods (Tedesco, McAlpine, and Porter 2020).\n\n\n\n\n\n14.4.3 Physical Exposure\nPhysical exposure involves the presence of people, infrastructure, housing, production capacities and other tangible human assets located in hazard-prone areas.\n\n\n14.4.4 Economic Exposure\nEconomic exposure refers to the economic value of assets that could be affected by climate hazards. This includes direct economic assets as well as economic activities that depend on climate-sensitive resources.\n\n\n14.4.5 Social Exposure\nSocial exposure encompasses the social systems, networks, and populations that could be affected by climate hazards.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#vulnerability-assessment",
    "href": "chapters/risk/exposure-vulnerability.html#vulnerability-assessment",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.5 Vulnerability Assessment",
    "text": "14.5 Vulnerability Assessment\n\n14.5.1 Changing vulnerability\nInterventions such as floodproofing can shift the vulnerability curve\n\n\n\n\n\n\nFigure¬†14.6: Floodproofing in Houston, TX (Houston Public Media)\n\n\n\n\n\n14.5.2 Physical Vulnerability\nPhysical vulnerability refers to the degree of damage that a specific asset or system would experience when exposed to a hazard of a given intensity.\n\n\n14.5.3 Social Vulnerability\nSocial vulnerability reflects the characteristics of people and communities that influence their capacity to prepare for, respond to, and recover from climate hazards.\n\n\n14.5.4 Economic Vulnerability\nEconomic vulnerability encompasses the economic factors that affect the ability to cope with and recover from climate impacts.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#quantifying-exposure-and-vulnerability",
    "href": "chapters/risk/exposure-vulnerability.html#quantifying-exposure-and-vulnerability",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.6 Quantifying Exposure and Vulnerability",
    "text": "14.6 Quantifying Exposure and Vulnerability\n\n14.6.1 Exposure Mapping\nMethods for mapping and quantifying exposure include: - Asset inventories - Population databases - Land use and land cover data - Economic activity data\n\n\n14.6.2 Vulnerability Indices\nApproaches to measuring vulnerability: - Composite vulnerability indices - Indicator-based assessments - Survey-based methods - Participatory vulnerability assessments",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#mathematical-framework-for-expected-damages",
    "href": "chapters/risk/exposure-vulnerability.html#mathematical-framework-for-expected-damages",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.7 Mathematical framework for expected damages",
    "text": "14.7 Mathematical framework for expected damages\n\n14.7.1 Expectation\nThe expected value of a function \\(f(x)\\) of a random variable \\(x\\) is: \\[\n\\mathbb{E}[f(x)] = \\int f(x) p(x) dx\n\\] where \\(p(x)\\) is the probability density function of \\(x\\).\nIn other words, for each possible value of \\(x\\), calculate \\(f(x)\\). We then take a weighted average, where the weights are the probability of each value of \\(x\\).\n\n\n14.7.2 Key insight\nYou cannot get the expected value of a function by plugging the expected value of the random variable into the function. \\[\n\\mathbb{E}[f(x)] \\neq f(\\mathbb{E}[x])\n\\]\n\n\n14.7.3 Expected damages\nIf we want to calculate the expected damages then we can use this formula \\[\n\\mathbb{E}[f(x)] = \\int f(x) p(x) dx\n\\] by defining \\(x\\) to be the hazard (e.g., flood depth) and \\(f(x)\\) the damage function.\n\n\n\n\n\n\nThe key assumption we need is that we know the probability density function \\(p(x)\\)!\n\n\n\n\n\n14.7.4 Monte Carlo approximation\nIn general, \\(\\int f(x) p(x) dx\\) is hard to compute analytically. We can use Monte Carlo methods to approximate this integral.\n\n\n\n\n\n\nMonte Carlo methods are a wide class of computational algorithms for approximating integrals.\n\n\n\n\n\n14.7.5 Monte Carlo expectation\nIf we draw \\(N\\) samples independently and identically distributed (i.i.d.) from a probability distribution \\(p(x)\\), denoted as \\(\\left\\{x_i\\right\\}_{i=1}^N\\) where \\(x_i \\sim p(x)\\), then \\[\n\\mathbb{E}[f(x)] \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n\\]\n\n\n\n\n\n\nDrawing \\(N\\) samples iid from \\(p(x)\\) can be inefficient. Most of the samples will be in regions where \\(f(x)\\) is small, so we are wasting computational effort. There are many clever ways around this.\n\n\n\n\n\n14.7.6 For example: Trapezoidal EAD\nRunning regional flood models is computationally expensive. Often, a model may have been run for a few different nominal return levels. For example, we might have flood depths at each grid for the nominal 10, 25, 50, 100, 250, and 500 year floods.\n\n\n\n\n\n\nFigure¬†14.7: de Moel, van Vliet, and Aerts (2014) uses trapezoidal methods to approximate the expected annual damages.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#integrating-with-hazard-information",
    "href": "chapters/risk/exposure-vulnerability.html#integrating-with-hazard-information",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.8 Integrating with Hazard Information",
    "text": "14.8 Integrating with Hazard Information\nThe combination of hazard, exposure, and vulnerability information enables comprehensive risk assessment through: - Risk mapping - Loss estimation - Scenario analysis - Uncertainty quantification",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#key-points",
    "href": "chapters/risk/exposure-vulnerability.html#key-points",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.9 Key points",
    "text": "14.9 Key points\n\nImpacts (damage) depend on hazard, exposure, and vulnerability.\nExposure and vulnerability are changing rapidly.\nTo estimate risk, we need a model for how our system responds to a hazard.\nIf we know the probability density function of the hazard, we can use Monte Carlo methods to estimate expected impacts.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#limitations-of-this-framework",
    "href": "chapters/risk/exposure-vulnerability.html#limitations-of-this-framework",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.10 Limitations of this framework",
    "text": "14.10 Limitations of this framework\n\nHow to think about damages that are not direct damages (e.g., ‚Äúindirect‚Äù damages)?\nHow to think about complex systems and risks (e.g., breadbasket failures) that are not easy summarized by a single metric?",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#case-studies",
    "href": "chapters/risk/exposure-vulnerability.html#case-studies",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.11 Case Studies",
    "text": "14.11 Case Studies\nContent to be added",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#further-reading",
    "href": "chapters/risk/exposure-vulnerability.html#further-reading",
    "title": "14¬† Exposure and Vulnerability üöß",
    "section": "14.12 Further reading",
    "text": "14.12 Further reading\nReferences to be added\n\n\n\n\nGidaris, Ioannis, Jamie E. Padgett, Andre R. Barbosa, Suren Chen, Daniel Cox, Bret Webb, and Amy Cerato. 2017. ‚ÄúMultiple-Hazard Fragility and Restoration Models of Highway Bridges for Regional Risk and Resilience Assessment in the United States: State-of-the-Art Review.‚Äù Journal of Structural Engineering 143 (3): 04016188. https://doi.org/10.1061/(ASCE)ST.1943-541X.0001672.\n\n\nJongman, Brenden, Philip J Ward, and Jeroen C J H Aerts. 2012. ‚ÄúGlobal Exposure to River and Coastal Flooding: Long Term Trends and Changes.‚Äù Global Environmental Change 22 (4): 823‚Äì35. https://doi.org/10.1016/j.gloenvcha.2012.07.004.\n\n\nMoel, Hans de, Mathijs van Vliet, and Jeroen C. J. H. Aerts. 2014. ‚ÄúEvaluating the Effect of Flood Damage-Reducing Measures: A Case Study of the Unembanked Area of Rotterdam, the Netherlands.‚Äù Regional Environmental Change 14 (3): 895‚Äì908. https://doi.org/10.1007/s10113-013-0420-z.\n\n\nTedesco, Marco, Steven McAlpine, and Jeremy R. Porter. 2020. ‚ÄúExposure of Real Estate Properties to the 2018 Hurricane Florence Flooding.‚Äù Natural Hazards and Earth System Sciences 20 (3): 907‚Äì20. https://doi.org/10.5194/nhess-20-907-2020.\n\n\nWing, Oliver E. J., Nicholas Pinter, Paul D. Bates, and Carolyn Kousky. 2020. ‚ÄúNew Insights into US Flood Vulnerability Revealed from Flood Insurance Big Data.‚Äù Nature Communications 11 (1, 1): 1444. https://doi.org/10.1038/s41467-020-15264-2.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Exposure and Vulnerability üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html",
    "href": "chapters/risk/expectations-cost-benefit.html",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#learning-objectives",
    "href": "chapters/risk/expectations-cost-benefit.html#learning-objectives",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "",
    "text": "Understand the theoretical foundation of cost-benefit analysis and Bayesian decision theory\nApply net present value calculations with appropriate discount rates\nQuantify costs and benefits using utility functions for climate risk decisions\nHandle uncertainty in cost-benefit frameworks using expected value\nRecognize the limitations and appropriate applications of cost-benefit analysis\nEvaluate economic trade-offs over different time horizons and scenarios\n\n\nSee first\n\nExpectations\nProbability and Statistics",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#motivation",
    "href": "chapters/risk/expectations-cost-benefit.html#motivation",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.1 Motivation",
    "text": "15.1 Motivation\nWe often want a quantitative way to compare two or more decisions. Hence, cost-benefit analysis.\nIt‚Äôs a simple idea. For a given ‚Äúdecision‚Äù (being deliberately vague about what we mean by this), we need some function that tells us how good or bad the decision is. Then, we can compare the goodness of different decisions.\n\n\n\n\n\n\nAs a motivating example, consider that we are have been asked to help a homeowner decide whether to elevate their home by 5ft to protect against future flooding, or whether to leave it as-is.\n\n\n\n\n\n\n\n\n\nFigure¬†15.1: Floodproofing in Houston, TX (Houston Public Media)",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#quantifying-costs-and-benefits",
    "href": "chapters/risk/expectations-cost-benefit.html#quantifying-costs-and-benefits",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.2 Quantifying costs and benefits",
    "text": "15.2 Quantifying costs and benefits\nWe want to pick the decision that maximizes the ‚Äúgoodness‚Äù of the decision. We‚Äôll call this a ‚Äúutility‚Äù function, but could also use ‚Äúobjective function‚Äù, ‚Äúloss function‚Äù, ‚Äúdamage function‚Äù, etc. Let‚Äôs give this some formal notation: \\[\nu(a, \\mathbf{s}): \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}\n\\] where \\(u\\) is the utility function, \\(a \\in \\mathcal{A}\\) is the decision, and \\(\\mathbf{s} \\in \\mathcal{S}\\) is the state of the world.\n\nWe‚Äôll use ‚Äústate of the world‚Äù as a deliberately vague term that we will use to describe all the uncertainties we want to think about when making a decision.\nYou can remember that \\(a\\) is the decision because it‚Äôs the first letter of ‚Äúaction‚Äù.\n\nOften, utility is measured in money. This is not because money is the only thing that matters, but because it is a common unit of account that allows us to compare different things. However, utility can be measured in other units as well ‚Äì it only requires that we put all the costs and benefits into a common unit.\n\n\n\n\n\n\nIn our house elevation example, we might consider the following costs and benefits: up-front cost of elevating; cost of flood insurance; change in property value; and cost of damages not covered by flood insurance. These might be well-described in monetary terms. However, putting a value on the peace of mind from not having to worry about future flooding, or the reduced risk of death, is trickier.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#dealing-with-time-discounting",
    "href": "chapters/risk/expectations-cost-benefit.html#dealing-with-time-discounting",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.3 Dealing with time / Discounting",
    "text": "15.3 Dealing with time / Discounting\nA key feature of nearly all climate adaptation problems is that costs and benefits are spread out over time. How should we weigh costs and benefits that occur at different times?\n\n\n\n\n\n\nFor example, the up-front cost of elevating a house is a cost that occurs now, while the benefits of reduced flood risk occur in the future.\n\n\n\nThe most common way to deal with this is to use net present value (NPV). The idea is to discount future costs and benefits to the present day. If our discount rate is \\(\\gamma = 2\\% = 0.02\\), then we care about a dollar of benefits in one year the same as we care about 98 cents today. More generally, if we have a discount rate of \\(\\gamma\\) then a dollar of benefits in \\(t\\) years is worth \\((1 - \\gamma) ^ t\\) today.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#net-present-value-npv",
    "href": "chapters/risk/expectations-cost-benefit.html#net-present-value-npv",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.4 Net present value (NPV)",
    "text": "15.4 Net present value (NPV)\nThe net present value of a decision is the sum of the present values of all costs and benefits: \\[\nNPV = \\sum_{t=0}^T (1 - \\gamma)^t u(a, \\mathbf{s}_t)\n\\] where \\(T\\) is the time horizon of the decision. Note that we write \\(\\mathbf{s}_t\\) to indicate that the state of the world might have time-dependent variables.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#cost-benefit-analysis",
    "href": "chapters/risk/expectations-cost-benefit.html#cost-benefit-analysis",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.5 Cost-Benefit Analysis",
    "text": "15.5 Cost-Benefit Analysis\nCost-benefit analysis is everywhere! Companies use it to decide whether to invest in new products or technologies, governments use it to decide whether to build new infrastructure or regulate pollution, and much more!\nA standard approach is:\n\nCome up with a state of the world \\(\\mathbf{s}\\) that represents your uncertainties.\nWrite down a utility function \\(u(a, \\mathbf{s})\\) that represents your preferences.\nChoose a discount rate \\(\\gamma\\)\nCalculate the net present value of each decision\nPick the decision with the highest net present value",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#integrating-uncertainty-in-cost-benefit-frameworks",
    "href": "chapters/risk/expectations-cost-benefit.html#integrating-uncertainty-in-cost-benefit-frameworks",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.6 Integrating uncertainty in cost-benefit frameworks",
    "text": "15.6 Integrating uncertainty in cost-benefit frameworks\nBayesian decision theory is a mathematical formalization of a simple concept: decision that gives us the best utility on average (i.e., in expectation):\n\\[\na^* = \\arg \\max_a \\mathbb{E}_\\mathbf{s} \\left[ u(a, \\mathbf{s}) \\right]\n\\] where \\(a \\in \\mathcal{A}\\) is the decision, \\(\\mathbf{s} \\in \\mathcal{S}\\) is the state of the world, and \\(u: \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}\\) is a utility function. ‚ÄúState of the world‚Äù is another deliberately vague term that we will use to describe all the uncertainties we want to think about when making a decision.\n\n\n\n\n\n\nFor our house elevation problem, a very simple framing would consider the state of the world to be the time series of future flood levels. A more complex framing might also consider the depth-damage function for the house, the cost of elevating the house, the cost of rebuilding a house over time, etc. etc.\n\n\n\nRecall the definition of expected value: \\[\n\\mathbb{E}_\\mathbf{s} \\left[ u(a, \\mathbf{s}) \\right] = \\int p(\\mathbf{s}) u(a, \\mathbf{s}) d\\mathbf{s}\n\\] which requires a probability distribution over states of the world. So applying this theory requires a ‚Äúsubjective‚Äù (Savage 1954) or ‚Äúpersonal‚Äù probability distribution over states of the world and a utility function.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#critiques-and-limitations",
    "href": "chapters/risk/expectations-cost-benefit.html#critiques-and-limitations",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.7 Critiques and limitations",
    "text": "15.7 Critiques and limitations\n\nSimple idea:\n\nAdd up all the costs\nAdd up all the benefits\n\nSometimes it‚Äôs hard to combine different things that we care about into a single number!\n\nCost and safety\nImpacts on different groups of people\nWe will revisit this in the context of multi-criteria decision analysis.\n\nIn practice, this often leads us to care only about costs and benefits that are easy to quantify / monetize\n\nValue of ecosystems?\n\nThe limitations of discounting are especially relevant for some types of climate adaptation decisions\n\nWe will revisit this on Wednesday\n\nWe often deal with ‚Äúdeep‚Äù uncertainties for which it‚Äôs hard to come up with a probability distribution\n\nWe will revisit this much later in the semester\n\nIs Bayesian decision theory a good model of how people actually make decisions?\n\nThis framework was largely formalized by Savage (1954), who postulated that people often behave as though maximizing expected utility\nEllsberg (1961) and others: this is not how people really make decisions!\nOur goal is to support decision-making, not predict how people will actually make decisions. So the fact that this is not how people actually make decisions is not a problem for us.\nThat said: when our predictions of ‚Äúwhat is rational‚Äù diverge from what people actually do, we should be curious about why instead of assuming they are stupid!",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#a-defense",
    "href": "chapters/risk/expectations-cost-benefit.html#a-defense",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "15.8 A defense",
    "text": "15.8 A defense\nCost-benefit analysis is still useful when applied thoughtfully.\n\nIt forces us to be explicit about our assumptions\n\nWhat we care about and how we are measuring it\nWhat we are ignoring\nHow we think about uncertainty\n\nAllows an apples-to-apples comparison of different decisions\n\nUltimately, cost-benefit analysis is a great decision-support tool, but it is not a decision-making tool. When applied well, it‚Äôs an iterative process through which we repeatedly refine our understanding of the decision problem. When applied poorly, it‚Äôs a black-box process that spits out a number that is used to justify a decision that was already made.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#further-reading",
    "href": "chapters/risk/expectations-cost-benefit.html#further-reading",
    "title": "15¬† Cost-Benefit Analysis and Net Present Value üöß",
    "section": "Further reading",
    "text": "Further reading\n\n\n\n\nEllsberg, Daniel. 1961. ‚ÄúRisk, Ambiguity, and the Savage Axioms.‚Äù The Quarterly Journal of Economics 75 (4): 643‚Äì69. https://doi.org/10.2307/1884324.\n\n\nSavage, L. J. 1954. Foundations of Statistics. New York: Wiley.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html",
    "href": "chapters/risk/policy-search.html",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Optimization - Expectations and Discounting",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#learning-objectives",
    "href": "chapters/risk/policy-search.html#learning-objectives",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nFormulate policy design problems as optimization tasks.\nExplore multi-objective policy search (e.g., cost, emissions, equity).\nEvaluate strategies (carbon taxes, cap-and-trade) under uncertainty.\n\nYou‚Äôll want to refer heavily to the chapter on optimization",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#overview",
    "href": "chapters/risk/policy-search.html#overview",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "16.1 Overview",
    "text": "16.1 Overview\n\n16.1.1 Review of notation\nNotation for our system dynamics:\n\nState of the world: encapsulates all inputs to our model\nDecisions: can be very simple (how high do we elevate a house right now?) or very complex (spatial and/or temporal optimization problems)\nOutcomes: can be a single number (scalar) or a vector if there are multiple outcomes we care about\n\n\n\n16.1.2 Key components of an optimization problem\n\nObjective function\nConstraints\nDecision variables\n\n\n\n\n\n\n\nReflect\n\n\n\nTo what extent is this [in]consistent with exploratory modeling?\n\n\n\n\n16.1.3 Why optimize?\nLarge action spaces (many decision variables) make it difficult to find the best solution by trial and error.\n\n\n\n\n\n\nToday we‚Äôll say optimization but even an exact solution is only optimal in our model, not the real world. I prefer the term policy search which emphasizes the use of computers to suggest promising strategies.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#optimization-in-the-wild",
    "href": "chapters/risk/policy-search.html#optimization-in-the-wild",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "16.2 Optimization in the wild",
    "text": "16.2 Optimization in the wild\n\n16.2.1 Where have you seen optimization used?\n\n\n\n\n\n\nReflect\n\n\n\nTake 2-3 minutes, then share.\n\n\n\n\n16.2.2 Linear programming\nFind a vector \\(\\mathbf{x}\\) that maximizes \\(c^T \\mathbf{x}\\) subject to \\(A \\mathbf{x} \\leq \\mathbf{b}\\) and \\(\\mathbf{x} \\geq 0\\)\n\nLimitations: requires strong assumptions (is linearizing your function a good approximation?)\nStrengths: very fast (can scale to large problems)\nExamples: how much should each pump in a water distribution network be run at a given time step to maintain pressure?\n\n\n\n16.2.3 Linear programming with discrete decisions\n\nFixed costs create discontinuities in the objective function\nExample: which electricity generators should be on/off?\nNeed to create new indicator variables which flag on/off status: \\(\\mathbb{I}_i = \\begin{cases} 0 & \\textrm{off} \\\\ 1 & \\textrm{on} \\end{cases}\\).\nCan be solved with mixed-integer linear programming (MILP)\n\n\n\n16.2.4 Gradient descent\nIf you have a differentiable function, you can use gradient descent to find the minimum.\n\\[\n\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha \\nabla f(\\mathbf{x}_n)\n\\]\n\n\n16.2.5 Simulation-optimization\n\nStrengths: can handle complex, non-linear systems (model can be a black box)\nLimitations: slow (‚Äúguess and check‚Äù), rely on ‚Äúheuristics‚Äù to decide a solution is good enough\nExamples: design of water resource systems under uncertainty",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#key-points",
    "href": "chapters/risk/policy-search.html#key-points",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "16.3 Key points",
    "text": "16.3 Key points\n\nOptimization can be used at a high level (e.g., system design) or can be embedded in a problem (e.g., operations at each time step).\nEvery optimization problem has an objective and decision variables. Many have constraints.\nOptimization is a field, with many techniques.\nIn this course, I want you to understand and critique how optimization problems are framed in the wild. Take other courses to focus on the techniques.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#multiobjective-policy-search-and-optimization",
    "href": "chapters/risk/policy-search.html#multiobjective-policy-search-and-optimization",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "16.4 Multiobjective Policy Search and Optimization",
    "text": "16.4 Multiobjective Policy Search and Optimization\n\n16.4.1 Enriching our notation\nWhen we move from single to multiple objectives, our system dynamics notation becomes more complex: - We may have multiple outcome functions rather than a single scalar outcome\n\n\n16.4.2 Single-objective policy search\nUsing this notation, we want to \\(\\min / \\max g(a)\\), subject to constraints.\nIn other words, we want a single solution that scores the best according to \\(g(a)\\).\n\n\n16.4.3 Objectives can be hard to combine\nSometimes, combining objectives into a single function is difficult.\n\nOperating a reservoir to maximize power generation, minimize flood risk, and supply water to a city.\nDesigning a levee to balance cost, financial flood risk, ecological impact, and human safety.\n\nAddressing multiple, sometimes-competing, needs is often called ‚Äúmulti-criteria decision analysis‚Äù\n\n\n16.4.4 Goals\nGoal: find a set of solutions that are not dominated by any other solution\nWe call this the Pareto front or Pareto set.\n\n\n16.4.5 Mathematical formulation\n\\[\n\\begin{align}\n& \\min / \\max f_m(x), & \\quad m = 1, 2, \\ldots, M \\\\\n\\text{subject to} \\quad &g_j(x) \\leq 0, &\\quad  j = 1, 2, \\ldots, J \\\\\n&h_k(x) = 0, &\\quad  k = 1, 2, \\ldots, K \\\\\n&x_i^{(L)} \\leq x_i \\leq x_i^{(U)}, &\\quad  i = 1, 2, \\ldots, N\n\\end{align}\n\\]\nWhat‚Äôs new? - \\(M\\) objective functions - Lots of notation for constraints\n\n\n16.4.6 Dominance\nConcept of Dominance: 1. Single-objective: Goodness of solution defined by objective function value 2. Multi-objective: Goodness of solution defined by dominance relationships\nDefinition: Solution \\(a_1\\) dominates solution \\(a_2\\) if: 1. \\(a_1\\) is no worse than \\(a_2\\) in all objectives 2. \\(a_1\\) is strictly better than \\(a_2\\) in at least one objective\nExample Analysis: Consider 5 solutions with objectives \\(f_1\\) (maximize) and \\(f_2\\) (minimize): - Solution 1: \\((2, 2)\\) - Solution 2: \\((1, 4)\\) - Solution 3: \\((4, 1)\\) - Solution 4: \\((3, 3)\\) - Solution 5: \\((5, 2)\\)\nDominance relationships: - Solution 1 vs 2: Solution 1 dominates (better in \\(f_1\\): 2&gt;1, better in \\(f_2\\): 2&lt;4) - Solution 1 vs 5: Solution 5 dominates (better in \\(f_1\\): 5&gt;2, same in \\(f_2\\): 2=2) - Solution 1 vs 4: Neither dominates (trade-offs exist)\nSolutions that are not dominated by any other feasible solution form the Pareto front.\n\n\n16.4.7 Goals of multiobjective optimization\n\nFind solutions as close to Pareto front as possible\nFind solutions as diverse as possible (since infinite points lie along the Pareto front but we can only sample a finite number)\n\n\n\n16.4.8 Implementation Approaches\n\n\n16.4.9 Weighted Sum Method\nWe can scalarize the multi-objective problem into a single-objective problem using a weighted sum:\n\\[\n\\min F(x) =\\sum_{m=1}^M w_m f_m(x)\n\\] subject to the same constraints as before.\nAdvantages: - Simplicity: Easy to understand and implement - Computational efficiency: Can use existing single-objective optimization algorithms - Pareto front exploration: Can vary weights systematically to explore trade-offs\nDisadvantages: - Weight selection: Requires a priori knowledge of preferences - Pareto front coverage: May miss parts of the Pareto front, especially in non-convex regions - Uniform weight distribution: Does not guarantee uniform distribution of solutions along Pareto front\nConvex vs.¬†Non-Convex Pareto Fronts:\nFor convex Pareto fronts, the weighted sum method can find any point on the front by varying weights.\nFor non-convex Pareto fronts, the weighted sum method cannot find solutions in concave regions - certain trade-offs are mathematically unreachable regardless of weight selection.\nThis limitation motivates the need for population-based methods like genetic algorithms.\n\n\n16.4.10 Genetic Algorithms for Multi-Objective Optimization\nEvolutionary Approach: Genetic algorithms are particularly well-suited for multi-objective optimization because they work with populations of solutions rather than single points.\nCore Principles: 1. Population-based: GAs operate on a set (‚Äúpopulation‚Äù or ‚Äúgeneration‚Äù) of candidate solutions 2. Evolutionary inspiration: The best solutions are more likely to survive and reproduce 3. Fitness evaluation: Compute a fitness score for each solution based on objectives and diversity 4. Selection and reproduction: Use fitness scores to select solutions for reproduction (crossover) 5. Mutation: Add controlled noise (mutation) to create the next ‚Äúgeneration‚Äù 6. Algorithmic variety: Many specific algorithms exist (NSGA-II, SPEA2, etc.)\nMulti-Objective Fitness: Unlike single-objective GAs, multi-objective versions must balance: - Convergence: Solutions should be close to the true Pareto front - Diversity: Solutions should be well-distributed along the Pareto front\nSelection Mechanisms: - Non-dominated sorting: Rank solutions by dominance levels - Crowding distance: Maintain diversity by favoring solutions in less crowded regions - Archive maintenance: Preserve best non-dominated solutions across generations\nAdvantages: - Global search: Can handle complex, multimodal objective spaces - Non-convex fronts: Can find solutions in concave regions of Pareto fronts - No weight specification: Does not require a priori preference information - Population diversity: Naturally maintains diverse set of trade-off solutions\nDisadvantages: - Computational cost: Requires many function evaluations - Parameter tuning: Multiple algorithm parameters need careful adjustment - Stochastic nature: Results may vary between runs - Convergence uncertainty: Difficult to guarantee global optimality\n\n\n16.4.11 A Population of Trade-off Solutions\nFinal Output: Multi-objective optimization produces a set of solutions rather than a single optimal solution.\nCharacteristics: 1. Non-dominated solutions: A set of solutions where no solution dominates any other 2. Trade-off representation: Each solution represents a different trade-off between objectives\n3. Pareto front coverage: Solutions collectively approximate different parts of the Pareto front\nInterpretation and Value: One key insight of multi-objective optimization is that it maps out the trade-offs between competing objectives.\nThis is valuable because: - Decision support: Shows range of feasible trade-offs to decision-makers - Trade-off quantification: Makes explicit the costs of improving one objective at expense of others - Robust analysis: Multiple good solutions provide options under different preference structures - Stakeholder engagement: Different solutions may appeal to different stakeholders or constituencies\nExample Applications: - Reservoir operation: Trade-offs between flood protection, water supply, and hydropower generation - Urban planning: Balance between development density, green space, and transportation access - Climate policy: Trade-offs between mitigation costs, emission reductions, and economic impacts\n\n\n16.4.12 Convergence Assessment and Quality Control\nChallenge of Heuristic Methods: Genetic algorithms are not exact optimization methods. They rely on heuristics to decide when a solution is ‚Äúgood enough‚Äù even if it‚Äôs not a global optimum. This creates challenges for assessing solution quality and algorithm convergence.\nQuality Assessment Strategies:\n1. Multiple Runs with Different Seeds: - Repeat optimization with different random initial conditions - Compare results across runs to assess robustness - If results vary significantly, may need longer runs or parameter adjustment\n2. Hypervolume Indicator: - Definition: Volume of objective space dominated by the solution set - Purpose: Measures both convergence (proximity to true front) and diversity (spread along front)\n- Application: Track hypervolume over generations to assess convergence - Advantages: Single metric combining multiple quality aspects\n3. Method Comparison: - Compare genetic algorithm results to weighted sum method - Check if GA finds solutions in regions missed by weighted sum (especially non-convex areas) - Cross-validate using different algorithmic approaches (NSGA-II, SPEA2, etc.)\n4. Benchmark Comparison: - Compare optimization results to human-generated or expert solutions - Verify that algorithm finds solutions that dominate known alternatives - Use analytical solutions for simplified test problems to validate implementation\n5. Convergence Monitoring: - Track solution set changes over generations - Monitor when new non-dominated solutions stop being found - Use metrics like generational distance and inverted generational distance\nPractical Implementation Guidelines: - Run length: Balance computational cost with solution quality - Population size: Larger populations provide better diversity but increase cost - Multiple objectives: More objectives exponentially increase difficulty - Archive management: Maintain external archive of best solutions found\n\n\n16.4.13 The challenge of Many Objectives\nDiminishing Returns from Additional Objectives:\nWhile it may seem beneficial to include all relevant objectives explicitly, there are significant drawbacks to adding more objectives:\nComputational Challenges: - Exponential scaling: The number of solutions needed grows exponentially with objectives - Pareto front explosion: In high dimensions, most solutions become non-dominated - Search difficulty: Algorithms struggle to maintain diversity across many objectives\nCognitive and Communication Challenges:\n- Conceptual complexity: Humans cannot easily visualize or understand trade-offs among many objectives - Decision paralysis: Too many options can make decision-making more difficult - Communication barriers: Difficult to present and discuss results with stakeholders\nThe ‚ÄúCurse of Dimensionality‚Äù in Objective Space: As the number of objectives increases, the volume of the Pareto front grows exponentially relative to the volume of the entire objective space. This makes it increasingly difficult to find truly inferior solutions, reducing the effectiveness of dominance-based selection.\nPractical Guidelines:\nObjective Consolidation: - Combine related objectives into composite indices when appropriate - Use weighted sums for objectives that can be meaningfully combined - Focus on the most critical and independent objectives\nObjective Hierarchy: - Distinguish between primary objectives (core concerns) and secondary objectives (constraints or preferences) - Consider using some objectives as constraints rather than explicit objectives - Implement lexicographic ordering for objectives with clear priority structure\nRule of Thumb: - 2-3 objectives: Manageable and interpretable - 4-6 objectives: Possible but requires careful analysis - &gt;6 objectives: Generally not recommended without specialized techniques\nWhen Many Objectives Are Necessary: - Use techniques like objective reduction or dimensionality reduction in objective space - Consider interactive optimization where decision-maker preferences guide the search - Apply many-objective evolutionary algorithms (e.g., NSGA-III) specifically designed for high-dimensional objective spaces\nConclusion: The goal is to find the minimum number of objectives that adequately capture the essential trade-offs in the decision problem. Quality over quantity applies strongly in multi-objective optimization.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#constraints-political-feasibility-budget-limits",
    "href": "chapters/risk/policy-search.html#constraints-political-feasibility-budget-limits",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "16.5 Constraints (political feasibility, budget limits)",
    "text": "16.5 Constraints (political feasibility, budget limits)\nContent to be developed",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/policy-search.html#further-reading",
    "href": "chapters/risk/policy-search.html#further-reading",
    "title": "16¬† Policy Search & Optimization üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Policy Search & Optimization üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html",
    "href": "chapters/risk/risk-transfer.html",
    "title": "17¬† Risk Transfer üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Exposure and Vulnerability - Expectations and Discounting",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#learning-objectives",
    "href": "chapters/risk/risk-transfer.html#learning-objectives",
    "title": "17¬† Risk Transfer üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nExplore financial instruments (insurance, reinsurance, catastrophe bonds) for spreading or transferring climate risk.\nUnderstand parametric insurance triggers and how they differ from indemnity-based approaches.\nAssess the role of public-private partnerships in risk transfer mechanisms.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#insurancereinsurance-fundamentals",
    "href": "chapters/risk/risk-transfer.html#insurancereinsurance-fundamentals",
    "title": "17¬† Risk Transfer üöß",
    "section": "17.1 Insurance/reinsurance fundamentals",
    "text": "17.1 Insurance/reinsurance fundamentals",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#parametric-vs.-indemnity-coverage",
    "href": "chapters/risk/risk-transfer.html#parametric-vs.-indemnity-coverage",
    "title": "17¬† Risk Transfer üöß",
    "section": "17.2 Parametric vs.¬†indemnity coverage",
    "text": "17.2 Parametric vs.¬†indemnity coverage",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#catastrophe-bonds-index-based-schemes",
    "href": "chapters/risk/risk-transfer.html#catastrophe-bonds-index-based-schemes",
    "title": "17¬† Risk Transfer üöß",
    "section": "17.3 Catastrophe bonds, index-based schemes",
    "text": "17.3 Catastrophe bonds, index-based schemes",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#challenges-in-emerging-markets-and-vulnerable-regions",
    "href": "chapters/risk/risk-transfer.html#challenges-in-emerging-markets-and-vulnerable-regions",
    "title": "17¬† Risk Transfer üöß",
    "section": "17.4 Challenges in emerging markets and vulnerable regions",
    "text": "17.4 Challenges in emerging markets and vulnerable regions",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#further-reading",
    "href": "chapters/risk/risk-transfer.html#further-reading",
    "title": "17¬† Risk Transfer üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Risk Transfer üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html",
    "href": "chapters/risk/deep-uncertainty.html",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics - Model Validation and Comparison",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#learning-objectives",
    "href": "chapters/risk/deep-uncertainty.html#learning-objectives",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nDistinguish between aleatory and epistemic uncertainty in climate risk assessment\nUnderstand the challenges posed by structural uncertainty and model disagreement\nApply Bayesian Model Averaging (BMA) and stacking approaches to combine multiple models\nRecognize when deep uncertainty invalidates traditional decision frameworks\nIdentify sources of deep uncertainty in exposure and impact modeling",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#introduction-from-parameter-to-structural-uncertainty",
    "href": "chapters/risk/deep-uncertainty.html#introduction-from-parameter-to-structural-uncertainty",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.1 Introduction: From Parameter to Structural Uncertainty",
    "text": "18.1 Introduction: From Parameter to Structural Uncertainty\nWhen we move from hazard assessment (Part II) to risk analysis, we encounter increasingly severe forms of uncertainty. While climate hazard models have known physical constraints and observational data for validation, the mapping from hazards to societal impacts involves:\n\nSocioeconomic projections with high uncertainty\nComplex system interactions and cascading effects\nHuman behavioral responses that are difficult to predict\nInfrastructure and institutional changes over time\n\nThis chapter addresses how to handle situations where our uncertainty about model structure itself dominates other sources of uncertainty.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#the-spectrum-of-uncertainty",
    "href": "chapters/risk/deep-uncertainty.html#the-spectrum-of-uncertainty",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.2 The Spectrum of Uncertainty",
    "text": "18.2 The Spectrum of Uncertainty\nUnderstanding uncertainty requires recognizing that not all uncertainties are alike. We can conceptualize a spectrum from relatively certain knowledge to situations of deep uncertainty:\nDeterministic representations: Decision variables without any uncertainty - Example: Cost-benefit analysis with a single time series of future water demand - Example: A single deterministic depth-damage function\n‚ÄúObjective‚Äù probabilities: Uncertainties represented using well-established probabilistic models with widely agreed-upon parameters - Example: Distribution of storm surge (with very long records) - Example: Probabilistic depth-damage curve calibrated on many similar structures\n‚ÄúSubjective‚Äù probabilities: Uncertainties represented using probabilistic models informed by expert judgment - Note: Not fundamentally different from ‚Äúobjective‚Äù probabilities‚Äîassumptions are always required - Example: Future price of carbon - Example: Probability distribution for future sea-level rise\nDeep uncertainty: Situations where probabilities cannot be reliably quantified, and even the range of possible outcomes may be unknown - Example: Long-term impact of artificial intelligence on society and the economy - Example: Many samples of future sea-level rise - No one disputes that climate change research involves deep uncertainties‚Äîthe question is what to do about it\n\n18.2.1 What Tools Do Different Approaches Enable?\nTraditional decision-making tools like cost-benefit analysis and optimization require probabilities to quantify the likelihood of different outcomes and evaluate expected values of different strategies. Risk-based and risk-averse approaches can still be used (e.g., examining 95th percentile of damages, weighting bad outcomes more heavily).\nFor deep uncertainty, alternative approaches include: - Exploratory modeling: No assumptions of likelihood made - Robustness approaches: Aggregate metrics across scenarios, though this is implicitly similar to assuming a probability distribution",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#types-of-uncertainty",
    "href": "chapters/risk/deep-uncertainty.html#types-of-uncertainty",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.3 Types of Uncertainty",
    "text": "18.3 Types of Uncertainty\n\n18.3.1 Aleatory vs Epistemic Uncertainty\n\nAleatory uncertainty: Irreducible randomness inherent in natural processes\nEpistemic uncertainty: Reducible uncertainty due to incomplete knowledge\n\n\n\n18.3.2 Parametric vs Structural Uncertainty\n\nParametric uncertainty: Uncertainty in model parameters given fixed model structure\nStructural uncertainty: Uncertainty about which model structure is ‚Äúcorrect‚Äù",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#model-disagreement-and-ensemble-methods",
    "href": "chapters/risk/deep-uncertainty.html#model-disagreement-and-ensemble-methods",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.4 Model Disagreement and Ensemble Methods",
    "text": "18.4 Model Disagreement and Ensemble Methods\n\n18.4.1 Climate Model Ensembles\nWhile climate models agree on basic warming trends, they disagree substantially on: - Regional precipitation changes - Extreme event frequency and intensity - Timing and magnitude of tipping points\n\n\n18.4.2 Multi-Model Approaches\n\n18.4.2.1 Simple Model Averaging\n\n\n18.4.2.2 Bayesian Model Averaging (BMA)\nBayesian Model Averaging provides a principled approach to combining predictions from multiple models:\n\\[\np(y | \\mathbf{x}, \\mathcal{D}) = \\sum_{k=1}^K p(y | \\mathbf{x}, M_k, \\mathcal{D}) p(M_k | \\mathcal{D})\n\\]\nwhere \\(M_k\\) represents model \\(k\\), and \\(p(M_k | \\mathcal{D})\\) is the posterior model probability.\nAdvantages: - Principled uncertainty quantification - Automatic weighting based on model performance - Coherent probabilistic framework\nChallenges: - Requires proper scoring rules for model comparison - Sensitive to model set selection - May struggle with model dependence\n\n\n18.4.2.3 Model Stacking\nStacking optimizes model weights to minimize predictive error:\n\\[\n\\hat{w} = \\arg\\min_w \\sum_{i=1}^n \\left( y_i - \\sum_{k=1}^K w_k \\hat{y}_{i,k} \\right)^2\n\\]\nsubject to \\(\\sum_k w_k = 1\\) and \\(w_k \\geq 0\\).\nAdvantages: - Focused on predictive performance - Less sensitive to model specification - Computationally efficient\nChallenges: - No natural uncertainty quantification - Weights may not reflect model reliability",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#scenario-analysis",
    "href": "chapters/risk/deep-uncertainty.html#scenario-analysis",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.5 Scenario Analysis",
    "text": "18.5 Scenario Analysis\nLet‚Äôs tighten our notation just a little bit.\n\nState of the world: encapsulates all inputs to our model.\nDecisions: can be very simple (how high do we elevate a house right now?) or very complex (spatial and/or temporal optimization problems)\nOutcomes: can be a single number (scalar) or a vector if there are multiple outcomes we care about.\n\n\n18.5.1 Cost-benefit analysis\n\nDo this for a single state of the world\nEmphasis on discounted cash flow to define ‚Äúoutcomes‚Äù\n\n\n\n18.5.2 Uncertainty\nUncertainty commonly divided into two broad classes:\n\nAleatory uncertainty, or uncertainties resulting from randomness;\nEpistemic uncertainty, or uncertainties resulting from lack of knowledge.\n\nWe can also categorize uncertainty based on the source of the uncertainty:\n\nParameter uncertainty\nModel structure uncertainty\nExternal / boundary condition / scenario uncertainty\n\nUncertainty is represented in models in many different ways. For example:\n\nDeterministic: hold some things fixed (we always do this!)\nProbabilistic with expectations (lab)\nProbabilistic with sampling\n\nThese are all assumptions, and these assumptions can be varied.\n\n\n18.5.3 Scenario analysis\nScenario analysis is a broad class of methods that are used in many fields. They fall into two very broad categories:\n\nStress-testing: pick a few scenarios and make sure the system performs acceptably\n\nFinancial stress testing\nDesign storms for flooding\n\nTraditional scenario analysis: explore several scenarios and see how the system performs\n\nOptimizing energy systems\nClimate science (where RCP scenarios are a key input)\nOften a focus on scenarios that are difficult to put probabilities on\n\n\n\n\n\n\n\n\nKey point\n\n\n\nScenario analysis is a way to explore the consequences of different assumptions. However, decisions that perform well on some scenarios may not perform well on others. Scenario analysis does not attempt to quantify the likelihood of different scenarios. Recall that estimating expectations requires a probability distribution.\n\n\n\n\n18.5.4 Examples\nLet‚Äôs return to our house elevation problem. What are some things we could consider?\n\nSea-level rise: could consider a few different scenarios of sea-level rise\nDiscount rate\n\n\n\n18.5.5 Experiment design\n\nNumber of scenarios\n\nA few = interpretable\nMany = more systematic analysis. Interactions between different sources of uncertainty are important.\n\nWhat is a scenario?\n\nParameter values\nModel structure\n\nHow to generate / sample scenarios",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#sources-of-deep-uncertainty-in-climate-risk",
    "href": "chapters/risk/deep-uncertainty.html#sources-of-deep-uncertainty-in-climate-risk",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.6 Sources of Deep Uncertainty in Climate Risk",
    "text": "18.6 Sources of Deep Uncertainty in Climate Risk\n\n18.6.1 Exposure Projections\n\nPopulation growth and urbanization patterns\nEconomic development trajectories\n\nLand use and infrastructure changes\nAdaptation and protection investments\n\n\n\n18.6.2 Vulnerability Models\n\nSocial and economic vulnerability evolution\nInfrastructure aging and upgrades\nInstitutional capacity changes\nCultural and behavioral adaptations\n\n\n\n18.6.3 Impact Functions\n\nDose-response relationships for climate hazards\nThreshold effects and system tipping points\nCascading failure mechanisms\nRecovery and adaptation dynamics",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#when-models-fundamentally-disagree",
    "href": "chapters/risk/deep-uncertainty.html#when-models-fundamentally-disagree",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.7 When Models Fundamentally Disagree",
    "text": "18.7 When Models Fundamentally Disagree\n\n18.7.1 Scenario Discovery\nIdentifying conditions under which models yield conflicting predictions.\n\n\n18.7.2 Model Structural Deficiencies\nSometimes model disagreement reflects fundamental limitations: - Missing processes or feedbacks - Inappropriate spatial/temporal scales - Inadequate representation of extremes - Oversimplified human dimensions\n\n\n18.7.3 Deep Uncertainty Diagnostics\nHow to recognize when uncertainty is ‚Äúdeep‚Äù: - Model predictions span decision-relevant thresholds - Different models imply different optimal decisions - No clear basis for discriminating among models - Disagreement persists despite additional data",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#philosophical-approaches-to-deep-uncertainty",
    "href": "chapters/risk/deep-uncertainty.html#philosophical-approaches-to-deep-uncertainty",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.8 Philosophical Approaches to Deep Uncertainty",
    "text": "18.8 Philosophical Approaches to Deep Uncertainty\n\n18.8.1 The Lempert-Schneider Debate\nA fundamental tension in climate policy involves whether and how to assign probabilities to deeply uncertain scenarios. This debate is well illustrated by contrasting views from Robert Lempert and Stephen Schneider regarding IPCC scenarios.\n\n18.8.1.1 The Case Against Probabilities (Lempert‚Äôs Position)\nTraditional prediction-based approaches are inadequate: The conventional framework for climate policy relies on predicting the future and identifying optimal policies. However, such predictions are inherently unreliable for complex, long-term issues like climate change.\nOptimal policies based on point estimates are brittle: Policies optimized for specific future scenarios fail when different futures unfold. Moreover, they fail to generate consensus among stakeholders with divergent expectations about the future.\nRobust strategies are preferable to optimal ones: Rather than seeking optimal policies, we should identify robust strategies that perform reasonably well across a wide range of plausible futures. Robust strategies are less sensitive to uncertainty and more likely to garner stakeholder support.\nExploratory modeling enables robust decision-making: Computer simulations can create large ensembles of plausible scenarios based on different assumptions. Analysts can systematically explore strategy performance across scenarios, identifying those that work well under various conditions.\n\n\n18.8.1.2 The Case for Probabilities (Schneider‚Äôs Position)\nRisk requires both probability and consequence: The concept of risk inherently combines the probability of events with their consequences. Without probabilities, decision-makers are left with only consequences, potentially leading to misplaced priorities.\nIPCC‚Äôs failure to assign probabilities creates confusion: Presenting scenarios without indicating relative likelihood allows cherry-picking of results to support preferred narratives. Low-probability, high-consequence outcomes may receive undue emphasis, distorting public debate.\nExpert judgment provides valuable information: While subjective and uncertain, probabilistic assessments can provide valuable guidance when done transparently. These should be attempted for each step of the climate modeling chain‚Äîemissions scenarios, carbon cycle modeling, climate sensitivity, and impacts.\nProbability assumptions matter enormously: Different assumptions about scenario likelihoods can lead to vastly different risk assessments. For example, the probability of exceeding ‚Äúdangerous‚Äù warming thresholds can range from 25% to nearly 40% depending on probability assignments.\n\n\n18.8.1.3 Synthesis\nBoth perspectives offer important insights: - Lempert highlights the dangers of false precision in probability estimates - Schneider emphasizes the need for risk-based thinking in policy contexts\nIn practice, many successful approaches combine elements of both perspectives, using scenario analysis to explore robustness while acknowledging that some scenarios are more plausible than others.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#implications-for-decision-making",
    "href": "chapters/risk/deep-uncertainty.html#implications-for-decision-making",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.9 Implications for Decision Making",
    "text": "18.9 Implications for Decision Making\nDeep uncertainty fundamentally challenges traditional decision frameworks:\n\nExpected utility theory: Requires well-defined probabilities\nCost-benefit analysis: Sensitive to probability assignments\nOptimization approaches: May yield misleading ‚Äúoptimal‚Äù solutions\n\nThis motivates the robust decision-making approaches covered in the next chapter.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#case-studies",
    "href": "chapters/risk/deep-uncertainty.html#case-studies",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "18.10 Case Studies",
    "text": "18.10 Case Studies\n\n18.10.1 Sea Level Rise Projections\nDifferent physical models (ice sheet dynamics, thermal expansion, glacial isostatic adjustment) yield vastly different local sea level projections.\n\n\n18.10.2 Hurricane Intensification Under Climate Change\nModels disagree on both the sign and magnitude of tropical cyclone frequency and intensity changes.\n\n\n18.10.3 Ecosystem Service Valuation\nEconomic models show orders-of-magnitude differences in ecosystem service values under climate change.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#further-reading",
    "href": "chapters/risk/deep-uncertainty.html#further-reading",
    "title": "18¬† Deep Uncertainty and Model Structure üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Deep Uncertainty and Model Structure üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html",
    "href": "chapters/risk/robustness.html",
    "title": "19¬† Robustness üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Deep Uncertainty and Model Structure - Optimization",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#learning-objectives",
    "href": "chapters/risk/robustness.html#learning-objectives",
    "title": "19¬† Robustness üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nAddress deep uncertainty in climate decision-making.\nUse robust decision-making (RDM) methods to compare strategies across multiple futures.\nBalance robustness vs.¬†optimality in selecting climate actions.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#robustness-motivation",
    "href": "chapters/risk/robustness.html#robustness-motivation",
    "title": "19¬† Robustness üöß",
    "section": "19.1 Robustness motivation",
    "text": "19.1 Robustness motivation\n\n\n\n\n\n19.1.1 The problem\nStudies show that water utilities systematically over-estimate future demand. Using a single, certain, forecast of future water demand might motivate over-building infrastructure.\n\n\n\n\n\n\n\n\n19.1.2 Robustness\nWe want to make choices / design infrastructure that are robust to errors in demand forecasts.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#definition",
    "href": "chapters/risk/robustness.html#definition",
    "title": "19¬† Robustness üöß",
    "section": "19.2 Definition",
    "text": "19.2 Definition\n\nThe insensitivity of system design to errors, random or otherwise, in the estimates of those parameters affecting design choice (Matalas and Fiering 1977)\n\nMathematical definitions differ dramatically, however (Herman et al. 2015)! Today we will discuss some overlapping perspectives and ideas about robustness.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#bottom-up-analysis",
    "href": "chapters/risk/robustness.html#bottom-up-analysis",
    "title": "19¬† Robustness üöß",
    "section": "19.3 Bottom-up analysis",
    "text": "19.3 Bottom-up analysis\n\nTop-down, certain: experts develop a ‚Äúbest‚Äù forecast of future conditions, then choose a design that is optimal under that forecast\nTop-down, uncertain: experts assign likelihoods to uncertain states of the world, then choose a design that optimizes expected performance (Herman et al. 2015)\nBottom-up: first explore to identify SOWs a solution is vulnerable to, then assess likelihood (more Wednesday!)",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#deep-uncertainty-and-scenario-exploration",
    "href": "chapters/risk/robustness.html#deep-uncertainty-and-scenario-exploration",
    "title": "19¬† Robustness üöß",
    "section": "19.4 Deep uncertainty and scenario exploration",
    "text": "19.4 Deep uncertainty and scenario exploration",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#robustness-metrics",
    "href": "chapters/risk/robustness.html#robustness-metrics",
    "title": "19¬† Robustness üöß",
    "section": "19.5 Robustness metrics",
    "text": "19.5 Robustness metrics\n\n19.5.1 Taxonomy\n\n\n\nHerman et al. (2015)\n\n\n\n\n19.5.2 Regret\nRegret measures how sorry you are with your choice. There are two main definitions (Herman et al. 2015):\n\nDeviation of a single solution in the real world or a simulated SOW from its baseline (expected) performance\nDifference between the performance of a solution in the real world or a simulated SOW and the best possible performance in that SOW\n\n\n\n19.5.3 Satisficing\nSatisficing measures whether solutions achieve specific minimum requirements, condensing performance into a binary ‚Äúsatisfactory‚Äù or ‚Äúunsatisfactory‚Äù.\n\nWith many SOWs, many studies use a domain criterion: over what fraction of SOWs does a solution satisfy a performance threshold (Herman et al. 2015)?\nNote: this is equivalent to asking what is the probability that a solution satisfies a performance threshold, although many people who calculate robustness metrics are allergic to the word ‚Äúprobability‚Äù\nMore complex satisficing criteria: see McPhail et al. (2019).",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#robust-decision-making-rdm-info-gap-theory",
    "href": "chapters/risk/robustness.html#robust-decision-making-rdm-info-gap-theory",
    "title": "19¬† Robustness üöß",
    "section": "19.6 Robust decision-making (RDM), info-gap theory",
    "text": "19.6 Robust decision-making (RDM), info-gap theory",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#critiques-and-alternative-perspectives",
    "href": "chapters/risk/robustness.html#critiques-and-alternative-perspectives",
    "title": "19¬† Robustness üöß",
    "section": "19.7 Critiques and alternative perspectives",
    "text": "19.7 Critiques and alternative perspectives\n\n19.7.1 Parameters?\nThe robustness metrics we‚Äôve seen are defined in terms of parameters: we have a model with parameters, and we define SOWs as different values of those parameters.\nIs this a good way to quantify our conceptual ideas about robustness?\n\n\n19.7.2 Combinations of uncertainties\nIn practice, we often have a combination of parametric uncertainties and ‚Äúmodel structure‚Äù uncertainties (Doss-Gollin and Keller 2023). And not all SOWs are equally likely!\n\n\n\n\n\nClimate scenario uncertainties are ‚Äúdeep‚Äù (more next week!), but it would be a mistake to say we don‚Äôt know anything and all futures are equally likely (Hausfather and Peters 2020)\n\n\n\n\n\n\n\nDoss-Gollin and Keller (2023)\n\n\n\n\n19.7.3 Alternative perspective\n\nUsing ‚Äúprior beliefs‚Äù assign likelihoods to different SOWs\nUse quantitative toolkit (optimization, sensitivity analysis, etc.)\nVary prior belefs: a solution that is robust to different probability distributions rather than to different parameter values.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#stress-testing-policies-under-various-climate-economic-and-societal-conditions",
    "href": "chapters/risk/robustness.html#stress-testing-policies-under-various-climate-economic-and-societal-conditions",
    "title": "19¬† Robustness üöß",
    "section": "19.8 Stress testing policies under various climate, economic, and societal conditions",
    "text": "19.8 Stress testing policies under various climate, economic, and societal conditions",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#trade-offs-between-robust-vs.-best-estimate-solutions",
    "href": "chapters/risk/robustness.html#trade-offs-between-robust-vs.-best-estimate-solutions",
    "title": "19¬† Robustness üöß",
    "section": "19.9 Trade-offs between robust vs.¬†‚Äúbest-estimate‚Äù solutions",
    "text": "19.9 Trade-offs between robust vs.¬†‚Äúbest-estimate‚Äù solutions",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/robustness.html#further-reading",
    "href": "chapters/risk/robustness.html#further-reading",
    "title": "19¬† Robustness üöß",
    "section": "Further reading",
    "text": "Further reading\n\n\n\n\nDoss-Gollin, James, and Klaus Keller. 2023. ‚ÄúA Subjective Bayesian Framework for Synthesizing Deep Uncertainties in Climate Risk Management.‚Äù Earth‚Äôs Future 11 (1). https://doi.org/10.1029/2022EF003044.\n\n\nHausfather, Zeke, and Glen P. Peters. 2020. ‚ÄúEmissions ‚Äì the ‚ÄôBusiness as Usual‚Äô Story Is Misleading.‚Äù Nature 577 (7792, 7792): 618‚Äì20. https://doi.org/10.1038/d41586-020-00177-3.\n\n\nHerman, Jonathan D., Patrick M. Reed, Harrison B. Zeff, and Gregory W. Characklis. 2015. ‚ÄúHow Should Robustness Be Defined for Water Systems Planning Under Change?‚Äù Journal of Water Resources Planning and Management 141 (10): 04015012. https://doi.org/10.1061/(asce)wr.1943-5452.0000509.\n\n\nMatalas, Nicholas C, and Myron B Fiering. 1977. ‚Äú6. Water-Resource Systems Planning.‚Äù In Climate, Climatic Change, and Water Supply, 99‚Äì110. Washington, DC: The National Academies Press. https://www.nap.edu/read/185/chapter/11.\n\n\nMcPhail, C., H. R. Maier, J. H. Kwakkel, M. Giuliani, A. Castelletti, and S. Westra. 2019. ‚ÄúRobustness Metrics: How Are They Calculated, When Should They Be Used and Why Do They Give Different Results?‚Äù Earth‚Äôs Future, April, 169‚Äì91. https://doi.org/10.1002/2017ef000649.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Robustness üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html",
    "href": "chapters/risk/adaptive.html",
    "title": "20¬† Adaptive Planning and Flexibility üöß",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Deep Uncertainty - Robustness",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Adaptive Planning and Flexibility üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#learning-objectives",
    "href": "chapters/risk/adaptive.html#learning-objectives",
    "title": "20¬† Adaptive Planning and Flexibility üöß",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nPlan for uncertainty with adaptive management and iterative risk strategies.\nDevelop adaptation pathways that evolve with new information (e.g., climate data, impacts).\nIncorporate monitoring and feedback loops into long-term climate policy.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Adaptive Planning and Flexibility üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#adaptive-management-frameworks",
    "href": "chapters/risk/adaptive.html#adaptive-management-frameworks",
    "title": "20¬† Adaptive Planning and Flexibility üöß",
    "section": "20.1 Adaptive management frameworks",
    "text": "20.1 Adaptive management frameworks\n\n20.1.1 Real options and flexibility\nThe financial theory of real options defines ‚Äúthe right, but not obligation‚Äù to take a particular action in the future. For example, purchasing the right to buy stocks at a fixed price gives flexibility to exercise the option only if favorable conditions emerge.\nThis concept has motivated applications in engineering and policy design. Creating flexibility in decision making can be very valuable, though generating naturally flexible designs isn‚Äôt always straightforward.\nA classic example in infrastructure: when building a parking garage, should we: - Build to a few floors immediately? - Build to many floors immediately? - Build a few floors but with extra-strong foundations so we can add levels later if demand increases?\nThe third option provides valuable flexibility‚Äîit creates the option to expand without the full upfront cost. In climate adaptation, similar principles apply: - Design infrastructure that can be upgraded rather than replaced - Implement monitoring systems that can trigger additional interventions - Create institutional frameworks that can accommodate evolving policies\n\n\n20.1.2 Policy parameterization\nIn climate adaptation contexts, there are many ways to parameterize adaptive policies. Common approaches include:\nThreshold-based rules: Define triggers (e.g., sea level rise exceeding X cm, temperature increases beyond Y¬∞C) that activate specific interventions.\nBuffer-based approaches: Maintain safety margins that shrink over time, triggering adaptation when margins become inadequate.\nPathway-based strategies: Pre-define sequences of interventions, with decision points based on observed conditions and new information.\nMore complex adaptive policies can use machine learning approaches, though these require careful consideration of interpretability and stakeholder acceptance.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Adaptive Planning and Flexibility üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#dynamic-decision-making-under-uncertainty",
    "href": "chapters/risk/adaptive.html#dynamic-decision-making-under-uncertainty",
    "title": "20¬† Adaptive Planning and Flexibility üöß",
    "section": "20.2 Dynamic decision-making under uncertainty",
    "text": "20.2 Dynamic decision-making under uncertainty\nDynamic planning problems identify policies to select actions in response to new information over time. Policy design involves choosing the sequence, timing, and/or threshold of actions to achieve a desired outcome. This typically involves a combination of optimal control and adaptive design.\nUnlike static decision problems where all decisions are made upfront, sequential decisions allow us to wait and incorporate new information. For example, rather than deciding today how high to elevate a house, we could wait to see how fast local sea-levels are rising, then make a decision later with more information.\n\n20.2.1 Sequential decision framework\nIn sequential decision problems, the decision maker does not need to make all decisions at once. Instead, at each time step, the decision maker makes a decision based on the state of the system (which may not be fully observable). In climate risk management, the state might include current exposure levels, recent climate observations, and evolving socio-economic conditions.\nMathematically, the state evolves over time according to a dynamics model: \\[\n\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, a_t, e_{t+1}),\n\\] where\n\n\\(\\mathbf{x}_t\\) is the state at time \\(t\\)\n\\(a_t\\) is the decision at time \\(t\\) (e.g., whether to implement an adaptation measure)\n\\(e_{t+1}\\) is external forcing (e.g., climate change, socio-economic development)\n\n\\(f_t\\) describes how the system evolves in response to actions and external factors\n\n\n\n20.2.2 Policy and value\nThe decision maker‚Äôs strategy for choosing actions is called a policy. The policy is a function that maps states to actions, and can be either deterministic or stochastic.\nA central idea is to maximize the expected sum of future rewards (or minimize expected costs). Actions that give low rewards now might lead to high rewards in the future. For example, spending money on early adaptation might reduce future climate damages.\nThe value of a state is the expected sum of future rewards that can be obtained from that state, assuming the decision maker follows a particular policy.\n\n\n20.2.3 Solution approaches\nOpen loop control solves for all actions at once, producing a predetermined schedule of interventions. The advantage is simplicity of execution. The disadvantage is inflexibility‚Äîit doesn‚Äôt adapt to new information.\nDynamic programming uses the recursive Bellman equation to find optimal policies: \\[\nQ_t(\\mathbf{x}_t) = \\min_{a_t} \\left\\{ R_t + \\gamma Q_{t+1} (\\mathbf{x}_{t+1}) \\right\\}\n\\] where \\(\\gamma\\) is the discount factor and \\(Q_t\\) is the value function.\nThis approach can provide exact solutions but requires discretizing the problem and suffers from the ‚Äúcurse of dimensionality‚Äù as the number of states and actions grows.\nPolicy search assumes a specific functional form for the policy with parameters to optimize. Common approaches include linear decision rules, decision trees, and neural networks. This approach is flexible and works well with simulation-optimization frameworks, but can be computationally expensive.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Adaptive Planning and Flexibility üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#tipping-points-and-trigger-based-adaptation",
    "href": "chapters/risk/adaptive.html#tipping-points-and-trigger-based-adaptation",
    "title": "20¬† Adaptive Planning and Flexibility üöß",
    "section": "20.3 ‚ÄúTipping points‚Äù and trigger-based adaptation",
    "text": "20.3 ‚ÄúTipping points‚Äù and trigger-based adaptation",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Adaptive Planning and Flexibility üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#further-reading",
    "href": "chapters/risk/adaptive.html#further-reading",
    "title": "20¬† Adaptive Planning and Flexibility üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Adaptive Planning and Flexibility üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html",
    "href": "chapters/risk/social-science.html",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "",
    "text": "21.1 Value Judgements in Climate Risk Analysis\nAll climate risk assessments embed value judgements, whether explicit or implicit:\nMaking these judgements explicit and subjecting them to democratic deliberation is essential for legitimate decision-making.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#value-judgements-in-climate-risk-analysis",
    "href": "chapters/risk/social-science.html#value-judgements-in-climate-risk-analysis",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "",
    "text": "What time horizons matter?\nHow should we weigh costs and benefits across different groups?\nWhat discount rates reflect appropriate intergenerational concern?\nHow much should we pay to avoid low-probability, high-impact risks?\n\n\n\n21.1.1 Ethical Frameworks for Decision Analysis\n\n21.1.1.1 Utilitarianism\n\nMaximize aggregate welfare across all affected parties\nStandard cost-benefit analysis embeds utilitarian assumptions\nChallenges: measurement of welfare, distribution blindness\n\n\n\n21.1.1.2 Rawlsian Justice\n\nFocus on impacts on worst-off groups (‚Äúmaximin‚Äù criterion)\nRelevant for climate justice and adaptation prioritization\nChallenges: identifying worst-off, potential inefficiencies\n\n\n\n21.1.1.3 Rights-Based Approaches\n\nCertain fundamental rights should not be violated regardless of aggregate benefits\nRelevant for displacement, cultural preservation, procedural justice\nChallenges: defining rights, resolving conflicts between rights\n\n\n\n21.1.1.4 Capabilities Approach\n\nFocus on enabling human capabilities and functioning\nEmphasizes what people can do and be, not just what they have\nRelevant for adaptation and development co-benefits",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#climate-justice-and-distributional-equity",
    "href": "chapters/risk/social-science.html#climate-justice-and-distributional-equity",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "21.2 Climate Justice and Distributional Equity",
    "text": "21.2 Climate Justice and Distributional Equity\n\n21.2.1 Dimensions of Climate Justice\n\n21.2.1.1 Distributive Justice\n\nWho bears the costs and receives the benefits of climate policies?\nHow are climate risks distributed across space, time, and social groups?\nWhat constitutes fair allocation of adaptation resources?\n\n\n\n21.2.1.2 Procedural Justice\n\nWho participates in decision-making processes?\nAre affected communities meaningfully involved?\nHow are traditional and local knowledge systems included?\n\n\n\n21.2.1.3 Recognition Justice\n\nWhose knowledge and values are recognized as legitimate?\nHow are different ways of life and cultural practices acknowledged?\nWhat constitutes appropriate representation of diverse stakeholders?\n\n\n\n\n21.2.2 Intergenerational Ethics\nClimate change raises fundamental questions about obligations to future generations:\n\nDiscount rates: How much should we weigh future costs and benefits?\nIntergenerational equity: What do we owe future generations?\nIrreversibility: How should irreversible changes affect our decisions?\nUncertainty: How do we make decisions on behalf of people whose preferences we cannot know?\n\n\n\n21.2.3 Environmental Justice Frameworks\nEnvironmental justice provides a critical lens for understanding climate risks and policy impacts. As defined by Bullard (1999):\n\nEnvironmental racism refers to any environmental policy, practice or directive that differentially affects or disadvantages (whether intended or unintended) individuals, groups or communities based on race or colour.\n\nThis definition importantly focuses on effects rather than intent, recognizing that policies can create disparate impacts regardless of intention.\n\n21.2.3.1 Five Dimensions of Justice\nFollowing Wutich et al.¬†(2023), environmental justice encompasses multiple dimensions:\n\nDistributive Justice: Fair and equitable access to resources and outcomes across social groups\n\nExample: No disparities in flood protection measures between low-income and high-income neighborhoods\n\nInterpersonal Justice: Fair and equitable treatment of individuals regardless of identity\n\nExample: Equal consideration for post-flood assistance regardless of income level\n\nProcedural Justice: Fair and equitable rules, norms, and decision-making processes\n\nExample: Equal opportunities for community participation in flood risk management decisions\n\nRecognition Justice: Fair and equitable representation of different worldviews and values\n\nExample: Including Indigenous knowledge in flood risk management strategies\n\nTransformative (Restorative) Justice: Collaborative addressing of root causes of systemic oppression\n\nExample: Targeted investments to address historical inequities in flood risk exposure\n\n\n\n\n21.2.3.2 Quantitative Frameworks for Equity Assessment\nDifferent moral frameworks provide different approaches to quantifying equity (Jafino et al., 2022):\nUtilitarian: Maximize aggregate welfare - Maximize: \\(\\sum_{i=1}^n u(x_i)\\)\nStrict Egalitarian: Minimize inequality of outcomes - Minimize: \\(\\max(u(x_i)) - \\min(u(x_i))\\)\nRawlsian Difference Principle: Benefit the least advantaged - Maximize: \\(\\min(u(x_i))\\)\nPrioritarian: Weight welfare of worse-off individuals more heavily - Maximize: \\(\\frac{1}{1-\\gamma} \\sum_{i=1}^n u(\\frac{u(x_i)}{u(\\bar{x})})^{1-\\gamma}\\) where \\(\\gamma\\) is the inequality aversion factor\nSufficientarian: Ensure all individuals meet minimum welfare thresholds - Maximize: \\(\\{i \\in n : u(x_i) \\geq u(s)\\}\\) where \\(u(s)\\) is the minimum sufficient welfare level\nNote that quantifying welfare functions is notoriously challenging, involving fundamental questions about value comparison and measurement.\n\n\n\n21.2.4 Practical Considerations for Equity Assessment\n\n21.2.4.1 Equity in What?\nEnvironmental justice analysis should examine both burdens and benefits: - Risk: Distribution of potential consequences across groups - Funding: Allocation of resources for risk management and recovery - Recovery: Different groups‚Äô ability to recover from climate impacts - Benefits: Distribution of positive outcomes like reduced risk or improved quality of life\n\n\n21.2.4.2 Equity at What Scale?\nThe choice of spatial aggregation significantly impacts equity assessments: - Individual level: Household or building-level outcomes - Neighborhood level: Community-defined boundaries important for environmental justice - Small areas: Administrative boundaries like census tracts - Large areas: Cities or regions\nDifferent scales can reveal or obscure disparities, so the choice should reflect where inequities are produced and experienced.\n\n\n21.2.4.3 Equity with Respect to What?\nEquity can be assessed across multiple dimensions: - Demographics: Income, race, ethnicity, age, or composite vulnerability indices - Environmental burden: Exposure to pollution, access to environmental amenities - Procedural factors: Historical discrimination, differential policy treatment - Causal factors: Root causes that produce inequities rather than just describing disparities\n\n\n\n21.2.5 Distributional Analysis\nTools for analyzing how climate risks and policies affect different groups:\n\nIncidence analysis: Who bears costs and receives benefits?\nVulnerability mapping: Which groups face highest risks?\nEquity weighting: How to adjust welfare measures for distributional concerns?\nEnvironmental justice screening: Identifying cumulative impacts on disadvantaged communities",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#participatory-methods-and-stakeholder-engagement",
    "href": "chapters/risk/social-science.html#participatory-methods-and-stakeholder-engagement",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "21.3 Participatory Methods and Stakeholder Engagement",
    "text": "21.3 Participatory Methods and Stakeholder Engagement\n\n21.3.1 Why Participation Matters\n\nLegitimacy: Democratic participation is intrinsically valuable\nEffectiveness: Local knowledge improves decision quality\nEquity: Ensures voices of affected communities are heard\nCapacity: Builds local capacity for ongoing adaptation\n\n\n\n21.3.2 Spectrum of Participation\n\n21.3.2.1 Information Provision\n\nOne-way communication of technical information\nPublic meetings, websites, reports\nLimited role for public input\n\n\n\n21.3.2.2 Consultation\n\nSeeking public input on predetermined options\nSurveys, focus groups, public hearings\nInput may or may not influence decisions\n\n\n\n21.3.2.3 Involvement\n\nWorking with stakeholders to understand issues and develop alternatives\nWorkshops, citizen panels, deliberative polls\nCommitment to consider public input seriously\n\n\n\n21.3.2.4 Collaboration\n\nPartnering with stakeholders in decision-making\nCo-design processes, collaborative modeling\nShared responsibility for outcomes\n\n\n\n21.3.2.5 Empowerment\n\nFinal decision-making authority placed with stakeholders\nCommunity-based adaptation, participatory budgeting\nTransfer of power to affected communities\n\n\n\n\n21.3.3 Methods for Stakeholder Engagement\n\n21.3.3.1 Deliberative Processes\n\nCitizens‚Äô juries: Small groups deliberate on specific questions\nConsensus conferences: Structured dialogue between experts and citizens\nDeliberative polling: Combines representative sampling with informed deliberation\n\n\n\n21.3.3.2 Collaborative Modeling\n\nGroup model building: Stakeholders participate in creating models\nParticipatory scenario development: Co-creating future scenarios\nShared vision planning: Collaborative visioning and goal-setting\n\n\n\n21.3.3.3 Community-Based Approaches\n\nParticipatory rural appraisal: Community-led data collection and analysis\nParticipatory mapping: Communities map their own resources and risks\nCommunity-based monitoring: Local monitoring of environmental conditions\n\n\n\n\n21.3.4 Challenges in Participation\n\nPower imbalances: Unequal resources and capacities among participants\nTechnical complexity: Difficulty communicating complex climate science\nScale mismatches: Local participation in regional/global problems\nTokenism: Superficial consultation without real influence\nParticipation fatigue: Over-consultation without visible results\n\n\n21.3.4.1 Addressing Power Dynamics\nFollowing Fletcher et al.¬†(2022), effective participatory processes must explicitly address power imbalances:\nBeyond inclusion to equitable participation: Simply including marginalized communities is insufficient; the quality and equity of participation matters more than just presence.\nPower shapes knowledge prioritization: Power imbalances determine whose knowledge is considered legitimate and valuable, and who ultimately benefits from participatory processes.\nExplicit power analysis is essential: Successful participatory processes must identify existing power dynamics and create mechanisms for participants to share power more equitably.\nSkilled facilitation matters: Professional facilitation and collaboration with boundary organizations can help address power imbalances and create more equitable participation.\nSustained engagement required: Building trust and shifting power dynamics requires long-term commitment rather than one-off consultation events.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#communicating-climate-risk-and-uncertainty",
    "href": "chapters/risk/social-science.html#communicating-climate-risk-and-uncertainty",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "21.4 Communicating Climate Risk and Uncertainty",
    "text": "21.4 Communicating Climate Risk and Uncertainty\n\n21.4.1 Challenges in Risk Communication\n\nUncertainty communication: How to convey deep uncertainty without paralysis\nProbability translation: Making probabilistic information accessible\nScale comprehension: Helping people understand long time horizons and large spatial scales\nBehavioral biases: Accounting for cognitive limitations and heuristics\n\n\n\n21.4.2 Effective Communication Strategies\n\n21.4.2.1 Visualization Approaches\n\nScenario storytelling: Narrative descriptions of plausible futures\nRisk ladders: Comparing climate risks to familiar risks\nInteractive visualizations: Allowing exploration of uncertainty ranges\nLocal contextualization: Translating global changes to local impacts\n\n\n\n21.4.2.2 Framing and Messaging\n\nSolution-focused framing: Emphasizing response options alongside risks\nCo-benefits messaging: Highlighting multiple benefits of climate action\nTrusted messengers: Using credible local voices and institutions\nCultural adaptation: Tailoring messages to cultural contexts and values\n\n\n\n21.4.2.3 Uncertainty Communication\n\nConfidence levels: Expressing confidence in different aspects of projections\nScenario ranges: Showing plausible bounds rather than single predictions\nKnown unknowns: Being explicit about what we don‚Äôt know\nDecision relevance: Focusing on uncertainty that matters for decisions\n\n\n\n\n21.4.3 Science-Policy Interfaces\n\n21.4.3.1 Boundary Organizations\n\nOrganizations that span science and policy domains\nExamples: IPCC, national assessment programs, regional climate consortia\nFunctions: translation, legitimation, boundary work\n\n\n\n21.4.3.2 Co-production of Knowledge\n\nJoint production of knowledge by scientists and decision-makers\nEnsures research relevance and usability\nBuilds trust and shared understanding",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#case-studies-in-participatory-climate-risk-management",
    "href": "chapters/risk/social-science.html#case-studies-in-participatory-climate-risk-management",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "21.5 Case Studies in Participatory Climate Risk Management",
    "text": "21.5 Case Studies in Participatory Climate Risk Management\n\n21.5.1 Community-Based Adaptation in Bangladesh\n\n\n21.5.2 Indigenous Knowledge Integration in Arctic Planning\n\n\n21.5.3 Urban Climate Resilience Planning in European Cities\n\n\n21.5.4 Multi-Stakeholder Water Management in River Basins",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#building-capacity-for-participation",
    "href": "chapters/risk/social-science.html#building-capacity-for-participation",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "21.6 Building Capacity for Participation",
    "text": "21.6 Building Capacity for Participation\n\n21.6.1 Institutional Design\n\nCreating durable institutions for ongoing participation\nEnsuring adequate funding and staffing\nDeveloping appropriate legal and regulatory frameworks\n\n\n\n21.6.2 Skills and Training\n\nBuilding facilitation and communication skills\nTraining in participatory methods and tools\nDeveloping cultural competency and inclusive practices\n\n\n\n21.6.3 Technology and Tools\n\nDigital platforms for remote and hybrid participation\nDecision support tools accessible to non-experts\nTranslation and interpretation capabilities",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html#further-reading",
    "href": "chapters/risk/social-science.html#further-reading",
    "title": "21¬† Working with People: Values, Participation, and Communication üöß",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Working with People: Values, Participation, and Communication üöß</span>"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Overview",
    "section": "",
    "text": "This collection of computational notebooks demonstrates the methods and concepts discussed in the main text through practical applications. Each notebook is designed to be standalone and self-contained, using the Julia programming language for all computations.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "**Computational Case Studies**",
      "Overview"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References üéØ",
    "section": "",
    "text": "Abernathey, Ryan. 2024. An Introduction to\nEarth and Environmental Data Science. https://earth-env-data-science.github.io/intro.html.\n\n\nApplegate, Patrick, and Klaus Keller. 2015. Risk\nAnalysis in the Earth Sciences. Leanpub.\nhttps://leanpub.next/raes.\n\n\nBastani, Hamsa, Osbert Bastani, Alp Sungu, Haosen Ge, √ñzge Kabakcƒ±, and\nRei Mariman. 2025. ‚ÄúGenerative AI Without Guardrails\nCan Harm Learning: Evidence from High School\nMathematics.‚Äù Proceedings of the National Academy of\nSciences 122 (26): e2422633122. https://doi.org/10.1073/pnas.2422633122.\n\n\nBishop, Christopher M., and Hugh Bishop. 2024. Deep\nLearning: Foundations and\nConcepts. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-45468-4.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to\nProbability, Second Edition. 2nd Edition.\nBoca Raton: Chapman and Hall/CRC. http://probabilitybook.net.\n\n\nColes, Stuart. 2001. An Introduction to Statistical Modeling of\nExtreme Values. Springer Series in Statistics. London: Springer.\n\n\nCressie, Noel A. C., and Christopher K. Wikle. 2011. Statistics for\nSpatio-Temporal Data. Hoboken, N.J.: Wiley.\n\n\nDittes, Beatrice, Olga ≈†paƒçkov√°, Lukas Schoppa, and Daniel Straub. 2018.\n‚ÄúManaging Uncertainty in Flood Protection Planning with Climate\nProjections.‚Äù Hydrology and Earth System Sciences 22\n(4): 2511‚Äì26. https://doi.org/10.5194/hess-22-2511-2018.\n\n\nDoss-Gollin, James, and Klaus Keller. 2023. ‚ÄúA Subjective\nBayesian Framework for Synthesizing Deep Uncertainties in\nClimate Risk Management.‚Äù Earth‚Äôs Future 11 (1). https://doi.org/10.1029/2022EF003044.\n\n\nDowney, Allen B. 2021. Think Bayes. \"O‚ÄôReilly\nMedia, Inc.\". https://allendowney.github.io/ThinkBayes2/.\n\n\nEllsberg, Daniel. 1961. ‚ÄúRisk, Ambiguity, and the\nSavage Axioms.‚Äù The Quarterly Journal of\nEconomics 75 (4): 643‚Äì69. https://doi.org/10.2307/1884324.\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The\nElements of Statistical Learning. Vol. 1.\nSpringer series in statistics Springer, Berlin.\n\n\nGelman, Andrew. 2021. Regression and Other Stories. Analytical\nMethods for Social Research. Cambridge, United Kingdom ; Cambridge\nUniversity Press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 2014.\nBayesian Data Analysis. 3rd ed. Chapman &\nHall/CRC Boca Raton, FL, USA.\n\n\nGhil, M, P Yiou, S Hallegatte, B D Malamud, P Naveau, A Soloviev, P\nFriederichs, et al. 2011. ‚ÄúExtreme Events: Dynamics, Statistics\nand Prediction.‚Äù Nonlinear Processes in Geophysics 18\n(3): 295‚Äì350. https://doi.org/10/fvzxvv.\n\n\nGidaris, Ioannis, Jamie E. Padgett, Andre R. Barbosa, Suren Chen, Daniel\nCox, Bret Webb, and Amy Cerato. 2017. ‚ÄúMultiple-Hazard\nFragility and Restoration Models of Highway\nBridges for Regional Risk and Resilience\nAssessment in the United States: State-of-the-Art Review.‚Äù Journal of\nStructural Engineering 143 (3): 04016188. https://doi.org/10.1061/(ASCE)ST.1943-541X.0001672.\n\n\nHausfather, Zeke, and Glen P. Peters. 2020. ‚ÄúEmissions ‚Äì the\n‚ÄôBusiness as Usual‚Äô Story Is Misleading.‚Äù Nature 577\n(7792, 7792): 618‚Äì20. https://doi.org/10.1038/d41586-020-00177-3.\n\n\nHelsel, Dennis R., Robert M. Hirsch, Karen R. Ryberg, Stacey A.\nArchfield, and Edward J. Gilroy. 2020. Statistical Methods in Water\nResources. Techniques and Methods. U.S. Geological Survey.\nhttps://doi.org/10.3133/tm4A3.\n\n\nHerman, Jonathan D., Patrick M. Reed, Harrison B. Zeff, and Gregory W.\nCharacklis. 2015. ‚ÄúHow Should Robustness Be Defined for Water\nSystems Planning Under Change?‚Äù Journal of Water Resources\nPlanning and Management 141 (10): 04015012. https://doi.org/10.1061/(asce)wr.1943-5452.0000509.\n\n\nHerman, Jon, and Will Usher. 2017. ‚ÄúSALib:\nAn Open-Source Python Library for\nSensitivity Analysis.‚Äù Journal of Open Source\nSoftware 2 (9): 97. https://doi.org/10.21105/joss.00097.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2013. An Introduction to Statistical\nLearning. Vol. 103. Springer Texts in\nStatistics. New York, NY: Springer New York.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of\nScience. New York, NY: Cambridge University Press.\n\n\nJongman, Brenden, Philip J Ward, and Jeroen C J H Aerts. 2012.\n‚ÄúGlobal Exposure to River and Coastal Flooding: Long Term Trends\nand Changes.‚Äù Global Environmental Change 22 (4):\n823‚Äì35. https://doi.org/10.1016/j.gloenvcha.2012.07.004.\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. ‚ÄúAdam: A\nMethod for Stochastic Optimization.‚Äù January\n30, 2017. https://doi.org/10.48550/arXiv.1412.6980.\n\n\nKosmyna, Nataliya, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ,\nXian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, and Pattie\nMaes. 2025. ‚ÄúYour Brain on ChatGPT:\nAccumulation of Cognitive Debt When\nUsing an AI Assistant for Essay Writing\nTask.‚Äù June 10, 2025. https://doi.org/10.48550/arXiv.2506.08872.\n\n\nLafferty, David C., and Ryan L. Sriver. 2023. ‚ÄúDownscaling and\nBias-Correction Contribute Considerable Uncertainty to Local Climate\nProjections in CMIP6.‚Äù Npj Climate and\nAtmospheric Science 6 (1, 1): 1‚Äì13. https://doi.org/10.1038/s41612-023-00486-0.\n\n\nLanzante, John R, Keith W Dixon, Mary Jo Nath, Carolyn E Whitlock, and\nDennis Adams-Smith. 2018. ‚ÄúSome Pitfalls in\nStatistical Downscaling of Future\nClimate.‚Äù Bulletin of the American Meteorological\nSociety 99 (4): 791‚Äì803. https://doi.org/10.1175/bams-d-17-0046.1.\n\n\nLu, Yuchen, Benjamin Seiyon Lee, and James Doss-Gollin. 2025.\n‚ÄúBayesian Spatiotemporal Nonstationary Model Quantifies Robust\nIncreases in Daily Extreme Rainfall Across the Western Gulf\nCoast.‚Äù Environmental Research: Climate 4 (3):\n035016. https://doi.org/10.1088/2752-5295/adf56e.\n\n\nMatalas, Nicholas C, and Myron B Fiering. 1977. ‚Äú6.\nWater-Resource Systems Planning.‚Äù In Climate,\nClimatic Change, and Water Supply,\n99‚Äì110. Washington, DC: The National Academies Press. https://www.nap.edu/read/185/chapter/11.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. Second edition. Texts in Statistical Science\nSeries. Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nMcPhail, C., H. R. Maier, J. H. Kwakkel, M. Giuliani, A. Castelletti,\nand S. Westra. 2019. ‚ÄúRobustness Metrics: How Are They Calculated,\nWhen Should They Be Used and Why Do They Give Different Results?‚Äù\nEarth‚Äôs Future, April, 169‚Äì91. https://doi.org/10.1002/2017ef000649.\n\n\nMerz, Bruno, Jeroen C J H Aerts, Karsten Arnbjerg-Nielsen, M Baldi, A\nBecker, A Bichet, G√ºnter Bl√∂schl, et al. 2014. ‚ÄúFloods and\nClimate: Emerging Perspectives for Flood Risk Assessment and\nManagement.‚Äù Natural Hazards and Earth System Science 14\n(7): 1921‚Äì42. https://doi.org/10/gb9nzm.\n\n\nMignan, Arnaud. 2024. Introduction to Catastrophe Risk\nModelling: A Physics-based\nApproach. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781009437370.\n\n\nMilly, P C D, Julio Betancourt, M Falkenmark, R M Hirsch, Z W\nKundzewicz, D P Lettenmaier, and R J Stouffer. 2008. ‚ÄúStationarity\nIs Dead: Whither Water Management?‚Äù Science 319 (5863):\n573‚Äì74. https://doi.org/10.1126/science.1151915.\n\n\nMoel, Hans de, Mathijs van Vliet, and Jeroen C. J. H. Aerts. 2014.\n‚ÄúEvaluating the Effect of Flood Damage-Reducing Measures: A Case\nStudy of the Unembanked Area of Rotterdam, the\nNetherlands.‚Äù Regional Environmental Change\n14 (3): 895‚Äì908. https://doi.org/10.1007/s10113-013-0420-z.\n\n\nMudelsee, Manfred. 2020. ‚ÄúStatistical Analysis of Climate Extremes\n/ Manfred Mudelsee.‚Äù In Statistical Analysis of\nClimate Extremes. Cambridge, United Kingdom ; Cambridge University\nPress.\n\n\nNaghettini, Mauro, ed. 2017. Fundamentals of Statistical\nHydrology. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-43561-9.\n\n\nPiironen, Juho, and Aki Vehtari. 2017. ‚ÄúComparison of\nBayesian Predictive Methods for Model Selection.‚Äù\nStatistics and Computing 27 (3): 711‚Äì35. https://doi.org/10.1007/s11222-016-9649-y.\n\n\nPowell, Warren B. 2022. ‚ÄúSequential Decision\nAnalytics and Modeling: Modeling with\nPython,‚Äù November. https://nowpublishers.com/Article/BookDetails/9781638280828.\n\n\nPrice, Ilan, and Stephan Rasp. 2022. ‚ÄúIncreasing the Accuracy and\nResolution of Precipitation Forecasts Using Deep Generative\nModels.‚Äù March 23, 2022. https://doi.org/10.48550/arXiv.2203.12297.\n\n\nPyrcz, Michael J. 2024. Applied Machine Learning in\nPython: A Hands-on Guide with\nCode. https://geostatsguy.github.io/MachineLearningDemos_Book.\n\n\nRackauckas, Christopher, Yingbo Ma, Julius Martensen, Collin Warner,\nKirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan\nEdelman. 2020. ‚ÄúUniversal Differential Equations for\nScientific Machine Learning.‚Äù 2020. https://doi.org/10.48550/ARXIV.2001.04385.\n\n\nRazavi, Saman, Anthony Jakeman, Andrea Saltelli, Cl√©mentine Prieur,\nBertrand Iooss, Emanuele Borgonovo, Elmar Plischke, et al. 2020.\n‚ÄúThe Future of Sensitivity Analysis: An Essential Discipline for\nSystems Modeling and Policy Support.‚Äù Environmental Modelling\n& Software, December, 104954. https://doi.org/10.1016/j.envsoft.2020.104954.\n\n\nSalas, J D, J Obeysekera, and R M Vogel. 2018. ‚ÄúTechniques for\nAssessing Water Infrastructure for Nonstationary Extreme Events: A\nReview.‚Äù Hydrological Sciences Journal 63 (3): 325‚Äì52.\nhttps://doi.org/10.1080/02626667.2018.1426858.\n\n\nSaltelli, Andrea, Marco Ratto, Terry Andres, Francesca Campolongo,\nJessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano\nTarantola. 2008. Global Sensitivity Analysis: The Primer. John\nWiley & Sons, Ltd. http://onlinelibrary.wiley.com/doi/abs/10.1002/9780470725184.ch1.\n\n\nSavage, L. J. 1954. Foundations of Statistics. New York: Wiley.\n\n\nSchlef, Katherine E., Kenneth E. Kunkel, Casey Brown, Yonas Demissie,\nDennis P. Lettenmaier, Anna Wagner, Mark S. Wigmosta, et al. 2023.\n‚ÄúIncorporating Non-Stationarity from Climate Change into Rainfall\nFrequency and Intensity-Duration-Frequency (IDF)\nCurves.‚Äù Journal of Hydrology 616 (January): 128757. https://doi.org/10.1016/j.jhydrol.2022.128757.\n\n\nSeneviratne, S. I., X. Zhang, M. Adnan, W. Badi, C. Dereczynski, A. Di\nLuca, S. Ghosh, et al. 2021. ‚ÄúWeather and Climate Extreme Events\nin a Changing Climate.‚Äù Book section. In Climate Change 2021:\nThe Physical Science Basis. Contribution of\nWorking Group I to the Sixth Assessment Report of the\nIntergovernmental Panel on Climate Change, edited by V.\nMasson-Delmotte, P. Zhai, A. Pirani, S. L. Connors, C. P√©an, S. Berger,\nN. Caud, et al. Cambridge, UK; New York, NY, USA: Cambridge University\nPress. https://doi.org/10.1017/9781009157896.013.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. Second Edition. Cambridge,\nMassachusetts; London, England: MIT Press.\n\n\nTedesco, Marco, Steven McAlpine, and Jeremy R. Porter. 2020.\n‚ÄúExposure of Real Estate Properties to the 2018 Hurricane\nFlorence Flooding.‚Äù Natural Hazards and Earth System\nSciences 20 (3): 907‚Äì20. https://doi.org/10.5194/nhess-20-907-2020.\n\n\nThuerey, N., B. Holzschuh, P. Holl, G. Kohl, M. Lino, Q. Liu, P.\nSchnell, and F. Trost. 2024. Physics-Based Deep Learning. https://physicsbaseddeeplearning.org.\n\n\nWing, Oliver E. J., Nicholas Pinter, Paul D. Bates, and Carolyn Kousky.\n2020. ‚ÄúNew Insights into US Flood Vulnerability\nRevealed from Flood Insurance Big Data.‚Äù Nature\nCommunications 11 (1, 1): 1444. https://doi.org/10.1038/s41467-020-15264-2.",
    "crumbs": [
      "References üéØ"
    ]
  },
  {
    "objectID": "chapters/appendices/software.html",
    "href": "chapters/appendices/software.html",
    "title": "Appendix A ‚Äî Software Setup ‚úèÔ∏è",
    "section": "",
    "text": "A.1 Quick start\nIf you want to run the computational notebooks in this book, or apply a similar workflow, then these instructions are for you.\nThis section provides step-by-step instructions to get your development environment set up and running.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Software Setup ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/software.html#quick-start",
    "href": "chapters/appendices/software.html#quick-start",
    "title": "Appendix A ‚Äî Software Setup ‚úèÔ∏è",
    "section": "",
    "text": "A.1.1 Installation steps\n\nInstall Visual Studio Code - your code editor\n\nThere are other good IDEs out there, and you can absolutely use one.\nVS Code is a good and well-supported starting point\n\nInstall Quarto - for creating documents with code\n\nFor step 1, choose your operating system\nFor step 2, choose VS Code as your tool\n\nInstall Julia using JuliaUp - the programming language\n\nFollow the directions on the GitHub page based on your operating system\nDon‚Äôt worry about the Continuous Integration (CI) section or anything below it\nInstall Julia 1.11 using juliaup add 1.11\nSet this to be your default version using juliaup default 1.11\nYou should get a message that says something like Configured the default Julia version to be '1.11'\n\nIn VS Code: Install extensions from the Extensions marketplace\n\nInstall the Julia extension (provides syntax highlighting, code completion, and integrated REPL)\nInstall the Quarto extension (provides syntax highlighting and preview capabilities for .qmd files)\n\nInstall GitHub Desktop - for version control\n\nThis is optional if you prefer to use git through the command line or another app, but GitHub Desktop is a good default recommendation\n\n\n\n\nA.1.2 Verification\nAfter installation, you should be able to:\n\nOpen VS Code and see the Julia and Quarto extensions listed\nOpen a terminal and type julia to start the Julia REPL\nCreate a new Quarto document (.qmd file) in VS Code with syntax highlighting",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Software Setup ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/software.html#dig-deeper",
    "href": "chapters/appendices/software.html#dig-deeper",
    "title": "Appendix A ‚Äî Software Setup ‚úèÔ∏è",
    "section": "A.2 Dig deeper",
    "text": "A.2 Dig deeper\n\nA.2.1 Julia\nJulia is a fast, modern programming language designed for scientific computing. Its syntax closely mirrors mathematical notation, making it intuitive for researchers while delivering performance comparable to C and Fortran.\nJuliaUp is the official Julia version manager. It simplifies installation, allows you to maintain multiple Julia versions simultaneously, and keeps your installation current with the latest releases. This is especially useful as the Julia ecosystem evolves rapidly.\nSee the Julia page for more.\n\n\nA.2.2 Quarto\nQuarto is a scientific publishing system that enables you to combine code, results, and narrative text in reproducible documents. Think of it as the next generation of R Markdown, but with multi-language support (Julia, Python, R, and more).\nThis textbook is written in Quarto. Unlike traditional notebooks, Quarto documents are plain text files that render to multiple output formats (HTML, PDF, Word, presentations) while maintaining computational reproducibility.\nYou can learn more at:\n\nOfficial Tutorial: Hello, Quarto - basic document creation\nOfficial Tutorial: Computations - integrating code\nComprehensive Quarto documentation\n\n\nA.2.2.1 Writing with Markdown and math\nQuarto uses Markdown syntax with LaTeX math notation. Essential references:\n\nMarkdown Cheatsheet - basic text formatting\nLaTeX Cheatsheet - mathematical notation\nMathpix Snip - convert equation images to LaTeX code (free tier available)\nDetexify - draw symbols to find LaTeX commands\n\n\n\n\nA.2.3 Visual Studio Code\nVisual Studio Code is a free, open-source code editor developed by Microsoft. Its strength lies in its extensibility‚Äîthousands of extensions add language support, debugging capabilities, and productivity tools.\nFor our workflow, the Julia extension transforms VS Code into a full Julia development environment with syntax highlighting, intelligent code completion, integrated debugging, and a built-in REPL. The Quarto extension provides similar capabilities for computational documents, including live preview and cell execution.\nYou can learn more at the official tutorial.\n\n\nA.2.4 Git and GitHub\nGit is a distributed version control system that tracks changes in your code over time. GitHub is a cloud-based platform that hosts Git repositories and adds collaboration features like issue tracking, pull requests, and project management.\nVersion control is essential for reproducible research‚Äîit allows you to track changes, collaborate with others, recover from mistakes, and share your work publicly. This textbook itself is maintained on GitHub.\nYou can learn more at:\n\nGit and GitHub for Poets - beginner-friendly video series\nGitHub Hello World ‚Äì official docs\nVersion Control - comprehensive guide from MIT‚Äôs ‚ÄúMissing Semester‚Äù",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Software Setup ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/julia.html",
    "href": "chapters/appendices/julia.html",
    "title": "Appendix B ‚Äî Julia Learning Resources ‚úèÔ∏è",
    "section": "",
    "text": "B.1 Why Julia?\nThe computational examples in this textbook use the Julia programming language.\nJulia is a fast, modern, open-source programming language designed for scientific and numerical computing. The language is designed to be fast, dynamic, and easy to use and maintain.\nKey advantages for this textbook include:\nWhile Julia is powerful for computational thinking and research, many ecosystems remain stronger in other languages (like Python‚Äôs deep learning and climate data analysis tools), so a well-rounded programmer benefits from learning multiple languages.\nYou can read more about Julia‚Äôs design philosophy:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Julia Learning Resources ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/julia.html#why-julia",
    "href": "chapters/appendices/julia.html#why-julia",
    "title": "Appendix B ‚Äî Julia Learning Resources ‚úèÔ∏è",
    "section": "",
    "text": "High-Level Syntax: Julia has a clean and expressive syntax that closely parallels mathematical notation.\nPerformance: Julia compiles to efficient machine code, achieving speeds comparable to low-level languages like C and Fortran. This solves the ‚Äútwo-language problem,‚Äù where you might prototype in a high-level language but need to rewrite for performance.\nSimplified Dependencies: Eliminates or reduces the need for dependencies on C and Fortran libraries, which simplifies installation and maintenance.\nOpen-Source and Shareable: Julia is completely open-source with excellent package management for reproducible research environments.\nStrong Ecosystem: Despite being newer, Julia has a rapidly growing ecosystem of high-quality libraries for scientific domains.\n\n\n\n\nJulia Data Science textbook is didactic and clear\nWhy We Created Julia from the founders\nWhy Julia Manifesto is more comprehensive",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Julia Learning Resources ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/julia.html#learning-resources",
    "href": "chapters/appendices/julia.html#learning-resources",
    "title": "Appendix B ‚Äî Julia Learning Resources ‚úèÔ∏è",
    "section": "B.2 Learning resources",
    "text": "B.2 Learning resources\nThis textbook aims to never reinvent the wheel. There are lots of exceptional resources for learning Julia, or for learning computational concepts with Juila. Here are some favorites:\n\nMIT‚Äôs Introduction to Computational Thinking: Julia-based course covering applied mathematics and computational thinking\nJulia for Nervous Beginners: free course for people hesitant but curious about learning Julia\nJulia Data Science: comprehensive introduction to data science with Julia\nFastTrack to Julia cheatsheet\nComprehensive Julia Tutorials: YouTube playlist covering Julia topics\nMatlab-Python-Julia Cheatsheet: helpful if you‚Äôre experienced in one of these languages\n\n\nB.2.1 Specialized topics\nHere are some additional resources for specific Julia tools and packages developed in this class\n\nPlotting: Makie Tutorials and MakieCon 2023 YouTube Channel\nStatistical Modeling: Turing.jl tutorials has detailed examples of using Turing for modeling",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Julia Learning Resources ‚úèÔ∏è</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/llm.html",
    "href": "chapters/appendices/llm.html",
    "title": "Appendix C ‚Äî Large Language Models (‚ÄúAI‚Äù) ‚úèÔ∏è",
    "section": "",
    "text": "Coding is an integral part of real-world climate-risk analysis, and large language models (LLMs; often referred to as ‚ÄúAI‚Äù models) are rapidly changing how some kinds of coding happen. Beyond web-based chatbots, you may have useed tools like GitHub Copilot (free for students and educators) or Claude Code (see free Deeplearning.AI Course). LLMs use powerful new technologies that can support learning and replace tedious tasks, but they can also threaten your intellectual growth and skill development (Kosmyna et al. 2025; Bastani et al. 2025).\nIt is clear that there are some tasks that should be delegated to these models and some tasks that must remain human-driven. However, there are tremendous differences of opinion about how most tasks in the middle can or should be allocated. As you wrestle with these questions for yourself, you should explore resources like:\n\nAI Snake Oil is a blog that seeks to dispel hype, remove misconceptions, and clarify the limits of AI. The authors are in the Princeton University Department of Computer Science.\nAI software assistants make the hardest kinds of bugs to spot from Pluralistic is a thoughtful and deep blog post about the perils of (mis)using LLMs for coding.\nOne Useful Thing is a newsletter about AI focused on implications for work and education. The authors‚Äô prompt library is also a good resource for working with LLMs.\nEd Zitron‚Äôs Where‚Äôs Your Ed At is a newsletter that takes a critical perspective on the business models and hype narratives around AI.\n\n\n\n\n\nBastani, Hamsa, Osbert Bastani, Alp Sungu, Haosen Ge, √ñzge Kabakcƒ±, and Rei Mariman. 2025. ‚ÄúGenerative AI Without Guardrails Can Harm Learning: Evidence from High School Mathematics.‚Äù Proceedings of the National Academy of Sciences 122 (26): e2422633122. https://doi.org/10.1073/pnas.2422633122.\n\n\nKosmyna, Nataliya, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, and Pattie Maes. 2025. ‚ÄúYour Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task.‚Äù June 10, 2025. https://doi.org/10.48550/arXiv.2506.08872.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Large Language Models (\"AI\") ‚úèÔ∏è</span>"
    ]
  }
]