[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climate Risk Assessment and Management",
    "section": "",
    "text": "Welcome 🎯\nWelcome to Climate Risk Assessment and Management, an online textbook under construction by James Doss-Gollin.",
    "crumbs": [
      "Welcome 🎯"
    ]
  },
  {
    "objectID": "index.html#motivation-and-scope",
    "href": "index.html#motivation-and-scope",
    "title": "Climate Risk Assessment and Management",
    "section": "Motivation and Scope",
    "text": "Motivation and Scope\n\nHistory\nThis project emerged from two courses taught at Rice University by James Doss-Gollin: CEVE 543 focused on climate hazard and extremes and CEVE 421/521 focused on risk management.\n\n\nAim\nThe book is motivated by questions like\n\nWhat is the probability distribution of wind speeds that a building structure might experience?\nWhat will the probability distribution of extreme rainfall be in 2050, and what drives uncertainty in this estimate?\nWhat is the probability distribution of tropical cyclone losses across a regional portfolio?\nWhen, and how high, should a house be elevated to proactively manage future flood risk?\nWhat are robust, efficient, and equitable strategies for reducing flood risk in an urban area?\n\nThese questions span scales and sectors, yet they share fundamental challenges: characterizing extreme events, quantifying uncertainty, assessing risks, and making robust decisions when probability distributions are unknown or contested. Moreover, there is not a single correct answer to these questions, or a single method that will incontrovertibly answer them.",
    "crumbs": [
      "Welcome 🎯"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "Climate Risk Assessment and Management",
    "section": "How to Use This Resource",
    "text": "How to Use This Resource\nThe book is designed to be useful for practitioners, students, and teachers. Teachers may use individual chapters in their courses. Students may use it as a class text or reference. Practitioners may focus on specific chapters relevant to their work. Each chapter includes learning objectives and can be read independently, though some chapters build on concepts introduced in others.\n\nStructure\n\nThe Preface introduces the book’s motivation and frames key challenges\nPart 1 introduces key topics in probability, inference, Bayesian methods, optimization, machine learning, and Earth science. Rather than providing a comprehensive treatment, this part focuses on essential concepts and links to further resources.\nPart 2 focuses on hazard assessment, namely modeling climate hazards and extremes. Material is organized around thematic applications and predictive tasks. The foundational idea is integrating information from noisy and/or biased sources to estimate the joint probability distribution of relevant hydroclimatic variables.\nPart 3 risk management, which involves both mapping hazard to risk and designing interventions to manage these risks. Key ideas include the sequential nature of decisions, the pursuit of unclear and/or contested objectives, and the need to account for the sensitivity of estimated probability distributions (of hazard and of other relevant physical, social, and economic variables) to underlying models and assumptions.\nComputational notebooks written in Julia illustrate and complement the methods and concepts discussed in the text. While notebooks are referenced in the text, they are designed as standalone and self-contained resources.\n\n\n\nPrerequisites\nBasic probability and multivariate calculus, along with linear algebra, are sufficient mathematical foundations for this textbook. Some exposure to Earth science, hydrology, water resources, or related topics is strongly encouraged for context, though not strictly necessary for understanding methods. This book builds on a wide range of topics and methods in statistics, machine learning, optimization, and Earth science, and expertise in any of these areas may deepen your understanding, but is not necessary. No programming is required to read the book, but going through computational examples and applying methods to your own problems, which can substantially strengthen your understanding, does require programming.",
    "crumbs": [
      "Welcome 🎯"
    ]
  },
  {
    "objectID": "chapters/about/license.html",
    "href": "chapters/about/license.html",
    "title": "License",
    "section": "",
    "text": "This textbook is licensed under the CC BY-NC 4.0 License. It is free to use, share, and adapt for non-commercial purposes, provided that you give appropriate credit, provide a link to the license, and indicate if changes were made. If you would like to use this content for commercial purposes, please contact me.",
    "crumbs": [
      "**About this book**",
      "License"
    ]
  },
  {
    "objectID": "chapters/about/contributing.html",
    "href": "chapters/about/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "This textbook is a work in progress, and we welcome your contributions. Whether it’s fixing a typo or proposing a new module, every suggestion helps. The easiest way to contribute is to fork the repository and submit a pull request. If you’re not comfortable with that workflow, please open an issue on GitHub.",
    "crumbs": [
      "**About this book**",
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/about/citing.html",
    "href": "chapters/about/citing.html",
    "title": "Citing",
    "section": "",
    "text": "Please cite this resource as\n@book{doss-gollin_textbook:2025,\n  author = {Doss-Gollin, James},\n  title = {Climate Risk Management},\n  year = {2025},\n  url = {https://jdossgollin.github.io/climate-risk-book},\n}\nIn the future, we will move to stable releases with numbered versions.",
    "crumbs": [
      "**About this book**",
      "Citing"
    ]
  },
  {
    "objectID": "chapters/about/resources.html",
    "href": "chapters/about/resources.html",
    "title": "Further Reading",
    "section": "",
    "text": "Inspiration\nClimate risk assessment and management are complex and interdisciplinary topics, and we are by no means comprehensive here. This page provides some helpful resources (textbooks, detailed online tutorials, and class websites) for your continued and supplementary study.\nThis textbook draws inspiration and content from several courses and lecture notes, and I am grateful to the instructors who have shared their materials with me.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#inspiration",
    "href": "chapters/about/resources.html#inspiration",
    "title": "Further Reading",
    "section": "",
    "text": "Upmanu Lall’s Environmental Data Analysis course at Columbia\nVivek Srikrishnan’s Environmental Systems Analysis and Climate Risk Analysis classes at Cornell\nR. Balaji’s Advanced Data Analysis Techniques (Statistical Learning Techniques for Engineering and Science) course at CU Boulder\nAlberto Montanari’s collection of open course notes and lectures\nApplegate and Keller (2015) motivates this project and demonstrates problem-based learning.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#stats-ml-basics",
    "href": "chapters/about/resources.html#stats-ml-basics",
    "title": "Further Reading",
    "section": "Stats + ML basics",
    "text": "Stats + ML basics\nThis book assumes familiarity with these topics, but these resources may be helpful as a refresher.\n\nBlitzstein and Hwang (2019) provides a thorough introduction to key concepts and ideas in probability. The book accompanies a free online course, Stat 110, which is a great resource for learning probability and statistics. Practice problems and solutions, handouts, and lecture videos are all available online.\nDowney (2021) offers an introduction to Bayesian statistics using computational methods. It’s not environment focused but provides code and a clear explanation of core concepts.\nGelman (2021) is a textbook designed for a first course on applied statistics. Clear and well-worked examples underpin discussion of fundamental ideas in statistical analysis and thinking about data.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#applications",
    "href": "chapters/about/resources.html#applications",
    "title": "Further Reading",
    "section": "Applications",
    "text": "Applications\nThere are lots of related books on catastrophe modeling, water resources research, geostats, statistical hydrology and related topics. Here is an incomplete list of some core references.\n\nNaghettini (2017) is a textbook on statistical hydrology that covers many of the same topics as this course. The statistical hydrology literature often obfuscates key ideas with complex notation and terminology, but this book is a helpful introduction to the field.\nHelsel et al. (2020) is a comprehensive introduction to water resources and hydrology, focusing on statistical methods for analyzing hydrologic data. Its methods are traditional, with less emphasis on machine learning or Bayesian methods and more attention to null hypothesis significance testing, but its case studies are well-worked and thoughtfully described.\nAbernathey (2024) is an excellent resource covering introductory topics in Earth and climate data science using Python, with an emphasis on foundational computations. These core computational concepts serves as a recommended prerequisite for more advanced material in this book.\nPyrcz (2024) is a textbook focused on applied machine learning, with a particular focus on geostatistics. There’s less focus on extremes, hydroclimate, and decision-making, but it provides very clear and interpretable explanations of many machine learning methods, including some that are not directly covered in this book.\nMignan (2024) is a modern introduction to catastrophe risk modeling that covers a wide range of hazards, including hydroclimatic extremes, from a physics-based perspective. It provides a structured framework for quantifying hazard, exposure, and vulnerability, following industry-standard CAT modeling approaches. While broader in scope and more introductory in level, it complements this book’s focus by illustrating foundational principles of probabilistic risk modeling in practice.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/about/resources.html#more-stats-ml",
    "href": "chapters/about/resources.html#more-stats-ml",
    "title": "Further Reading",
    "section": "More Stats + ML",
    "text": "More Stats + ML\nThis book covers a broad set of topics in statistics, machine learning, and optimization. Most chapters could be a textbook of their own, and in fact many exist.\n\nFriedman, Hastie, and Tibshirani (2001) is a classic introduction to machine learning, which complements the Bayesian perspective nicely.\nJaynes (2003) is a classic text on probability theory that you should read if you’re interested in questions like “what is probability?”\nGelman et al. (2014) and McElreath (2020) are the classic textbooks on Bayesian inference and provide a wealth of insight and detail. The Gelman textbook is a bit more dense while the McElreath book has a more conversational tone, but both cover similar topics.\nCressie and Wikle (2011) provides a detailed exploration of hierarchical space-time models. There have been some computational advances since then that are worth keeping in mind before you apply these models directly, but it’s a clearly written and overview.\nThuerey et al. (2024) is a new textbook on physics-based deep learning, which is a rapidly growing area of research. It provides a comprehensive overview of the field, including theoretical foundations and practical applications. It covers topics, including neural operators and diffusion models, that are not covered in this course, but which are increasingly used in the climate risk space.\nBishop and Bishop (2024) is a comprehensive, modern, and accessible start-to-finish textbook covering machine learning from basic probability through diffusion models.\nMichael Betancourt’s writing page has detailed and mathematically rigorous explanations of many topics in Bayesian data analysis and probabilistic modeling.\n\n\n\n\n\nAbernathey, Ryan. 2024. An Introduction to Earth and Environmental Data Science. https://earth-env-data-science.github.io/intro.html.\n\n\nApplegate, Patrick, and Klaus Keller. 2015. Risk Analysis in the Earth Sciences. Leanpub. https://leanpub.next/raes.\n\n\nBishop, Christopher M., and Hugh Bishop. 2024. Deep Learning: Foundations and Concepts. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-45468-4.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability, Second Edition. 2nd Edition. Boca Raton: Chapman and Hall/CRC. http://probabilitybook.net.\n\n\nCressie, Noel A. C., and Christopher K. Wikle. 2011. Statistics for Spatio-Temporal Data. Hoboken, N.J.: Wiley.\n\n\nDowney, Allen B. 2021. Think Bayes. \"O’Reilly Media, Inc.\". https://allendowney.github.io/ThinkBayes2/.\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. Springer series in statistics Springer, Berlin.\n\n\nGelman, Andrew. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge, United Kingdom ; Cambridge University Press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 2014. Bayesian Data Analysis. 3rd ed. Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nHelsel, Dennis R., Robert M. Hirsch, Karen R. Ryberg, Stacey A. Archfield, and Edward J. Gilroy. 2020. Statistical Methods in Water Resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. New York, NY: Cambridge University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second edition. Texts in Statistical Science Series. Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nMignan, Arnaud. 2024. Introduction to Catastrophe Risk Modelling: A Physics-based Approach. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781009437370.\n\n\nNaghettini, Mauro, ed. 2017. Fundamentals of Statistical Hydrology. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-43561-9.\n\n\nPyrcz, Michael J. 2024. Applied Machine Learning in Python: A Hands-on Guide with Code. https://geostatsguy.github.io/MachineLearningDemos_Book.\n\n\nThuerey, N., B. Holzschuh, P. Holl, G. Kohl, M. Lino, Q. Liu, P. Schnell, and F. Trost. 2024. Physics-Based Deep Learning. https://physicsbaseddeeplearning.org.",
    "crumbs": [
      "**About this book**",
      "Further Reading"
    ]
  },
  {
    "objectID": "chapters/preface.html",
    "href": "chapters/preface.html",
    "title": "Preface",
    "section": "",
    "text": "What is climate risk?\nClimate risks arise at the intersection of climate hazards, exposed systems, and vulnerability. They manifest when extreme or changing climate conditions—floods, droughts, extreme temperatures, sea-level rise, or shifting precipitation patterns—impact human and natural systems that are exposed and vulnerable to these conditions. The financial sector terms these “physical risks” to distinguish them from transition risks related to policy and market changes.\nClimate risks span scales from the hyperlocal (a single building’s flood exposure) to the global (climate impacts on agricultural productivity). They encompass immediate acute risks from individual extreme events and longer-term chronic risks from gradual climate changes. Crucially, climate risks are not solely natural phenomena but emerge from the complex interactions between climate hazards and the human systems—infrastructure, institutions, communities, and economies—that experience their impacts.\nClimate risk is often defined as the product of hazard (probability that something will happen) and consequences (exposure and vulnerability). However, it’s often helpful to start with the decisions we care about.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#what-is-climate-risk",
    "href": "chapters/preface.html#what-is-climate-risk",
    "title": "Preface",
    "section": "",
    "text": "Risk management\nThe goal of assessing climate risks is to manage them, as is the focus of Part III. We manage climate risks by\n\nbuilding infrastructure, such as seawalls, stormwater pipes, oyster beds, green roofs, dams\ndesigning policy, such as water pricing, land-use regulations, building codes\nresponding to climate disasters through disaster response and recovery. While emergency management is beyond the scope of the book, disaster prevention (through infrastructure, policy, etc) and preparation (planning evacuation routes, assessing resource needs, etc) are problems that the tools of this class can inform.\n\nA key insight from considering these applications is that climate risks are not natural phenomena, but occur at the intersection of natural and human systems. A second insight is that decisions about how to manage climate risks do not depend only on climate hazard, but also on human systems and values.\n\n\nExposure and vulnerability\nHazards do not create consequences by themselves. Hazards affect things that we care about, whether natural ecosystems, human homes, infrastructure systems, or something else. Quantitatively these are often described as exposure and vulnerability. However, this is not always a helpful framing because everything is exposed, to at least some degree, to climate hazards.\n\n\nClimate hazard\nClimate hazards have several key characteristics:\n\nLocation-specific impacts: Specific weather patterns cause different things in different places—tropical cyclones cause extreme winds on the Gulf Coast, while persistent intense rainfall causes flooding in major rivers\nRequire Earth science and data: Understanding hazards requires both physical process knowledge and empirical data\nVariable focus on extremes: Some applications care about extremes, but others (e.g., water management) care about shifts in the whole distribution\nMulti-scale variability: Characterized by variability across multiple spatial and temporal scales",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#what-are-good-strategies",
    "href": "chapters/preface.html#what-are-good-strategies",
    "title": "Preface",
    "section": "What are good strategies?",
    "text": "What are good strategies?\n\nThe simple story\nIn principle, managing climate risks should be straightforward. If we had clear objectives and well-characterized uncertainty, there are established mathematical formalisms for decision-making under uncertainty. Notably, Bayesian Decision Theory provides an elegant framework: find the action \\(a\\) that maximizes expected utility \\[\n\\mathbb{E}[U(a)] = \\int U(a, s) p(s) ds,\n\\] where \\(U(a, s)\\) is the utility of action \\(a\\) given state of the world \\(s\\), and \\(p(s)\\) is the joint probability distribution over states of the world. The expectation \\(\\mathbb{E}[U(a)]\\) represents the average utility we would expect from action \\(a\\) across all possible future states, weighted by their probabilities (see Chapter on Probability and Statistics for mathematical foundations).\nWith this framework and modern advances in operations research and optimization, we could frame climate risk management as a large-scale optimization problem. This might still be a challenging problem, requiring sophisticated optimization methods, large-ensemble Monte Carlo simulation, high-performance computing, and more, but fundamentally there would be a right answer that we could identify, at least seek to approximate.\n\n\nWhy this isn’t enough\nIn practice, climate risk management defies this idealized approach for several fundamental reasons:\n\nDeep uncertainty: Unlike textbook optimization problems, we rarely have well-defined probability distributions over future states. Climate risks involve poorly characterized, multiple, and interacting uncertainties spanning physical processes (climate projections), socioeconomic factors (development patterns, institutional capacity, human behavior), and their complex dependencies. The probability distributions we need span climate hazards, exposure patterns, vulnerability functions, and policy effectiveness—all evolving in ways that resist precise characterization.\nLarge and poorly defined decision spaces: The solution space includes not just individual projects but entire systems: infrastructure networks, policy portfolios, risk transfer arrangements, and adaptive management sequences. These decisions interact across scales, sectors, and time horizons in ways that resist comprehensive optimization.\nContested objectives: Different stakeholders hold different values about what we should optimize for—economic efficiency, equity, robustness, or flexibility. These objectives often conflict, and their relative importance is itself contested and evolving.\n\nThis brings us to a crucial insight: we cannot simply frame climate risk management as a big optimization problem. The field has witnessed an explosion of computational tools—climate models with ever-finer resolution, machine learning algorithms for processing vast datasets, and sophisticated visualization platforms for rendering complex projections. While these advances represent genuine progress, their proliferation has created new challenges for practitioners seeking to manage real-world climate risks.\nThe abundance of available tools does not automatically translate to better decisions. Indeed, the sophistication of modern computational approaches can obscure fundamental questions about problem framing, uncertainty characterization, and appropriate methods selection. Without solid conceptual foundations, practitioners may find themselves applying powerful tools inappropriately or mistaking methodological novelty for substantive insight.\n\n\nThe stakes of getting it wrong\nThe consequences of inadequate climate risk management are severe and diverse. Infrastructure failures occur when designs based on historical extremes prove insufficient for future conditions—leading to flooded neighborhoods when storm drains are undersized, or to costly over-design when extreme projections are treated as certainties. Policy mistakes compound these problems: development policies that ignore flood risks concentrate vulnerable populations in harm’s way, while overly conservative regulations can stifle economic development without commensurate risk reduction benefits.\nFinancial miscalculations affect both public and private sectors. Insurance companies that underestimate climate risks face catastrophic losses, while those that overestimate risks price themselves out of markets. Infrastructure investors struggle to balance climate resilience against cost constraints, often erring toward solutions that prove either inadequate or prohibitively expensive. These failures cascade across scales: a poorly designed local drainage system contributes to regional flood management challenges, while flawed national climate risk assessments misguide infrastructure investment priorities across entire countries.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#this-book",
    "href": "chapters/preface.html#this-book",
    "title": "Preface",
    "section": "This book",
    "text": "This book\nThis book develops both the technical tools and conceptual frameworks needed for climate risk management:\n\nPart I provides the statistical, optimization, and machine learning foundations that enable rigorous analysis of climate risks and decision alternatives\nPart II focuses on characterizing climate hazards and their uncertainties, emphasizing the integration of multiple imperfect information sources\nPart III addresses the transition from hazard to risk and the design of management strategies under deep uncertainty\n\nThroughout, we emphasize that technical sophistication must be coupled with conceptual clarity about the nature of climate risks and the limits of optimization approaches. The goal is not to abandon quantitative analysis, but to use it more wisely—focusing computational power where it adds most value while acknowledging the irreducible uncertainties that require adaptive, robust approaches to climate risk management.\nThis book aims to teach readers how to apply tools from applied mathematics, statistics, and machine learning to answer questions such as\n\nWhat is the probability distribution of some relevant hazards or variables, such as (rainfall, wind, flood, temperature, streamflows) at a specific location?\nHow do these probability distributions change in the next 50 years?\nHow uncertain are these estimates and what specific mechanisms drive these uncertainties?\nWhat is the distribution of annual losses of a portfolio of assets exposed to one or many climate risks?\nWhat are trade-offs between up-front costs and future damages for decisions like how high to elevate a house?\nWhat are robust strategies for sequentially hardening infrastructure against climate risks?\nWhat are trade-offs between flood and drought protection for managing a reservoir?\n\nWhile Part I does provide building blocks, they are intended to be self-contained references rather than a comprehensive overview to applied math, statistics, computer science, machine learning, and operations research. Instead, it aims to give you “just enough” context to think carefully about how to apply tools from these fields to climate risk management challenges.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/preface.html#what-this-book-is-not",
    "href": "chapters/preface.html#what-this-book-is-not",
    "title": "Preface",
    "section": "What this book is not",
    "text": "What this book is not\nThis book focuses on the technical foundations of climate risk assessment and quantitative decision-making under uncertainty. While we address design requirements, social dimensions, and stakeholder considerations throughout—recognizing that technical tools can significantly inform these challenges—there are important aspects of climate risk management that require specialized expertise beyond our scope.\nThis book will not primarily teach you how to:\n\nManage reputational and transition risks: While we focus on physical climate risks and their quantitative assessment, organizations also face complex risks from changing policies, markets, and stakeholder expectations that require specialized risk management expertise\nDesign and implement adaptive organizations: While we cover adaptive management strategies and robust decision-making frameworks, the organizational design and management expertise needed to implement these approaches in practice requires additional specialized knowledge\nFacilitate stakeholder processes: While the quantitative tools we teach can strongly support consensus building by clarifying trade-offs and uncertainties, the facilitation, negotiation, and collaborative governance skills needed to lead stakeholder processes require specialized training\nDevelop communication strategies: While we emphasize how to interpret and present quantitative risk assessments, developing effective communication strategies for diverse audiences—policymakers, communities, investors—requires specialized expertise in science communication and public engagement\nNavigate implementation challenges: While we address policy design and infrastructure planning from an analytical perspective, the practical challenges of construction management, regulatory processes, and community engagement require domain-specific expertise\n\nThis is an interdisciplinary text that draws insights from multiple fields and acknowledges the social, political, and institutional contexts that shape climate risk management. However, our primary focus remains on the quantitative and analytical foundations that can inform—but not replace—the broader expertise needed for effective practice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html",
    "href": "chapters/fundamentals/climate-science.html",
    "title": "1  Climate science 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate science 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#learning-objectives",
    "href": "chapters/fundamentals/climate-science.html#learning-objectives",
    "title": "1  Climate science 🚧",
    "section": "",
    "text": "Understand why climate science is essential for risk assessment\nIdentify key climate science topics you should study for effective risk management\nNavigate to appropriate resources for learning climate science fundamentals",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate science 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#why-climate-science-matters-for-risk-assessment",
    "href": "chapters/fundamentals/climate-science.html#why-climate-science-matters-for-risk-assessment",
    "title": "1  Climate science 🚧",
    "section": "1.1 Why climate science matters for risk assessment",
    "text": "1.1 Why climate science matters for risk assessment\nClimate hazards don’t occur in isolation. A hurricane’s intensity depends on sea surface temperatures and atmospheric conditions. Droughts emerge from large-scale circulation patterns and ocean-atmosphere interactions. Floods reflect not just local rainfall but also broader weather systems and seasonal cycles.\nUnderstanding these connections is crucial for risk assessment because climate science helps us answer three fundamental questions:\n\nWhat physical processes create hazardous weather patterns? Understanding the mechanisms behind hurricanes, droughts, heat waves, and floods helps us identify when and where they’re most likely to occur.\nHow do these patterns vary naturally over time? Climate systems exhibit variability on multiple timescales—from seasonal cycles to multi-decadal oscillations—that affect the frequency and intensity of extreme events.\nHow might climate change alter these patterns? As greenhouse gas concentrations rise, the statistical properties of weather and climate are shifting, requiring us to account for non-stationarity in our risk assessments.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate science 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#essential-climate-science-topics",
    "href": "chapters/fundamentals/climate-science.html#essential-climate-science-topics",
    "title": "1  Climate science 🚧",
    "section": "1.2 Essential climate science topics",
    "text": "1.2 Essential climate science topics\n\nClimate models and modeling\nMulti-scale variability\nSpecific weather patterns\nClimate change and sensitivity\nHydrologic cycle",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate science 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/climate-science.html#further-reading",
    "href": "chapters/fundamentals/climate-science.html#further-reading",
    "title": "1  Climate science 🚧",
    "section": "Further reading",
    "text": "Further reading\n\nMudelsee (2020): statistical approaches to climate extremes\nMerz et al. (2014): flood risk methods connecting climate to impacts\nGhil et al. (2011): physical processes and extreme value behavior\n\n\n\n\n\nGhil, M, P Yiou, S Hallegatte, B D Malamud, P Naveau, A Soloviev, P Friederichs, et al. 2011. “Extreme Events: Dynamics, Statistics and Prediction.” Nonlinear Processes in Geophysics 18 (3): 295–350. https://doi.org/10/fvzxvv.\n\n\nMerz, Bruno, Jeroen C J H Aerts, Karsten Arnbjerg-Nielsen, M Baldi, A Becker, A Bichet, Günter Blöschl, et al. 2014. “Floods and Climate: Emerging Perspectives for Flood Risk Assessment and Management.” Natural Hazards and Earth System Science 14 (7): 1921–42. https://doi.org/10/gb9nzm.\n\n\nMudelsee, Manfred. 2020. “Statistical Analysis of Climate Extremes / Manfred Mudelsee.” In Statistical Analysis of Climate Extremes. Cambridge, United Kingdom ; Cambridge University Press.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Climate science 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html",
    "href": "chapters/fundamentals/probability-stats.html",
    "title": "2  Probability and inference ✏️",
    "section": "",
    "text": "Learning objectives\nThis chapter covers the fundamental concepts of probabilistic modeling and statistical inference. Data analysis proceeds by building a generative model—a formal, probabilistic hypothesis about how data are created. Inference is the inverse problem of using observed data to learn about model parameters. Computational methods make complex inference problems tractable.\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and inference ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#probability-theory",
    "href": "chapters/fundamentals/probability-stats.html#probability-theory",
    "title": "2  Probability and inference ✏️",
    "section": "2.1 Probability theory",
    "text": "2.1 Probability theory\nThe concepts in this section provide the mathematical language for describing uncertainty. These mathematical tools support practical modeling applications.\n\n2.1.1 Basic concepts\n\n2.1.1.1 Random variables\nA random variable is a function that assigns numerical values to the outcomes of a random experiment. Random variables provide the mathematical foundation for describing uncertainty.\n\nDiscrete random variables take on countable values (e.g., number of floods per year)\nContinuous random variables take on uncountable values (e.g., temperature, precipitation amount)\n\n\n\n2.1.1.2 Notation conventions\n\nRandom variables: Capital letters (\\(X\\), \\(Y\\), \\(Z\\))\nRealizations (specific values): Lowercase letters (\\(x\\), \\(y\\), \\(z\\))\nParameters: Greek letters (\\(\\theta\\), \\(\\mu\\), \\(\\sigma\\))\nObserved data: \\(y\\) (following Bayesian convention)\nPredictions: \\(\\tilde{y}\\) (y-tilde)\n\n\n\n\n2.1.2 Distribution functions\nThe foundation of probability theory rests on three fundamental functions that describe random variables.\n\n2.1.2.1 Probability Mass Function (PMF)\nFor discrete random variables, \\(P(X = x)\\) gives the probability that the variable takes on a specific value \\(x\\). The PMF satisfies \\(\\sum_x P(X = x) = 1\\).\n\n\n2.1.2.2 Probability Density Function (PDF)\nFor continuous random variables, \\(p(x)\\) describes the relative likelihood of different values.\nPDF \\(p(x)\\) is not a probability but a density. Since probability is density multiplied by a (potentially very small) interval of \\(x\\), the value of \\(p(x)\\) itself can exceed 1 without violating the laws of probability. Probabilities are areas under the curve: \\(P(a \\leq X \\leq b) = \\int_a^b p(x) \\, dx\\). PDFs are sometimes written as \\(f(x)\\) or \\(f_X(x)\\) and must satisfy \\(\\int_{-\\infty}^{\\infty} p(x) \\, dx = 1\\).\n\n\n2.1.2.3 Cumulative Distribution Function (CDF)\n\\(F(x) = P(X \\leq x)\\) gives the probability that a random variable is less than or equal to \\(x\\). The CDF is the most fundamental descriptor, defined for all random variables (discrete and continuous). It unifies probability concepts and is essential for quantiles and return periods.\n\n\n2.1.2.4 Quantile function\nThe quantile function \\(Q(p) = F^{-1}(p)\\) is the inverse of the CDF. It takes a probability \\(p \\in [0,1]\\) and returns the value \\(x\\) such that \\(P(X \\leq x) = p\\).\nFor example, the median is \\(Q(0.5)\\). Then 99th percentile is \\(Q(0.99)\\).\n\n\n2.1.2.5 Examples of key distribution functions\nUnderstanding these concepts requires working through concrete examples. The Distribution Examples notebook demonstrates these relationships using normal and Poisson distributions.\nThese examples show both forward operations (finding probabilities from values) and inverse operations (finding values from probabilities), illustrating how the three fundamental functions work together to characterize uncertainty.\n\n\n\n2.1.3 Multiple variables\n\n2.1.3.1 Joint, marginal, and conditional distributions\nReal systems involve multiple random variables, requiring tools to describe their relationships. This machinery allows construction of complex models from simpler components.\nThe joint distribution \\(p(x,y)\\) for continuous or \\(P(X=x, Y=y)\\) for discrete gives the probability of events occurring together.\nThe marginal distribution \\(p(x)\\) or \\(P(X=x)\\) gives the probability of an event, irrespective of other variables. Calculated by summing or integrating over the other variables: \\(p(x) = \\int p(x,y) \\, dy\\).\nThe conditional distribution \\(p(y \\mid x)\\) or \\(P(Y=y \\mid X=x)\\) gives the probability of an event given that another event has occurred. Conditional distributions describe how variables depend on each other.\n\n\n2.1.3.2 Visualizing joint, marginal, and conditional distributions\nUnderstanding the relationships between joint, marginal, and conditional distributions becomes clearer with visualization. The Distribution Examples notebook includes a comprehensive multivariate example showing how joint distributions decompose into marginal and conditional components.\n\n\n2.1.3.3 Independence\nTwo random variables \\(X\\) and \\(Y\\) are independent if their joint distribution is the product of their marginal distributions: \\(p(x,y) = p(x)p(y)\\) for continuous variables, and \\(P(X=x, Y=y) = P(X=x)P(Y=y)\\) for discrete variables. For example, temperature and rainfall on a given day are typically not independent—hot days often have lower rainfall probability.\n\n\n2.1.3.4 IID (Independent and Identically Distributed)\nA sequence of random variables that are independent and have the same distribution. Many statistical models assume IID data points, which enables powerful analytical and computational techniques.\n\n\n2.1.3.5 Bayes’ rule\nThe mechanical relationship between joint, marginal, and conditional distributions:\n\\[p(y \\mid x) = \\frac{p(x \\mid y) p(y)}{p(x)}\\]\nBayes’ rule is a consequence of the definition of conditional probability. It becomes a tool for inference when interpreted probabilistically.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and inference ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#statistical-foundations",
    "href": "chapters/fundamentals/probability-stats.html#statistical-foundations",
    "title": "2  Probability and inference ✏️",
    "section": "2.2 Statistical foundations",
    "text": "2.2 Statistical foundations\nThis section provides the mathematical toolkit that underlies statistical inference. These results justify why statistical methods work and enable practical computation.\n\n2.2.1 Summary statistics\n\n2.2.1.1 Expectation (Expected Value)\nThe expectation is the formal definition of the quantity we approximate with the sample mean in a Monte Carlo simulation. The expectation of a function \\(g(X)\\) is:\n\\[\\mathbb{E}[g(X)] = \\int g(x) p(x) \\, dx\\]\nfor continuous variables, or \\[\\mathbb{E}[g(X)] = \\sum_x g(x) P(X = x)\\]\nfor discrete variables.\n\n\n2.2.1.2 Moments of a distribution\nProbability distributions are completely described by their PDF/PMF and CDF, but we often need summary statistics that capture essential properties.\n\nMean: \\(\\mu = \\mathbb{E}[X]\\) measures central tendency\nVariance: \\(\\sigma^2 = \\mathbb{E}[(X - \\mu)^2]\\) measures spread or scale; standard deviation is \\(\\sigma = \\sqrt{\\sigma^2}\\)\nHigher-order moments: Skewness measures asymmetry, kurtosis measures tail weight\n\nFor certain heavy-tailed distributions, some higher-order moments (or even the variance) may not exist because the defining integrals diverge.\n\n\n\n2.2.2 Fundamental theorems\nTwo fundamental theorems provide the mathematical foundation for both statistical estimation and computational methods. These results justify why statistical methods work and when we can trust their results.\n\n2.2.2.1 Law of Large Numbers\nSubject to mild conditions, the sample mean converges to the expected value as the number of samples increases:\n\\[\\frac{1}{N} \\sum_{i=1}^N X_i \\to \\mathbb{E}[X]\\]\nThis theorem underlies both parameter estimation and Monte Carlo simulation. It guarantees that maximum likelihood estimates become accurate with sufficient data, and that Monte Carlo approximations become precise with enough samples.\n\n\n2.2.2.2 Central Limit Theorem\nThe distribution of a sample mean approaches a Normal distribution as the sample size increases, regardless of the underlying distribution shape:\n\\[\\frac{\\sqrt{N}(\\bar{X} - \\mu)}{\\sigma} \\to N(0,1)\\]\nwhere \\(\\bar{X}\\) is the sample mean, \\(\\mu = \\mathbb{E}[X]\\), and \\(\\sigma^2 = \\text{Var}(X)\\).\nThis enables uncertainty quantification through confidence intervals and justifies the widespread use of normal approximations in statistical inference. The CLT explains why many phenomena follow normal distributions—they arise from sums of many small, independent effects.\n\n\n\n2.2.3 Monte Carlo Expectations\nMost decision-relevant quantities can be expressed as expectations. When analytical calculation is impossible, simulation provides a practical approximation method.\nThe basic Monte Carlo estimate of an expectation is:\n\\[\\mathbb{E}[g(X)] \\approx \\frac{1}{N} \\sum_{i=1}^N g(x_i)\\]\nwhere \\(x_1, x_2, \\ldots, x_N\\) are independent samples from the distribution of \\(X\\). More sophisticated methods (importance sampling, MCMC) exist for drawing samples from more complex distributions.\nThe Law of Large Numbers guarantees convergence, while the Central Limit Theorem provides the convergence rate. The Monte Carlo standard error is approximately \\(\\sigma/\\sqrt{N}\\), where \\(\\sigma\\) is the standard deviation of \\(g(X)\\). This means that to halve the error, we need four times as many samples.\nMonte Carlo methods become essential when dealing with high-dimensional integrals that arise in Bayesian inference and uncertainty propagation through complex models.\n\n\n2.2.4 Transformation of variables\nA core task in probabilistic modeling is understanding how randomness propagates through a system. If we have a random variable \\(X\\) with PDF \\(p_X(x)\\) and create \\(Y = g(X)\\), what is the PDF of \\(Y\\)?\nSimply substituting \\(x = g^{-1}(y)\\) in the original PDF is incorrect because functions can stretch or compress the probability space. We must account for this distortion.\nThe general change of variables formula is:\n\\[p_Y(y) = p_X(g^{-1}(y)) \\left| \\frac{d}{dy} g^{-1}(y) \\right|\\]\nThe term \\(\\left| \\frac{d}{dy} g^{-1}(y) \\right|\\) is the Jacobian—a “stretching factor” ensuring probability mass is conserved. When a function stretches a region, density decreases proportionally to keep total probability equal to 1. When it compresses a region, density increases.\nThis formula derives from working through CDFs and applying the chain rule, but the key insight is that transformations distort the coordinate system and we must adjust densities accordingly.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and inference ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#likelihood-and-maximum-likelihood-estimation",
    "href": "chapters/fundamentals/probability-stats.html#likelihood-and-maximum-likelihood-estimation",
    "title": "2  Probability and inference ✏️",
    "section": "2.3 Likelihood and maximum likelihood estimation",
    "text": "2.3 Likelihood and maximum likelihood estimation\nThe probability theory and statistical foundations we’ve covered provide the mathematical language for uncertainty and the tools for computation. We now turn from describing uncertainty to learning from data. The first major approach to statistical inference connects data to parameters through likelihood functions and optimization.\n\n2.3.1 The likelihood function\nThe central tool for connecting data to parameters is the likelihood function. The likelihood is the conditional probability \\(p(y \\mid \\theta)\\), where \\(y\\) represents our observed data.\n\n2.3.1.1 Definition\nThe likelihood tells us how likely we are to see the observed data \\(y\\) for some value of the parameters \\(\\theta\\).\n\n\n2.3.1.2 Crucial distinction\nThe likelihood is not the probability of the parameters. It’s the probability (or probability density) of the data given the parameters.\nThis confusion is common: \\(p(\\text{data}|\\text{parameters})\\) tells us about data likelihood, not parameter probability. MLE provides point estimates of the most likely parameter values, while Bayesian inference provides probability distributions over parameters. Only Bayesian inference gives us \\(p(\\text{parameters}|\\text{data})\\).\nFor continuous variables, since we’re dealing with a density, the probability of getting exactly that value is zero, but the probability of getting near it is the integral of the PDF over a small interval.\n\n\n2.3.1.3 Independence and the product form\nIf we assume our data points are independent and identically distributed (IID), then by the definition of independence: \\[p(y_1, y_2, \\ldots, y_n \\mid \\theta) = \\prod_{i=1}^n p(y_i \\mid \\theta)\\]\n\n\n2.3.1.4 The log-likelihood\nProducts are numerically unstable and difficult to work with. Since the logarithm is monotonic, \\[\n\\arg \\max_\\theta p(y \\mid \\theta) = \\arg \\max_\\theta \\log p(y \\mid \\theta)\n\\] although \\[\n\\max_\\theta p(y \\mid \\theta) \\neq \\max_\\theta \\log p(y \\mid \\theta)\n\\] in general.\nFor independent data, this gives us: \\[\n\\log p(y \\mid \\theta) = \\sum_{i=1}^n \\log p(y_i \\mid \\theta)\n\\]\n\n\n\n2.3.2 Maximum likelihood estimation\nMaximum likelihood estimation (MLE) finds the parameter values that maximize the likelihood function: \\[\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta p(y \\mid \\theta)\\]\n\n2.3.2.1 Why maximum likelihood makes sense\nThe likelihood function \\(p(y \\mid \\theta)\\) gives the probability of observed data under different parameter values. Maximum likelihood estimation finds the parameter values that maximize the probability of the observed data. The approach selects parameters that best explain the observations.\nIn practical applications, MLE estimates parameters of distributions describing observed phenomena by finding values that maximize the probability of historical observations. The estimates inform subsequent analysis and decision-making.\n\n\n2.3.2.2 Implementation\nWe find the actual parameter values using optimization approaches. This may involve analytical differentiation (setting derivatives to zero) or numerical optimization methods when closed-form solutions don’t exist.\nThis reframes the statistical problem of inference as a numerical problem of optimization.\n\n\n2.3.2.3 Properties and theoretical foundations\nUnderstanding when and why MLE works requires defining estimator quality. An estimator should be consistent (converge to the true value as sample size increases), efficient (achieve low variance), and unbiased (correct on average).\nUnder regularity conditions, MLE estimators have desirable asymptotic properties. As the sample size \\(n\\) grows large, the MLE estimator \\(\\hat{\\theta}_{\\text{MLE}}\\) becomes consistent—it converges to the true parameter value \\(\\theta_0\\).\n\n\n2.3.2.4 Computational considerations\nFinding maximum likelihood estimates requires different approaches depending on the complexity of the model.\nAnalytical solutions exist when we can solve \\[\n\\frac{d}{d\\theta} \\log p(y \\mid \\theta) = 0\n\\] in closed form. This works for simple models like Normal distributions with known variance, or the coin flip example we examine below. These cases provide valuable intuition and serve as building blocks for more complex problems.\nNumerical optimization becomes necessary when no closed-form solution exists. Practical challenges arise in applications. The likelihood surface may contain multiple local maxima, requiring different starting values to find the global optimum. Numerical stability requires working with log-likelihoods rather than products of small probabilities. Flat likelihood surfaces indicate that data contain limited information about parameters. All methods assume correct model specification – poor model approximations yield misleading results regardless of optimization quality.\n\n\n\n2.3.3 Example: coin flip maximum likelihood estimation\nThe coin flip example provides a pedagogically clean introduction to maximum likelihood estimation. The Coin Flip Inference notebook demonstrates both the analytical derivation and computational implementation of MLE.\nThis example shows how MLE connects intuitive parameter estimates (the observed proportion) with formal statistical theory. The same principles apply whether estimating coin bias or the frequency of extreme climate events from historical data.\n\n\n2.3.4 Linear regression example\nLinear regression extends inference to multiple parameters, demonstrating the connections between curve fitting, maximum likelihood estimation, and Bayesian approaches.\nThe Linear Regression Examples notebook shows how the same statistical problem can be approached from three different philosophical perspectives, each providing different insights into parameter estimation and uncertainty quantification.\nThis example illustrates how probabilistic models connect optimization-based and fully Bayesian approaches, with implications for modeling relationships between climate variables and their impacts.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and inference ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#bayesian-inference",
    "href": "chapters/fundamentals/probability-stats.html#bayesian-inference",
    "title": "2  Probability and inference ✏️",
    "section": "2.4 Bayesian inference",
    "text": "2.4 Bayesian inference\nMaximum likelihood estimation provides point estimates of parameters by finding values that maximize the probability of observed data. Bayesian inference takes a fundamentally different approach: it treats parameters as random variables and computes full probability distributions that quantify uncertainty. In other words, rather than searching for \\(\\hat{\\theta}\\) that maximizes the likelihood, Bayesian inference seeks to estimate the entire distribution \\(p(\\theta \\mid y)\\).\n\n2.4.1 Motivation and overview\nReal-world risk assessment relies on multiple, imperfect data sources: short instrumental records, longer regional records, qualitative historical accounts, and physical constraints from models. Traditional statistical methods often struggle to formally integrate these different types of information into a single analysis. The Bayesian framework provides a principled solution by treating all knowledge—both prior beliefs and new data—as probability distributions that can be mathematically combined.\nThe core relationship is Bayes’ rule: \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}\n\\]\nThis deceptively simple equation describes how we update our beliefs:\n\nPrior distribution \\(p(\\theta)\\): Quantifies existing knowledge about parameters before analyzing the current dataset\nLikelihood function \\(p(y \\mid \\theta)\\): The engine for learning from data (identical to the likelihood used in maximum likelihood estimation)\nPosterior distribution \\(p(\\theta \\mid y)\\): Our updated beliefs after combining prior knowledge with observed data\nMarginal likelihood \\(p(y)\\): A normalizing constant ensuring the posterior integrates to 1\n\nSince \\(p(y)\\) doesn’t depend on \\(\\theta\\) for a fixed dataset, we often write: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n\\]\nThe result is fundamentally different from maximum likelihood estimation: instead of a single “best” parameter estimate, we obtain a full probability distribution that naturally quantifies uncertainty. This enables direct probabilistic statements like “there is a 95% probability that the parameter lies between these values.”\n\n\n2.4.2 Maximum A Posteriori: a bridge to optimization\nBefore exploring full Bayesian inference, we can find the single most probable parameter value given the data. Maximum A Posteriori (MAP) estimation finds the mode of the posterior distribution:\n\\[\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta p(\\theta \\mid y) = \\arg\\max_\\theta p(y \\mid \\theta) p(\\theta)\\]\nTaking logarithms (since log is monotonic):\n\\[\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta [\\log p(y \\mid \\theta) + \\log p(\\theta)]\\]\nThis reveals an elegant connection to machine learning and optimization. The log-posterior decomposes into the familiar log-likelihood plus a log-prior term. The log-prior acts as a regularization penalty, preventing overfitting by favoring certain parameter values.\nWhen the prior is uniform (non-informative), the log-prior is constant and MAP reduces to maximum likelihood estimation. When the prior is informative, it regularizes the estimate by pulling it toward prior beliefs. This is mathematically identical to penalized likelihood methods like Ridge regression (with Gaussian priors) or Lasso regression (with Laplace priors).\nHowever, MAP provides only a point estimate and discards uncertainty information. To fully leverage the Bayesian framework, we need the entire posterior distribution.\n\n\n2.4.3 Analytic solutions and conjugate priors\nIn special cases, we can compute the posterior distribution analytically using conjugate priors. A prior is conjugate to a likelihood if the posterior has the same functional form as the prior. This mathematical convenience allows us to update our beliefs with a simple algebraic formula.\nThe coin flip example demonstrates this perfectly. For the Binomial likelihood, the Beta distribution is conjugate. To see why, let’s work through the mathematics.\nStarting with our prior and likelihood:\n\nPrior: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\) with PDF \\(p(\\theta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\)\nLikelihood: \\(y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)\\) with \\(p(y \\mid \\theta) \\propto \\theta^y(1-\\theta)^{n-y}\\)\n\nThe posterior is proportional to the product of prior and likelihood: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\cdot p(\\theta) \\propto \\theta^y(1-\\theta)^{n-y} \\cdot \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\] Combining the powers: \\[\np(\\theta \\mid y) \\propto \\theta^{(\\alpha + y) - 1}(1-\\theta)^{(\\beta + n - y) - 1}\n\\] This is exactly the kernel of a \\(\\text{Beta}(\\alpha + y, \\beta + n - y)\\) distribution! Therefore: \\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\n\\]\nThe posterior parameters are intuitive: prior “successes” (\\(\\alpha\\)) plus observed successes (\\(y\\)), and prior “failures” (\\(\\beta\\)) plus observed failures (\\(n-y\\)). This demonstrates the core Bayesian principle: new data updates our beliefs in a mathematically principled way.\nThe following example shows this updating process in action, demonstrating how the posterior distribution evolves with each new coin flip:\n\n\n\n\n\n\n\n\n\n\n\nAs data accumulate, the influence of the prior diminishes relative to that of the likelihood. With sufficient data, Bayesian and maximum likelihood estimates converge regardless of the prior choice.\nIn practice, conjugate priors exist for only a limited set of models.\n\n\n2.4.4 Markov Chain Monte Carlo\nFor any non-trivial model, analytically computing the posterior distribution becomes mathematically intractable. A brute-force approach of evaluating the posterior on a grid fails catastrophically: with \\(k\\) parameters and \\(n\\) grid points per parameter, we need \\(n^k\\) evaluations. For even modest problems (say, 10 parameters with 100 grid points each), this requires \\(100^{10} = 10^{20}\\) calculations—computationally impossible.\nThis “curse of dimensionality” means that analytical approaches work only for the simplest models. Real scientific applications require computational methods.\n\n2.4.4.1 Example: A Metropolis-Hastings Sampler from Scratch\nModern MCMC algorithms differ in how they propose new parameter values. A classical, foundational approach is the Metropolis-Hastings algorithm, which gives us a powerful intuition for how MCMC works.\nThe algorithm allows us to sample from a target distribution that we can’t directly sample from, as long as we can evaluate its density up to a normalizing constant. We use a separate proposal distribution to suggest new parameter values, and then we probabilistically accept or reject these proposals based on how well they align with the target.\n(Shout out to Danielle Navarro for a clear explanation and the working example that inspired this section).\nThe algorithm proceeds as follows:\n\nStart with an initial parameter value \\(\\theta^{(0)}\\).\nFor each iteration \\(i\\):\n\nPropose a new value \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* \\mid \\theta^{(i-1)})\\).\nCompute the acceptance ratio: \\[\nr = \\frac{p(\\theta^* \\mid y) \\, q(\\theta^{(i-1)} \\mid \\theta^*)}{p(\\theta^{(i-1)} \\mid y) \\, q(\\theta^* \\mid \\theta^{(i-1)})}\n\\]\nAccept the proposal with probability \\(\\min(1, r)\\):\n\nIf a random number is less than \\(r\\), set \\(\\theta^{(i)} = \\theta^*\\).\nOtherwise, reject the proposal and set \\(\\theta^{(i)} = \\theta^{(i-1)}\\).\n\n\nRepeat for many iterations to generate a chain of samples that approximates the posterior distribution \\(p(\\theta \\mid y)\\).\n\n\nusing CairoMakie\nusing LaTeXStrings\nusing Distributions\n\n\n2.4.4.1.1 Step 1: Target Distribution\nFirst, let’s define the distribution we want to sample from. To make it interesting, we’ll use a “squiggly” function that isn’t a standard probability distribution. This is our unnormalized target density, which corresponds to \\(p(\\theta^* \\mid y)\\) in the ratio above.\n\n# Define the target distribution (unnormalized)\ntarget(x) = exp(-x^2 / 2) * (2 + sin(5 * x) + sin(2 * x))\n\n# Visualize what it looks like\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel = L\"θ\",\n    ylabel = L\"p(θ | y) \\propto \\text{Target Density}\")\nlines!(ax, -4:0.01:4, target, linewidth = 3)\nfig\n\n\n\n\n\n\n\n\nOur goal is to generate samples whose histogram matches the shape of this distribution.\n\n\n2.4.4.1.2 Step 2: Proposal Distribution\nNext, we need a way to propose new steps. For this example, we’ll use a Normal distribution centered on our current position. This is like telling our sampler to take a random “hop” left or right.\nThis choice is purely for didactic purposes. A Normal distribution is symmetric, meaning the probability of hopping from \\(\\theta_A\\) to \\(\\theta_B\\) is the same as hopping from \\(\\theta_B\\) to \\(\\theta_A\\). This simplifies the acceptance ratio, as the proposal terms \\(q(\\theta^{(i-1)} \\mid \\theta^*)\\) and \\(q(\\theta^* \\mid \\theta^{(i-1)})\\) cancel out, leaving us with the simpler Metropolis algorithm.\n\nproposal(μ, σ) = Normal(μ, σ)\n\nThe standard deviation, σ, controls the “hop size.” A larger σ will propose more distant jumps, while a smaller σ will explore the local area more finely.\n\n\n2.4.4.1.3 Step 3: Sampler\nNow we can write the core function that implements the Metropolis-Hastings logic. The function will take the number of samples, a starting value, and the proposal hop size as inputs. It will loop through the algorithm, building the chain one sample at a time.\n\nfunction metropolis_hastings_sampler(n_samples, start_val, proposal_sd)\n    # 1. Initialize an array to store the chain\n    chain = zeros(n_samples)\n    chain[1] = start_val\n    n_accepted = 0\n\n    # 2. Loop from the second sample to the end\n    for i in 2:n_samples\n        current_x = chain[i-1]\n\n        # 3. Propose a new value\n        proposed_x = rand(proposal(current_x, proposal_sd))\n\n        # 4. Calculate acceptance ratio (r). \n        # Since the proposal is symmetric, the q terms cancel.\n        r = target(proposed_x) / target(current_x)\n\n        # 5. Accept or reject the proposal\n        if rand() &lt; r\n            chain[i] = proposed_x # Accept\n            n_accepted += 1\n        else\n            chain[i] = current_x # Reject (stay put)\n        end\n    end\n\n    return chain, n_accepted / n_samples\nend\n\n\n\n2.4.4.1.4 Step 4: Implement\nFinally, we run the sampler and plot the results. We need to check two things:\n\nThe Trace Plot: This shows the value of the parameter at each iteration. We are looking for a “fuzzy caterpillar” pattern, which indicates the chain is exploring the distribution well and is stationary.\nThe Histogram: This shows the distribution of the collected samples. If the sampler worked, its shape should match our target density.\n\n\n# Run the sampler\nn_samples = 20000\nburn_in = 2000 # Number of initial samples to discard\nchain, acceptance_rate = metropolis_hastings_sampler(n_samples, 0.0, 1.0)\n\nprintln(\"Acceptance rate: \", round(acceptance_rate, digits = 3))\n\n# Visualize the results\nfig = Figure(size = (800, 600))\n\n# 1. Trace Plot\nax1 = Axis(fig[1, 1],\n    title = \"MCMC Trace Plot\",\n    xlabel = \"Iteration\",\n    ylabel = L\"θ\")\nlines!(ax1, chain, label = \"Trace\")\n\n# 2. Histogram of Samples\nax2 = Axis(fig[2, 1],\n    title = \"Posterior Approximation\",\n    xlabel = L\"θ\",\n    ylabel = \"Density\")\n\n# Plot the histogram, removing the initial \"burn-in\" period\nhist!(ax2, chain[burn_in+1:end],\n    bins = 60,\n    normalization = :pdf,\n    label = \"MCMC Samples\")\n\n# Overlay the true target density (scaled to be a valid PDF)\nx_grid = -4:0.01:4\ntarget_pdf = target.(x_grid)\ntarget_pdf ./= sum(target_pdf .* 0.01) # Normalize\nlines!(ax2, x_grid, target_pdf,\n    label = \"Target Density\",\n    linewidth = 3,\n    color = :purple)\n\n# Add a legend to the second plot\naxislegend(ax2)\n\nfig\n\nAcceptance rate: 0.594\n\n\n\n\n\nResults from our Metropolis-Hastings sampler. The trace plot (top) shows the sampler exploring the parameter space. The histogram of the MCMC samples (bottom) closely matches the shape of the unnormalized target density.\n\n\n\n\nThe acceptance rate is a useful diagnostic. A very high rate (&gt; 0.8) might mean the hop size is too small, and the chain isn’t exploring efficiently. A very low rate (&lt; 0.1) might mean the hop size is too large, and most proposals are being rejected. Rates between 0.2 and 0.5 are often considered good for this type of sampler.\nAs we can see, the final histogram of samples beautifully recovers the shape of the target distribution, demonstrating that this simple algorithm successfully turned a function we can evaluate into a distribution we can sample from.\n\n\n\n2.4.4.2 Modern MCMC algorithms\nIn practice, Metropolis-Hastings is rarely used due to its inefficiency and because it scales poorly to high-dimensional spaces where the acceptance rate can become very low. Modern MCMC algorithms, such as Hamiltonian Monte Carlo (HMC) and the No-U-Turn Sampler (NUTS), leverage gradient information to propose more informed jumps in parameter space. These methods explore the posterior distribution more efficiently, especially in high-dimensional problems common in climate modeling and other scientific applications.\nWHen using these advanced samplers, several best practices ensure robust and reliable inference:\n\nRunning multiple independent chains helps assess convergence and identify multimodal posteriors. You will often see four chains as a default, but that’s mainly because laptops ten years ago mainly had four cores. More is often better.\nInitial iterations allow the sampler to adapt to the posterior geometry and find the typical set. We typically discard these “warm-up” samples before final inference.\nMost samplers have hyperparameters (e.g., step size, number of leapfrog steps) that require tuning for optimal performance. Many software packages include automatic tuning during a warm-up phase.\nDiagnostics are essential to ensure the sampler has converged and is exploring the posterior adequately.\n\nTrace plots show the parameter values over iterations. We are looking for a “fuzzy caterpillar” pattern, which indicates the chain is exploring the distribution well and is stationary. If you see trends or drifts, the chain has not converged.\nTrace plots for multiple chains should overlap and mix well, indicating they are sampling from the same distribution.\nAutocorrelation plots help assess the degree of correlation between samples. Rapidly decreasing autocorrelation suggests good mixing, while high autocorrelation indicates the chain is stuck in local modes.\nQuantitative metrics, such as the \\(\\hat{R}\\) statistic and effective sample size, provide numerical assessments of convergence and sampling efficiency.\n\n\nIn all cases, we can rule out some kinds of convergence problems, but actually proving convergence is impossible, so we must always include other checks. Avoid the tendency common in applied work to state without reflection or critique that because \\(\\hat{R} &lt; 1.1\\) no further thought is needed. For tricky problems, work with a computational statistician (or take Bayesian stats courses beyond the scope of these lecture notes).\n\n\n2.4.4.3 From samples to inference\nAfter running the chain for thousands of iterations, the resulting samples serve as a high-fidelity numerical approximation of the true posterior distribution. We can use these samples to approximate any posterior quantity: \\[\n\\mathbb{E}[g(\\theta) \\mid y] \\approx \\frac{1}{N} \\sum_{i=1}^N g(\\theta^{(i)})\n\\] where \\(\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(N)}\\) are our MCMC samples after discarding warm-up iterations.\nThis Monte Carlo approximation enables: - Parameter estimates and credible intervals - Posterior predictive distributions for forecasting - Model comparison through marginal likelihood estimation - Decision analysis under parameter uncertainty\nThe quality of these approximations depends on sample size, chain mixing, and the specific posterior quantity being estimated. Tail probabilities and extreme quantiles require more samples than central tendencies.\n\n\n\n2.4.5 Example: coin flip Bayesian analysis\nThe coin flip example demonstrates the complete Bayesian workflow, from prior specification through posterior computation and interpretation.\nThe Coin Flip Inference notebook shows how treating parameters as random variables enables full uncertainty quantification. This example compares analytical solutions (using conjugate priors) with computational approaches, validating MCMC methods against known exact results.\nThe example illustrates key Bayesian concepts: how priors influence posterior distributions, the role of conjugacy in analytical tractability, and the use of MCMC when analytical solutions don’t exist.\n\n\n2.4.6 Example: linear regression Bayesian analysis\nLinear regression demonstrates Bayesian inference for multivariate problems requiring computational methods.\nThe Linear Regression Examples notebook provides a comprehensive comparison of least squares, maximum likelihood, and Bayesian approaches to the same problem. This comparison illuminates the philosophical differences between methods and their practical implications for uncertainty quantification.\nThe Bayesian approach excels when parameter uncertainty must be propagated through subsequent calculations or decision processes—a crucial capability for climate risk assessment and decision-making under uncertainty.\n\n\n2.4.7 Posterior predictive distribution and model checking\nThe Bayesian workflow extends beyond parameter estimation to include systematic model validation through posterior predictive checking. This approach uses the fitted model to generate simulated data, then compares these simulations to observed data on relevant metrics.\nThe posterior predictive distribution represents our beliefs about new data given what we’ve observed: \\[p(y^{\\text{rep}} | y^{\\text{obs}}) = \\int p(y^{\\text{rep}} | \\theta) p(\\theta | y^{\\text{obs}}) d\\theta\\]\nwhere \\(y^{\\text{rep}}\\) denotes replicated (simulated) data and \\(y^{\\text{obs}}\\) denotes observed data.\n\n2.4.7.1 The posterior predictive checking workflow\nThe systematic approach follows these steps:\n\nUse MCMC to obtain samples from \\(p(\\theta | y^{\\text{obs}})\\)\nFor each posterior sample \\(\\theta^{(i)}\\), draw \\(y^{\\text{rep}(i)} \\sim p(y | \\theta^{(i)})\\)\nCalculate relevant “test statistics” or metrics \\(T(y^{\\text{rep}(i)})\\) and \\(T(y^{\\text{obs}})\\)\nAssess whether \\(T(y^{\\text{obs}})\\) falls within the distribution of \\(T(y^{\\text{rep}})\\)\n\nThis is related to the frequentist idea of p-values explored in the mosquitos case study.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and inference ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/probability-stats.html#further-reading",
    "href": "chapters/fundamentals/probability-stats.html#further-reading",
    "title": "2  Probability and inference ✏️",
    "section": "Further reading",
    "text": "Further reading\nFor deeper study of probability and statistics:\n\nBlitzstein and Hwang (2019) provides excellent intuition with computational examples\nDowney (2021) emphasizes Bayesian thinking with practical applications\nGelman (2021) connects regression to broader statistical modeling\nGelman et al. (2014) comprehensive treatment of Bayesian computation\n\nThis chapter’s concepts are also demonstrated through focused computational notebooks:\n\nDistribution Examples: Understanding probability distributions and their relationships\nCoin Flip Inference: Introduction to statistical inference with likelihood and Bayesian methods\nLinear Regression Examples: Comparing curve fitting, MLE, and Bayesian approaches\n\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability, Second Edition. 2nd Edition. Boca Raton: Chapman and Hall/CRC. http://probabilitybook.net.\n\n\nDowney, Allen B. 2021. Think Bayes. \"O’Reilly Media, Inc.\". https://allendowney.github.io/ThinkBayes2/.\n\n\nGelman, Andrew. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge, United Kingdom ; Cambridge University Press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 2014. Bayesian Data Analysis. 3rd ed. Chapman & Hall/CRC Boca Raton, FL, USA.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability and inference ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html",
    "href": "chapters/fundamentals/ml-nonparametric.html",
    "title": "3  Machine learning and nonparametric methods 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine learning and nonparametric methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#learning-objectives",
    "href": "chapters/fundamentals/ml-nonparametric.html#learning-objectives",
    "title": "3  Machine learning and nonparametric methods 🚧",
    "section": "",
    "text": "Apply fundamental supervised learning concepts to climate hazard assessment problems\nUnderstand nonparametric and semiparametric methods for flexible modeling\nCritically evaluate machine learning applications in climate risk literature\nUnderstand bias-variance tradeoffs and model validation approaches",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine learning and nonparametric methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#essential-machine-learning-concepts",
    "href": "chapters/fundamentals/ml-nonparametric.html#essential-machine-learning-concepts",
    "title": "3  Machine learning and nonparametric methods 🚧",
    "section": "3.1 Essential machine learning concepts",
    "text": "3.1 Essential machine learning concepts\n\nSupervised and unsupervised learning paradigms\nParametric vs nonparametric methods\nBias-variance tradeoff and regularization\nCross-validation and model selection\nTree-based methods and ensemble learning",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine learning and nonparametric methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/ml-nonparametric.html#further-reading",
    "href": "chapters/fundamentals/ml-nonparametric.html#further-reading",
    "title": "3  Machine learning and nonparametric methods 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Machine learning and nonparametric methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html",
    "href": "chapters/fundamentals/correlation-dimensionality.html",
    "title": "4  Correlation and dimensionality 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation and dimensionality 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#learning-objectives",
    "href": "chapters/fundamentals/correlation-dimensionality.html#learning-objectives",
    "title": "4  Correlation and dimensionality 🚧",
    "section": "",
    "text": "Model and interpret spatial dependence in climate fields\nApply time series analysis methods to detect trends and patterns in climate data\nUse dimension reduction techniques for high-dimensional climate datasets\nIntegrate spatial and temporal methods for spatiotemporal climate analysis",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation and dimensionality 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#essential-concepts",
    "href": "chapters/fundamentals/correlation-dimensionality.html#essential-concepts",
    "title": "4  Correlation and dimensionality 🚧",
    "section": "4.1 Essential concepts",
    "text": "4.1 Essential concepts\n\nSpatial statistics and geostatistical methods\nTime series analysis and trend detection\nPrincipal component analysis and empirical orthogonal functions\nHigh-dimensional methods for climate data\nSpatiotemporal integration approaches",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation and dimensionality 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/correlation-dimensionality.html#further-reading",
    "href": "chapters/fundamentals/correlation-dimensionality.html#further-reading",
    "title": "4  Correlation and dimensionality 🚧",
    "section": "Further reading",
    "text": "Further reading\nFor spatial and temporal analysis in climate science:\n\nCressie and Wikle (2011): Comprehensive treatment of spatial statistics\n\n\n\n\n\nCressie, Noel A. C., and Christopher K. Wikle. 2011. Statistics for Spatio-Temporal Data. Hoboken, N.J.: Wiley.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlation and dimensionality 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html",
    "href": "chapters/fundamentals/model-comparison.html",
    "title": "5  Model validation and comparison 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model validation and comparison 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#learning-objectives",
    "href": "chapters/fundamentals/model-comparison.html#learning-objectives",
    "title": "5  Model validation and comparison 🚧",
    "section": "",
    "text": "Apply graphical diagnostic methods to assess model fit quality\nUse information criteria (AIC, DIC, BIC) for quantitative model comparison\nUnderstand the relationship between model selection and predictive accuracy\nRecognize the subjective nature of model selection and make transparent choices",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model validation and comparison 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#essential-model-validation-concepts",
    "href": "chapters/fundamentals/model-comparison.html#essential-model-validation-concepts",
    "title": "5  Model validation and comparison 🚧",
    "section": "5.1 Essential model validation concepts",
    "text": "5.1 Essential model validation concepts\n\nGraphical diagnostic methods and model checking\nInformation criteria for model comparison\nPredictive accuracy and cross-validation\nModel selection philosophy and transparency\nEnsemble methods and model averaging",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model validation and comparison 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/fundamentals/model-comparison.html#further-reading",
    "href": "chapters/fundamentals/model-comparison.html#further-reading",
    "title": "5  Model validation and comparison 🚧",
    "section": "Further reading",
    "text": "Further reading\n\nPiironen and Vehtari (2017): Technical treatment of predictive accuracy\n\n\n\n\n\nPiironen, Juho, and Aki Vehtari. 2017. “Comparison of Bayesian Predictive Methods for Model Selection.” Statistics and Computing 27 (3): 711–35. https://doi.org/10.1007/s11222-016-9649-y.",
    "crumbs": [
      "**I: Foundations**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model validation and comparison 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html",
    "href": "chapters/hazard/extremes.html",
    "title": "6  Extreme value theory ✏️",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from Probability and Statistics.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#motivation",
    "href": "chapters/hazard/extremes.html#motivation",
    "title": "6  Extreme value theory ✏️",
    "section": "6.1 Motivation",
    "text": "6.1 Motivation\nThe design of reliable infrastructure, such as stormwater systems in Harris County, Texas, depends on a quantitative understanding of extreme environmental events. Engineers must characterize the magnitude and frequency of rare phenomena. For example, they might need to determine the expected recurrence interval of a daily rainfall total that exceeds a critical design threshold, such as 300 mm.\nAn initial, intuitive approach is to analyze the historical record.\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of over 70 years of daily precipitation data from a station in Houston reveals 13 days where rainfall exceeded 150 mm. This allows for a simple empirical estimate of the event’s frequency. Thirteen exceedances in 71 years suggests an average recurrence interval of approximately 5.5 years. For the 300 mm threshold, however, the historical record contains zero events. An empirical estimate of the probability is therefore zero, which is uninformative for risk assessment and infrastructure design. We require a method for extrapolating beyond the range of observed data.\nA natural extension would be to fit a standard parametric probability distribution—such as a Gamma distribution, which is commonly used for precipitation—to the entire set of daily rainfall observations. One could then use the extreme upper tail of the fitted distribution to estimate the probability of exceeding 300 mm.\nThis approach, however, is fundamentally unreliable. The goodness-of-fit of a parametric model is driven by its ability to describe the bulk of the data, where observations are plentiful. Minor model misspecifications in this high-density region can translate into substantial errors in the far tails of the distribution, where data are sparse or nonexistent. As stated by Coles (2001), the foundational text on this topic, “very small discrepancies in the estimate of \\(F\\) can lead to substantial discrepancies for \\(F^n\\)”, where in our example \\(F\\) is the cumulative distribution function (CDF) of daily rainfall while \\(F^n\\) is the CDF of the maximum daily rainfall over \\(n\\) days.\nThe models that best describe the central tendency of a process are not necessarily suitable for describing its extremes. This fragility of tail extrapolation necessitates a theoretical framework developed specifically for modeling the behavior of extreme values.\nA naive empirical estimator for the exceedance probability of a value \\(x\\) based on \\(n\\) observations \\(X_i\\) is the simple proportion of exceedances:\n\\[\n\\hat{P}(X &gt; x) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{1}(X_i &gt; x)\n\\]\nwhere \\(\\mathbf{1}(\\cdot)\\) is the indicator function. This is useful when the number of exceedances is large, but fails for extrapolation when the sum is zero, as was the case for the 300mm threshold. This mathematically demonstrates the limitations of simple empirical methods and motivates the specialized approaches that follow.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#approaches-for-modeling-extreme-values",
    "href": "chapters/hazard/extremes.html#approaches-for-modeling-extreme-values",
    "title": "6  Extreme value theory ✏️",
    "section": "6.2 Approaches for modeling extreme values",
    "text": "6.2 Approaches for modeling extreme values\nExtreme Value Theory (EVT) provides such a framework. Rather than modeling the entire distribution of a process, EVT focuses on the distribution of its extreme values. There are two primary methods for extracting these values from a time series.\n\n6.2.1 Block maxima\nIn this method, the period of observation is divided into non-overlapping blocks of equal duration (e.g., years). For each block of size \\(n\\), we define the block maximum as \\(M_n = \\max\\{X_1, \\dots, X_n\\}\\). This yields a new time series of block maxima. A crucial conceptual point is that we are not studying a single maximum possible value of the process. Instead, we are studying the distribution of the sample maximum, \\(M_n\\), which is itself a random variable. This is why a distribution like the GEV is needed to describe its behavior. This approach is intuitive and directly relates to concepts of annual risk, but it can be inefficient as it discards other potentially extreme events within a block.\n\n\n6.2.2 Peaks-over-threshold\nThe peaks-over-threshold (POT) method defines a set of exceedances over a high threshold \\(u\\) as \\(\\{X_i : X_i &gt; u\\}\\). The variable of interest is the value of the excesses themselves, \\(Y = X_i - u\\), for all \\(X_i\\) in the set of exceedances. The GPD is a model for these excess amounts, not the raw values. This approach is more data-efficient than the block maxima method. Its primary challenge lies in selecting an appropriate threshold, which involves a trade-off between bias (if the threshold is too low) and variance (if the threshold is too high, yielding too few data points). In practice, exceedances may occur in clusters (e.g., during a multi-day storm event). Therefore, a declustering algorithm is often applied to the raw exceedances to ensure that the events being modeled are approximately independent.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#asymptotic-theory-for-extremes",
    "href": "chapters/hazard/extremes.html#asymptotic-theory-for-extremes",
    "title": "6  Extreme value theory ✏️",
    "section": "6.3 Asymptotic theory for extremes",
    "text": "6.3 Asymptotic theory for extremes\nThe statistical justification for both the block maxima and POT approaches comes from asymptotic theorems that describe the limiting distributions of extreme values.\n\n6.3.1 Theory for block maxima: The GEV distribution\nThe theoretical justification for the block maxima approach is the Extremal Types Theorem. This theorem states that for a large class of underlying distributions, if a stable, non-degenerate limiting distribution for the block maximum \\(M_n\\) exists, it must be the Generalized Extreme Value (GEV) distribution.\nA critical point is that the theorem applies to a linearly rescaled or normalized maximum. Consider the raw maximum, \\(M_n\\). As the block size \\(n\\) increases, the expected value of \\(M_n\\) will also increase (or stay the same)—its distribution drifts towards larger values and does not converge. A similar issue arises in the Central Limit Theorem (CLT), which describes the convergence of a sample mean. The CLT does not apply to the raw sum of random variables, but to a normalized version of it.\nThe Extremal Types Theorem is the direct analog of the CLT for maxima. It states that there exist sequences of normalizing constants, a location parameter \\(b_n\\) and a scale parameter \\(a_n &gt; 0\\), such that the distribution of the normalized maximum, \\((M_n - b_n)/a_n\\), converges to the GEV distribution as \\(n \\to \\infty\\).\nIn practice, we do not need to know the specific functional forms of \\(a_n\\) and \\(b_n\\). Instead, for a fixed, large block size \\(n\\) (e.g., one year of daily data), we fit the GEV distribution directly to the series of block maxima. The GEV’s location and scale parameters, \\(\\mu\\) and \\(\\sigma\\), effectively absorb and account for the normalization that the theory requires. The GEV is a flexible three-parameter family with a cumulative distribution function (CDF) given by:\n\\[\nG(z) = \\exp\\left\\{ -\\left[ 1 + \\xi \\left( \\frac{z - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi} \\right\\}\n\\]\nThis is defined on the set \\(\\{z: 1 + \\xi(z - \\mu)/\\sigma &gt; 0\\}\\). The parameters are location (\\(\\mu\\)), scale (\\(\\sigma &gt; 0\\)), and shape (\\(\\xi\\)). These parameters implicitly depend on the block size \\(n\\).\n\n\n6.3.2 Theory for threshold exceedances: The GPD\nThe corresponding theory for the POT approach is the Pickands–Balkema–de Haan Theorem. It states that for a wide range of distributions, the distribution of excesses over a high threshold \\(u\\) can be approximated by the Generalized Pareto Distribution (GPD). Conceptually, if the GEV describes the behavior of the maximum of a large block, the GPD describes the behavior of the distribution’s tail that produced that maximum. It is the distribution one would expect to see by “zooming in” on the tail of a distribution above a high threshold.\nThe GPD is a two-parameter family with a CDF for excesses \\(Y = X - u\\) given by:\n\\[\nH(y) = 1 - \\left( 1 + \\frac{\\xi y}{\\sigma_u} \\right)^{-1/\\xi}\n\\]\nThis is defined on \\(\\{y: y &gt; 0 \\text{ and } (1 + \\xi y / \\sigma_u) &gt; 0\\}\\). The parameters are the shape, \\(\\xi\\), and a scale parameter \\(\\sigma_u\\) that depends on the threshold \\(u\\).\nIf the parent distribution’s maxima are GEV-distributed with parameters \\((\\mu, \\sigma, \\xi)\\), then the GPD scale parameter is given by \\(\\sigma_u = \\sigma + \\xi(u - \\mu)\\). This key result shows that the GPD scale parameter is a linear function of the threshold \\(u\\), a property that is used in advanced diagnostics for threshold selection, and illustrates the links between the GPD and GEV models for extremes.\n\n\n6.3.3 The shape parameter\nThe shape parameter, \\(\\xi\\), is the most critical parameter in extreme value analysis and has the same interpretation in both the GEV and GPD. It governs the tail behavior of the distribution.\n\n\\(\\xi = 0\\) (Gumbel-type tail): The tail decays exponentially (light tail).\n\\(\\xi &gt; 0\\) (Fréchet-type tail): The tail decays as a polynomial (heavy tail), with no upper bound.\n\\(\\xi &lt; 0\\) (Weibull-type tail): The distribution has a finite upper bound.\n\n\n\n6.3.4 Connection between GEV and GPD\nThe GEV and GPD models are intrinsically linked. If the block maxima of a process follow a GEV distribution with parameters \\((\\mu, \\sigma, \\xi)\\), then for a high threshold \\(u\\), the threshold excesses follow a GPD with the same shape parameter \\(\\xi\\). This provides a theoretical consistency between the two primary modeling approaches.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#return-periods-and-return-levels",
    "href": "chapters/hazard/extremes.html#return-periods-and-return-levels",
    "title": "6  Extreme value theory ✏️",
    "section": "6.4 Return periods and return levels",
    "text": "6.4 Return periods and return levels\n\n6.4.1 Definitions and Calculation\nThe output of an extreme value analysis is typically expressed in terms of return periods and return levels. The N-year return level, \\(z_N\\), is the level expected to be exceeded on average once every \\(N\\) years. It corresponds to the quantile of the distribution with an annual exceedance probability of \\(p = 1/N\\).\nFor the GEV distribution, it is calculated by inverting the CDF:\n\\[\nz_N = \\mu - \\frac{\\sigma}{\\xi} \\left[ 1 - (-\\ln(1-p))^{-\\xi} \\right]\n\\]\nCalculating return levels from a GPD model requires an additional step. If \\(\\zeta_u\\) is the rate of threshold exceedances (e.g., the average number of exceedances per year), the \\(N\\)-year return level is the value \\(z_N\\) that is exceeded with probability \\(1/(N \\zeta_u)\\). It is calculated by adding the threshold back to the corresponding quantile of the GPD:\n\\[\nz_N = u + \\frac{\\sigma_u}{\\xi} \\left[ (N \\zeta_u)^\\xi - 1 \\right]\n\\]\n\n\n6.4.2 Return level plots\nA standard diagnostic and visualization tool is the return level plot. This plot graphs the estimated return level \\(z_N\\) against the return period \\(N\\), with \\(N\\) typically plotted on a logarithmic scale. The curvature of the fitted line is a direct visualization of the shape parameter, \\(\\xi\\): a straight line implies \\(\\xi=0\\), a concave curve implies \\(\\xi&gt;0\\), and a convex curve implies \\(\\xi&lt;0\\).\n[Placeholder for a Return Level Plot showing a fitted GEV or GPD curve with confidence intervals.]",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#inference",
    "href": "chapters/hazard/extremes.html#inference",
    "title": "6  Extreme value theory ✏️",
    "section": "6.5 Inference",
    "text": "6.5 Inference\n\n6.5.1 Plotting Positions\nTo visually assess model fit, the fitted model is plotted against the observed maxima. For this purpose, we require a method to assign a non-exceedance probability (and thus a return period) to each observed maximum. For a sample of \\(n\\) block maxima, let \\(z_{(1)} &lt; z_{(2)} &lt; \\dots &lt; z_{(n)}\\) be the ordered values. We estimate the probability \\(P(Z \\le z_{(k)})\\) using a plotting position formula. These formulas generally take the form:\n\\[\np_k = \\frac{k - a}{n + 1 - 2a}\n\\]\nwhere \\(k\\) is the rank of the observation and \\(a\\) is a parameter. Common choices include:\n\nWeibull: \\(a=0\\), giving \\(p_k = k/(n+1)\\).\nGringorten: \\(a=0.44\\), giving \\(p_k = (k-0.44)/(n+0.12)\\). This is often recommended as an unbiased choice for Gumbel-type distributions.\n\nThe empirical return period for the \\(k\\)-th observation is then estimated as \\(1/(1-p_k)\\).\n\n\n6.5.2 Moments\n\n\n6.5.3 MLE\n\n\n6.5.4 Bayesian",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#sampling-variability",
    "href": "chapters/hazard/extremes.html#sampling-variability",
    "title": "6  Extreme value theory ✏️",
    "section": "6.6 Sampling variability",
    "text": "6.6 Sampling variability",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#regionalization",
    "href": "chapters/hazard/extremes.html#regionalization",
    "title": "6  Extreme value theory ✏️",
    "section": "6.7 Regionalization",
    "text": "6.7 Regionalization",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#nonstationarity",
    "href": "chapters/hazard/extremes.html#nonstationarity",
    "title": "6  Extreme value theory ✏️",
    "section": "6.8 Nonstationarity",
    "text": "6.8 Nonstationarity",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/extremes.html#further-reading",
    "href": "chapters/hazard/extremes.html#further-reading",
    "title": "6  Extreme value theory ✏️",
    "section": "Further reading",
    "text": "Further reading\n\n(Coles 2001): Canonical extreme value textbook with mathematical rigor and practical examples\n\n\n\n\n\nColes, Stuart. 2001. An Introduction to Statistical Modeling of Extreme Values. Springer Series in Statistics. London: Springer.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extreme value theory ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html",
    "href": "chapters/hazard/downscaling-bias-correction.html",
    "title": "7  Downscaling and Bias Correction 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Downscaling and Bias Correction 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#learning-objectives",
    "href": "chapters/hazard/downscaling-bias-correction.html#learning-objectives",
    "title": "7  Downscaling and Bias Correction 🚧",
    "section": "",
    "text": "Distinguish between supervised and distributional downscaling approaches\nUnderstand the motivation for downscaling climate model outputs\nApply bias correction and quantile-quantile mapping techniques\nRecognize the stationarity assumption and its implications\nEvaluate different downscaling methods for specific applications\nUnderstand modern machine learning approaches to climate downscaling",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Downscaling and Bias Correction 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/downscaling-bias-correction.html#further-reading",
    "href": "chapters/hazard/downscaling-bias-correction.html#further-reading",
    "title": "7  Downscaling and Bias Correction 🚧",
    "section": "Further reading",
    "text": "Further reading\n\nLanzante et al. (2018) for comprehensive review of downscaling challenges\nLafferty and Sriver (2023)\nFarnham, Doss-Gollin, and Lall (2018)\n\n\n\n\n\nFarnham, David J, James Doss-Gollin, and Upmanu Lall. 2018. “Regional Extreme Precipitation Events: Robust Inference from Credibly Simulated GCM Variables.” Water Resources Research 54 (6). https://doi.org/10.1002/2017wr021318.\n\n\nLafferty, David C., and Ryan L. Sriver. 2023. “Downscaling and Bias-Correction Contribute Considerable Uncertainty to Local Climate Projections in CMIP6.” Npj Climate and Atmospheric Science 6 (1, 1): 1–13. https://doi.org/10.1038/s41612-023-00486-0.\n\n\nLanzante, John R, Keith W Dixon, Mary Jo Nath, Carolyn E Whitlock, and Dennis Adams-Smith. 2018. “Some Pitfalls in Statistical Downscaling of Future Climate.” Bulletin of the American Meteorological Society 99 (4): 791–803. https://doi.org/10.1175/bams-d-17-0046.1.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Downscaling and Bias Correction 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html",
    "href": "chapters/hazard/generators.html",
    "title": "8  Stochastic weather generators 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Stochastic weather generators 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html#learning-objectives",
    "href": "chapters/hazard/generators.html#learning-objectives",
    "title": "8  Stochastic weather generators 🚧",
    "section": "",
    "text": "Understand the principles of stochastic weather generation\nApply statistical models for synthetic weather data\nUse weather generators for downscaling climate model output\nEvaluate the performance of weather generation models",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Stochastic weather generators 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html#essential-concepts",
    "href": "chapters/hazard/generators.html#essential-concepts",
    "title": "8  Stochastic weather generators 🚧",
    "section": "8.1 Essential concepts",
    "text": "8.1 Essential concepts\n\nHidden Markov models for weather state modeling\nMulti-site and multi-variable generation\nStatistical downscaling applications\nSynthetic data validation and evaluation\nIntegration with physical models",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Stochastic weather generators 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/generators.html#further-reading",
    "href": "chapters/hazard/generators.html#further-reading",
    "title": "8  Stochastic weather generators 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Stochastic weather generators 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html",
    "href": "chapters/hazard/physics-models.html",
    "title": "9  Physics-based models and calibration 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Physics-based models and calibration 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html#learning-objectives",
    "href": "chapters/hazard/physics-models.html#learning-objectives",
    "title": "9  Physics-based models and calibration 🚧",
    "section": "",
    "text": "Navigate trade-offs between model complexity, interpretability, and computational cost\nCharacterize and communicate within- and between-model uncertainty\nUse surrogate models to approximate complex model output\nApply model calibration techniques for climate applications",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Physics-based models and calibration 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html#essential-concepts",
    "href": "chapters/hazard/physics-models.html#essential-concepts",
    "title": "9  Physics-based models and calibration 🚧",
    "section": "9.1 Essential concepts",
    "text": "9.1 Essential concepts\n\nPhysics-based vs data-driven modeling spectrum\nModel chaining and uncertainty propagation\nCalibration methods and parameter estimation\nSurrogate modeling for computational efficiency\nModel structure uncertainty quantification",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Physics-based models and calibration 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/physics-models.html#further-reading",
    "href": "chapters/hazard/physics-models.html#further-reading",
    "title": "9  Physics-based models and calibration 🚧",
    "section": "Further reading",
    "text": "Further reading\nFor physics-based modeling in climate applications:\n\nRackauckas et al. (2020): Scientific machine learning for differential equations\n\n\n\n\n\nRackauckas, Christopher, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan Edelman. 2020. “Universal Differential Equations for Scientific Machine Learning.” 2020. https://doi.org/10.48550/ARXIV.2001.04385.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Physics-based models and calibration 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html",
    "href": "chapters/hazard/sampling.html",
    "title": "10  Optimal sampling methods 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal sampling methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#learning-objectives",
    "href": "chapters/hazard/sampling.html#learning-objectives",
    "title": "10  Optimal sampling methods 🚧",
    "section": "",
    "text": "Apply sampling techniques to generate synthetic event sets\nUse importance and stratified sampling to improve efficiency in hazard modeling\nEvaluate how sampling choices affect estimates of extreme risk\nBalance computational cost vs accuracy in climate risk estimation",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal sampling methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#essential-sampling-concepts",
    "href": "chapters/hazard/sampling.html#essential-sampling-concepts",
    "title": "10  Optimal sampling methods 🚧",
    "section": "10.1 Essential sampling concepts",
    "text": "10.1 Essential sampling concepts\n\nMonte Carlo sampling and variance reduction\nImportance sampling for rare events\nStratified sampling strategies\nSynthetic event generation methods\nComputational efficiency optimization",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal sampling methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sampling.html#further-reading",
    "href": "chapters/hazard/sampling.html#further-reading",
    "title": "10  Optimal sampling methods 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Optimal sampling methods 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html",
    "href": "chapters/hazard/sensitivity-analysis.html",
    "title": "11  Global sensitivity analysis 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Global sensitivity analysis 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#learning-objectives",
    "href": "chapters/hazard/sensitivity-analysis.html#learning-objectives",
    "title": "11  Global sensitivity analysis 🚧",
    "section": "",
    "text": "Understand the role of sensitivity analysis in climate risk modeling\nApply variance-based sensitivity methods (Sobol indices) to identify key parameters\nUse Morris screening methods for initial parameter importance ranking\nInterpret sensitivity analysis results for model simplification and uncertainty reduction",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Global sensitivity analysis 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#essential-sensitivity-analysis-concepts",
    "href": "chapters/hazard/sensitivity-analysis.html#essential-sensitivity-analysis-concepts",
    "title": "11  Global sensitivity analysis 🚧",
    "section": "11.1 Essential sensitivity analysis concepts",
    "text": "11.1 Essential sensitivity analysis concepts\n\nGlobal vs local sensitivity analysis approaches\nSobol indices and variance decomposition\nMorris elementary effects for screening\nMulti-model sensitivity analysis\nImplementation strategies for complex model chains",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Global sensitivity analysis 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/hazard/sensitivity-analysis.html#further-reading",
    "href": "chapters/hazard/sensitivity-analysis.html#further-reading",
    "title": "11  Global sensitivity analysis 🚧",
    "section": "Further reading",
    "text": "Further reading\nFor global sensitivity analysis:\n\nSaltelli et al. (2008): Comprehensive introduction to GSA methods\nHerman and Usher (2017): Practical implementation guide with software tools\n\n\n\n\n\nHerman, Jon, and Will Usher. 2017. “SALib: An Open-Source Python Library for Sensitivity Analysis.” Journal of Open Source Software 2 (9): 97. https://doi.org/10.21105/joss.00097.\n\n\nSaltelli, Andrea, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. 2008. Global Sensitivity Analysis: The Primer. John Wiley & Sons, Ltd. http://onlinelibrary.wiley.com/doi/abs/10.1002/9780470725184.ch1.",
    "crumbs": [
      "**II: Hazard Assessment**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Global sensitivity analysis 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html",
    "href": "chapters/risk/exposure-vulnerability.html",
    "title": "12  Exposure and Vulnerability 🚧",
    "section": "",
    "text": "12.1 Learning objectives\nBy the end of this chapter, you should be able to:",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exposure and Vulnerability 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#learning-objectives",
    "href": "chapters/risk/exposure-vulnerability.html#learning-objectives",
    "title": "12  Exposure and Vulnerability 🚧",
    "section": "",
    "text": "Define exposure and vulnerability in the context of climate risk assessment\nDistinguish between different types of vulnerability (physical, social, economic)\nUnderstand methods for quantifying and mapping exposure\nApply vulnerability assessment frameworks to real-world scenarios\nIntegrate exposure and vulnerability data with hazard information for risk assessment",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exposure and Vulnerability 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/exposure-vulnerability.html#further-reading",
    "href": "chapters/risk/exposure-vulnerability.html#further-reading",
    "title": "12  Exposure and Vulnerability 🚧",
    "section": "12.2 Further reading",
    "text": "12.2 Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exposure and Vulnerability 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html",
    "href": "chapters/risk/expectations-cost-benefit.html",
    "title": "13  Cost-Benefit Analysis and Net Present Value 🚧",
    "section": "",
    "text": "Learning objectives\nAfter reading this chapter, you should be able to:",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#learning-objectives",
    "href": "chapters/risk/expectations-cost-benefit.html#learning-objectives",
    "title": "13  Cost-Benefit Analysis and Net Present Value 🚧",
    "section": "",
    "text": "Understand the theoretical foundation of cost-benefit analysis and Bayesian decision theory\nApply net present value calculations with appropriate discount rates\nQuantify costs and benefits using utility functions for climate risk decisions\nHandle uncertainty in cost-benefit frameworks using expected value\nRecognize the limitations and appropriate applications of cost-benefit analysis\nEvaluate economic trade-offs over different time horizons and scenarios",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/expectations-cost-benefit.html#further-reading",
    "href": "chapters/risk/expectations-cost-benefit.html#further-reading",
    "title": "13  Cost-Benefit Analysis and Net Present Value 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Cost-Benefit Analysis and Net Present Value 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html",
    "href": "chapters/risk/risk-transfer.html",
    "title": "15  Risk Transfer 🚧",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Exposure and Vulnerability - Expectations and Discounting",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#learning-objectives",
    "href": "chapters/risk/risk-transfer.html#learning-objectives",
    "title": "15  Risk Transfer 🚧",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nExplore financial instruments (insurance, reinsurance, catastrophe bonds) for spreading or transferring climate risk.\nUnderstand parametric insurance triggers and how they differ from indemnity-based approaches.\nAssess the role of public-private partnerships in risk transfer mechanisms.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#insurancereinsurance-fundamentals",
    "href": "chapters/risk/risk-transfer.html#insurancereinsurance-fundamentals",
    "title": "15  Risk Transfer 🚧",
    "section": "15.1 Insurance/reinsurance fundamentals",
    "text": "15.1 Insurance/reinsurance fundamentals",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#parametric-vs.-indemnity-coverage",
    "href": "chapters/risk/risk-transfer.html#parametric-vs.-indemnity-coverage",
    "title": "15  Risk Transfer 🚧",
    "section": "15.2 Parametric vs. indemnity coverage",
    "text": "15.2 Parametric vs. indemnity coverage",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#catastrophe-bonds-index-based-schemes",
    "href": "chapters/risk/risk-transfer.html#catastrophe-bonds-index-based-schemes",
    "title": "15  Risk Transfer 🚧",
    "section": "15.3 Catastrophe bonds, index-based schemes",
    "text": "15.3 Catastrophe bonds, index-based schemes",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#challenges-in-emerging-markets-and-vulnerable-regions",
    "href": "chapters/risk/risk-transfer.html#challenges-in-emerging-markets-and-vulnerable-regions",
    "title": "15  Risk Transfer 🚧",
    "section": "15.4 Challenges in emerging markets and vulnerable regions",
    "text": "15.4 Challenges in emerging markets and vulnerable regions",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/risk-transfer.html#further-reading",
    "href": "chapters/risk/risk-transfer.html#further-reading",
    "title": "15  Risk Transfer 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Risk Transfer 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html",
    "href": "chapters/risk/deep-uncertainty.html",
    "title": "16  Deep Uncertainty and Model Structure 🚧",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Probability and Statistics - Model Validation and Comparison",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deep Uncertainty and Model Structure 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#learning-objectives",
    "href": "chapters/risk/deep-uncertainty.html#learning-objectives",
    "title": "16  Deep Uncertainty and Model Structure 🚧",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nDistinguish between aleatory and epistemic uncertainty in climate risk assessment\nUnderstand the challenges posed by structural uncertainty and model disagreement\nApply Bayesian Model Averaging (BMA) and stacking approaches to combine multiple models\nRecognize when deep uncertainty invalidates traditional decision frameworks\nIdentify sources of deep uncertainty in exposure and impact modeling",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deep Uncertainty and Model Structure 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/deep-uncertainty.html#further-reading",
    "href": "chapters/risk/deep-uncertainty.html#further-reading",
    "title": "16  Deep Uncertainty and Model Structure 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deep Uncertainty and Model Structure 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html",
    "href": "chapters/risk/adaptive.html",
    "title": "18  Adaptive Planning and Flexibility 🚧",
    "section": "",
    "text": "See first\nThis chapter builds on concepts from: - Deep Uncertainty - Robustness",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Adaptive Planning and Flexibility 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#learning-objectives",
    "href": "chapters/risk/adaptive.html#learning-objectives",
    "title": "18  Adaptive Planning and Flexibility 🚧",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nPlan for uncertainty with adaptive management and iterative risk strategies.\nDevelop adaptation pathways that evolve with new information (e.g., climate data, impacts).\nIncorporate monitoring and feedback loops into long-term climate policy.",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Adaptive Planning and Flexibility 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/adaptive.html#further-reading",
    "href": "chapters/risk/adaptive.html#further-reading",
    "title": "18  Adaptive Planning and Flexibility 🚧",
    "section": "Further reading",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Adaptive Planning and Flexibility 🚧</span>"
    ]
  },
  {
    "objectID": "chapters/risk/social-science.html",
    "href": "chapters/risk/social-science.html",
    "title": "19  Working with People: Values, Participation, and Communication 🚧",
    "section": "",
    "text": "Further reading",
    "crumbs": [
      "**III: Risk Management**",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Working with People: Values, Participation, and Communication 🚧</span>"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Overview",
    "section": "",
    "text": "This collection of computational notebooks demonstrates the methods and concepts discussed in the main text through practical applications. Each notebook is designed to be standalone and self-contained, using the Julia programming language for all computations.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nExtreme Value Theory Examples ✏️\n\n\n\n\n\n\nInference Example: Flipping a Coin ✏️\n\n\n\n\n\n\nLinear regression: three perspectives on the same problem ✏️\n\n\n\n\n\n\nMosquito bites and beer consumption: simulation-based inference ✏️\n\n\n\n\n\n\nUnderstanding probability distributions ✏️\n\n\n\n\n\n\n \n\n\n\n\n\n\n \n\n\n\n\n\n\n \n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "**Computational Case Studies**",
      "Overview"
    ]
  },
  {
    "objectID": "notebooks/distribution-examples.html",
    "href": "notebooks/distribution-examples.html",
    "title": "Understanding probability distributions ✏️",
    "section": "",
    "text": "Distribution functions and their relationships\nThis notebook demonstrates the fundamental concepts underlying probability distributions. Understanding these relationships forms the foundation for statistical inference and uncertainty quantification in climate risk assessment.\nWe explore three essential functions that describe random variables: probability density functions (PDFs) or probability mass functions (PMFs), cumulative distribution functions (CDFs), and quantile functions. These concepts apply whether we’re modeling temperature variability, extreme precipitation events, or flood frequencies.\nEvery probability distribution can be characterized by three related functions. Understanding their relationships helps build intuition for how probability models describe uncertainty.",
    "crumbs": [
      "**Computational Case Studies**",
      "Understanding probability distributions ✏️"
    ]
  },
  {
    "objectID": "notebooks/distribution-examples.html#distribution-functions-and-their-relationships",
    "href": "notebooks/distribution-examples.html#distribution-functions-and-their-relationships",
    "title": "Understanding probability distributions ✏️",
    "section": "",
    "text": "Helper functions for visualization\nWe start by creating reusable functions for common visualization tasks. This approach keeps our main examples clean while demonstrating good programming practices.\n\nfunction add_pdf_area!(ax, dist, a, b; color = (:orange, 0.4), label = nothing)\n    \"\"\"Add shaded area under PDF curve between bounds a and b\"\"\"\n    x_fill = a:0.01:b\n    pdf_fill = pdf.(dist, x_fill)\n    band!(ax, x_fill, zeros(length(x_fill)), pdf_fill, color = color, label = label)\n    prob = cdf(dist, b) - cdf(dist, a)\n    return prob\nend\n\nfunction add_forward_cdf!(ax, dist, x_point; color = :red, x_min = -4)\n    \"\"\"Demonstrate forward CDF operation: given x, find F(x)\"\"\"\n    y_point = cdf(dist, x_point)\n    scatter!(ax, [x_point], [y_point], color = color, markersize = 8)\n    lines!(ax, [x_point, x_point], [0, y_point], color = color, linestyle = :dash)\n    lines!(ax, [x_min, x_point], [y_point, y_point], color = color, linestyle = :dash)\n    return y_point\nend\n\nfunction add_inverse_cdf!(ax, dist, p_target; color = :green, x_min = -4)\n    \"\"\"Demonstrate inverse CDF operation: given p, find x such that F(x) = p\"\"\"\n    x_inv = quantile(dist, p_target)\n    y_actual = cdf(dist, x_inv)\n    scatter!(ax, [x_inv], [y_actual], color = color, markersize = 8)\n    lines!(ax, [x_inv, x_inv], [0, y_actual], color = color, linestyle = :dash)\n    lines!(ax, [x_min, x_inv], [p_target, p_target], color = color, linestyle = :dash)\n    return x_inv, y_actual\nend\n\nThese helper functions encapsulate common visualization patterns. The add_pdf_area! function demonstrates how probabilities correspond to areas under density curves. The forward and inverse CDF functions show the relationship between values and cumulative probabilities.",
    "crumbs": [
      "**Computational Case Studies**",
      "Understanding probability distributions ✏️"
    ]
  },
  {
    "objectID": "notebooks/distribution-examples.html#normal-distribution-example",
    "href": "notebooks/distribution-examples.html#normal-distribution-example",
    "title": "Understanding probability distributions ✏️",
    "section": "Normal distribution example",
    "text": "Normal distribution example\nThe normal distribution illustrates these concepts for continuous random variables. Its smooth curves and well-known properties make it ideal for understanding probability fundamentals.\n\nfunction create_normal_example()\n    μ, σ = 0.0, 1.0\n    x_range = -4:0.01:4\n    normal_dist = Normal(μ, σ)\n\n    fig = Figure(size = (900, 400))\n\n    # PDF with area illustration\n    ax1 = Axis(fig[1, 1],\n        xlabel = L\"x\",\n        ylabel = L\"\\text{Density } p(x)\",\n        title = \"Normal(0, 1) PDF\")\n\n    lines!(ax1, x_range, pdf.(normal_dist, x_range),\n        color = :blue, linewidth = 2, label = L\"p(x)\")\n\n    prob_area = add_pdf_area!(ax1, normal_dist, -1, 1,\n        label = L\"P(-1 \\leq X \\leq 1)\")\n\n    text!(ax1, -0.6, 0.125,\n        text = L\"\\text{Area} = %$(round(prob_area, digits=3))\",\n        fontsize = 14, color = :black)\n\n    axislegend(ax1, position = :rt)\n\n    # CDF with forward and inverse operations\n    ax2 = Axis(fig[1, 2],\n        xlabel = L\"x\",\n        ylabel = L\"\\text{Probability } F(x)\",\n        title = \"Normal CDF: Forward and Inverse\")\n\n    lines!(ax2, x_range, cdf.(normal_dist, x_range),\n        color = :blue, linewidth = 2, label = L\"F(x)\")\n\n    y_point = add_forward_cdf!(ax2, normal_dist, 1.0)\n    text!(ax2, 1.2, y_point - 0.1,\n        text = L\"F(1) = %$(round(y_point, digits=3))\", color = :red)\n\n    x_inv, _ = add_inverse_cdf!(ax2, normal_dist, 0.25)\n    text!(ax2, x_inv - 0.8, 0.35,\n        text = L\"F^{-1}(0.25) = %$(round(x_inv, digits=2))\", color = :green)\n\n    axislegend(ax2, position = :rb)\n    return fig\nend\n\nfig_normal = create_normal_example()\nfig_normal\n\n\n\n\n\n\n\n\nThe normal distribution example shows how probability density relates to cumulative probability. The left panel demonstrates that probabilities correspond to areas under the density curve. The right panel shows the CDF’s S-shaped curve and illustrates both forward operations (finding probabilities from values) and inverse operations (finding values from probabilities).\nThese operations are fundamental to risk assessment: forward operations answer “what’s the probability of exceeding this threshold?” while inverse operations answer “what value corresponds to this probability?”",
    "crumbs": [
      "**Computational Case Studies**",
      "Understanding probability distributions ✏️"
    ]
  },
  {
    "objectID": "notebooks/distribution-examples.html#discrete-distributions-poisson-example",
    "href": "notebooks/distribution-examples.html#discrete-distributions-poisson-example",
    "title": "Understanding probability distributions ✏️",
    "section": "Discrete distributions: Poisson example",
    "text": "Discrete distributions: Poisson example\nDiscrete distributions illustrate the same concepts but with point masses rather than continuous densities. The Poisson distribution commonly models count data like the number of extreme events per year.\n\nfunction plot_pmf_stems!(ax, dist, x_range; color = :blue, linewidth = 3, markersize = 8)\n    \"\"\"Plot discrete PMF as stems with points\"\"\"\n    pmf_vals = pdf.(dist, x_range)\n    for (i, x) in enumerate(x_range)\n        lines!(ax, [x, x], [0, pmf_vals[i]], color = color, linewidth = linewidth)\n        scatter!(ax, [x], [pmf_vals[i]], color = color, markersize = markersize)\n    end\n    return pmf_vals\nend\n\nfunction highlight_pmf_mass!(ax, dist, x_range; color = :orange)\n    \"\"\"Highlight specific probability masses\"\"\"\n    pmf_vals = pdf.(dist, x_range)\n    for (i, x) in enumerate(x_range)\n        lines!(ax, [x, x], [0, pmf_vals[i]], color = color, linewidth = 5)\n        scatter!(ax, [x], [pmf_vals[i]], color = color, markersize = 10)\n    end\n    return sum(pmf_vals)\nend\n\nfunction plot_discrete_cdf!(ax, dist, x_range; color = :blue, linewidth = 2, markersize = 6)\n    \"\"\"Create step function visualization for discrete CDF\"\"\"\n    cdf_vals = cdf.(dist, x_range)\n    for i in 1:(length(x_range)-1)\n        lines!(ax, [x_range[i], x_range[i+1]], [cdf_vals[i], cdf_vals[i]],\n            color = color, linewidth = linewidth)\n    end\n    scatter!(ax, x_range, cdf_vals, color = color, markersize = markersize)\n    return cdf_vals\nend\n\nThese helper functions handle the specific visualization needs of discrete distributions. Unlike continuous distributions, discrete probabilities are point masses, and CDFs are step functions.\n\nfunction create_poisson_example()\n    λ = 3.0\n    x_range = 0:10\n    poisson_dist = Poisson(λ)\n\n    fig = Figure(size = (900, 400))\n\n    # PMF with highlighted probabilities\n    ax1 = Axis(fig[1, 1],\n        xlabel = L\"x\",\n        ylabel = L\"P(X = x)\",\n        title = L\"\\text{Poisson}(3) \\text{ PMF}\",\n        xticks = 1:10)\n\n    plot_pmf_stems!(ax1, poisson_dist, x_range)\n    prob_mass = highlight_pmf_mass!(ax1, poisson_dist, 0:2)\n\n    text!(ax1, 6, 0.15,\n        text = L\"P(X \\leq 2) = %$(round(prob_mass, digits=3))\",\n        fontsize = 14, color = :black)\n\n    # CDF with step function\n    ax2 = Axis(fig[1, 2],\n        xlabel = L\"x\",\n        ylabel = L\"\\text{Probability } F(x)\",\n        title = L\"\\text{Poisson CDF}\",\n        xticks = 1:10)\n\n    plot_discrete_cdf!(ax2, poisson_dist, x_range)\n\n    # Add example operations\n    y_point = cdf(poisson_dist, 4)\n    scatter!(ax2, [4], [y_point], color = :red, markersize = 10)\n    text!(ax2, 4.2, y_point - 0.1,\n        text = L\"F(4) = %$(round(y_point, digits=3))\", color = :red)\n\n    x_inv = quantile(poisson_dist, 0.4)\n    scatter!(ax2, [x_inv], [0.4], color = :green, markersize = 10)\n    text!(ax2, x_inv - 1.5, 0.5,\n        text = L\"F^{-1}(0.4) = %$(Int(x_inv))\", color = :green)\n\n    return fig\nend\n\nfig_poisson = create_poisson_example()\nfig_poisson\n\n\n\n\n\n\n\n\nThe Poisson distribution demonstrates these same fundamental concepts for discrete random variables. Individual probabilities are represented as point masses rather than areas under curves. The CDF becomes a step function that jumps at each possible value.\nThis distribution often appears in climate applications when modeling rare events like the annual number of hurricanes making landfall or the count of days exceeding extreme temperature thresholds.",
    "crumbs": [
      "**Computational Case Studies**",
      "Understanding probability distributions ✏️"
    ]
  },
  {
    "objectID": "notebooks/distribution-examples.html#multiple-variables-and-dependence",
    "href": "notebooks/distribution-examples.html#multiple-variables-and-dependence",
    "title": "Understanding probability distributions ✏️",
    "section": "Multiple variables and dependence",
    "text": "Multiple variables and dependence\nReal systems involve multiple interconnected variables. Understanding joint, marginal, and conditional distributions enables modeling of complex dependencies.\n\nfunction create_multivariate_example()\n    # Bivariate normal parameters\n    μ₁, μ₂ = 2.0, 1.0\n    σ₁, σ₂ = 1.0, 0.8\n    ρ = 0.6  # correlation coefficient\n\n    # Create bivariate normal distribution\n    Σ = [σ₁^2 ρ*σ₁*σ₂; ρ*σ₁*σ₂ σ₂^2]\n    mvn = MvNormal([μ₁, μ₂], Σ)\n\n    # Generate samples for visualization\n    Random.seed!(123)\n    n_samples = 1000\n    samples = rand(mvn, n_samples)\n    x_samples = samples[1, :]\n    y_samples = samples[2, :]\n\n    fig = Figure(size = (1000, 800))\n\n    # Main joint distribution (bottom left)\n    ax_main = Axis(fig[2, 1],\n        xlabel = L\"X\",\n        ylabel = L\"Y\",\n        title = \"Joint Distribution\")\n\n    scatter!(ax_main, x_samples, y_samples,\n        color = (:blue, 0.4), markersize = 4)\n\n    # Conditional distribution line\n    x_condition = 2.5\n    vlines!(ax_main, [x_condition], color = :red, linewidth = 3,\n        linestyle = :dash, label = L\"X = %$(x_condition)\")\n\n    # Marginal distribution of X (top)\n    ax_top = Axis(fig[1, 1],\n        ylabel = \"Density\",\n        title = L\"\\text{Marginal Distribution of }X\")\n\n    hist!(ax_top, x_samples, bins = 30, normalization = :pdf,\n        color = (:green, 0.6))\n\n    # True marginal density overlay\n    x_range = range(-1, 5, length = 100)\n    marginal_x = Normal(μ₁, σ₁)\n    lines!(ax_top, x_range, pdf.(marginal_x, x_range),\n        color = :green, linewidth = 3, label = \"True marginal\")\n\n    vlines!(ax_top, [x_condition], color = :red, linewidth = 2, linestyle = :dash)\n\n    # Marginal distribution of Y (right)\n    ax_right = Axis(fig[2, 2],\n        xlabel = \"Density\",\n        title = L\"Marginal Distribution of $Y$\")\n\n    hist!(ax_right, y_samples, bins = 30, normalization = :pdf,\n        color = (:orange, 0.6), direction = :x)\n\n    # True marginal density\n    y_range = range(-2, 4, length = 100)\n    marginal_y = Normal(μ₂, σ₂)\n    lines!(ax_right, pdf.(marginal_y, y_range), y_range,\n        color = :orange, linewidth = 3, label = \"True marginal\")\n\n    # Conditional distribution (top right)\n    ax_cond = Axis(fig[1, 2],\n        xlabel = L\"Y\",\n        ylabel = \"Conditional Density\",\n        title = L\"Conditional: $p(Y \\mid X = %$(x_condition))$\")\n\n    # Calculate conditional distribution parameters\n    μ_conditional = μ₂ + ρ * (σ₂ / σ₁) * (x_condition - μ₁)\n    σ_conditional = σ₂ * sqrt(1 - ρ^2)\n    conditional_dist = Normal(μ_conditional, σ_conditional)\n\n    lines!(ax_cond, y_range, pdf.(conditional_dist, y_range),\n        color = :red, linewidth = 3, label = L\"p(y | X = %$(x_condition))\")\n\n    # Show samples near conditioning value\n    tolerance = 0.2\n    near_condition = abs.(x_samples .- x_condition) .&lt; tolerance\n    y_near = y_samples[near_condition]\n\n    hist!(ax_cond, y_near, bins = 15, normalization = :pdf,\n        color = (:red, 0.4), label = L\"\\text{Samples near $X = %$(x_condition)$}\")\n\n    # Link axes for coordinated viewing\n    linkxaxes!(ax_main, ax_top)\n    linkyaxes!(ax_main, ax_right)\n\n    # Hide overlapping decorations\n    hidexdecorations!(ax_top, grid = false)\n    hideydecorations!(ax_right, grid = false)\n\n    # Add legends\n    axislegend(ax_main, position = :rt)\n    axislegend(ax_cond, position = :rt)\n\n    return fig\nend\n\nfig_joint = create_multivariate_example()\nfig_joint\n\n\n\n\n\n\n\n\nThis multivariate example demonstrates how joint distributions decompose into marginal and conditional components. The joint distribution (bottom left) shows the full relationship between variables. Marginal distributions (top and right panels) show each variable’s behavior independently. The conditional distribution (top right) shows how one variable behaves given specific values of another.\nThese concepts are essential for climate modeling where variables like temperature and precipitation are correlated. Understanding their joint behavior enables more accurate risk assessment than treating them independently.",
    "crumbs": [
      "**Computational Case Studies**",
      "Understanding probability distributions ✏️"
    ]
  },
  {
    "objectID": "notebooks/distribution-examples.html#key-insights-and-climate-applications",
    "href": "notebooks/distribution-examples.html#key-insights-and-climate-applications",
    "title": "Understanding probability distributions ✏️",
    "section": "Key insights and climate applications",
    "text": "Key insights and climate applications\nThe examples in this notebook illustrate fundamental principles that apply across all probability distributions:\nDistribution functions work together: PDFs/PMFs, CDFs, and quantile functions provide complementary views of the same underlying uncertainty.\nDiscrete and continuous cases follow similar logic: The mathematical relationships remain consistent whether dealing with counts or continuous measurements.\nMultiple variables require joint modeling: Real climate systems involve correlated variables that must be modeled together for accurate risk assessment.\nIn climate applications, these concepts appear when: - Modeling temperature distributions to assess heat wave probabilities - Analyzing extreme precipitation using heavy-tailed distributions - Understanding joint temperature-humidity relationships for heat stress assessment - Characterizing the frequency of compound events like concurrent drought and heat\nThe computational tools demonstrated here provide the foundation for more complex statistical inference methods covered in subsequent notebooks.",
    "crumbs": [
      "**Computational Case Studies**",
      "Understanding probability distributions ✏️"
    ]
  },
  {
    "objectID": "notebooks/inference-coin-flip.html",
    "href": "notebooks/inference-coin-flip.html",
    "title": "Inference Example: Flipping a Coin ✏️",
    "section": "",
    "text": "Problem\nThis notebook demonstrates the fundamental principles of statistical inference using the pedagogically clean example of coin flipping. While simple, this example illuminates the core concepts that underlie all statistical analysis: likelihood functions, maximum likelihood estimation, and Bayesian inference.\nThe same principles apply whether we’re estimating the probability of a fair coin or the frequency of extreme climate events. Understanding these methods enables principled learning from data under uncertainty.\nConsider flipping a coin of unknown success rate \\(\\theta\\) (the probability of heads) multiple times. After observing the outcomes, we want to estimate \\(\\theta\\). This mirrors many climate problems where we observe limited data and seek to understand underlying probabilities.",
    "crumbs": [
      "**Computational Case Studies**",
      "Inference Example: Flipping a Coin ✏️"
    ]
  },
  {
    "objectID": "notebooks/inference-coin-flip.html#problem",
    "href": "notebooks/inference-coin-flip.html#problem",
    "title": "Inference Example: Flipping a Coin ✏️",
    "section": "",
    "text": "Generating example data\nWe start with a known scenario to validate our inference methods.\n\n# Set parameters for reproducible example\nRandom.seed!(43)\ntrue_θ = 0.55  # Unknown to our \"inference\" methods\nn_flips = 12\n\n# Simulate the experiment (for illustration)\ncoin_flips = rand(n_flips) .&lt; true_θ\nobserved_heads = sum(coin_flips)\n\n# display as H or T\nflip_results = [flip ? \"H\" : \"T\" for flip in coin_flips]\n\nThis setup allows us to evaluate how well different inference methods recover the true parameter value. In real applications, we never know the true value and must rely on the methods demonstrated here.",
    "crumbs": [
      "**Computational Case Studies**",
      "Inference Example: Flipping a Coin ✏️"
    ]
  },
  {
    "objectID": "notebooks/inference-coin-flip.html#likelihood-based-inference",
    "href": "notebooks/inference-coin-flip.html#likelihood-based-inference",
    "title": "Inference Example: Flipping a Coin ✏️",
    "section": "Likelihood-based inference",
    "text": "Likelihood-based inference\nThe likelihood function bridges observed data and unknown parameters. It quantifies how probable our observed data would be under different parameter values.\n\nUnderstanding the likelihood function\nFor coin flips, the likelihood follows the binomial distribution: \\[p(\\text{data} \\mid \\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}\\]\nwhere \\(k\\) is the number of observed heads and \\(n\\) is the total number of flips.\n\nfunction plot_likelihood(y, n)\n    θ_grid = 0.01:0.01:0.99\n\n    # Calculate likelihood for each θ value\n    likelihood_vals = [pdf(Binomial(n, θ), y) for θ in θ_grid]\n\n    fig = Figure(size = (700, 500))\n    ax = Axis(fig[1, 1],\n        xlabel = L\"$\\theta$ \\text{ (probability of heads)}\",\n        ylabel = \"Likelihood\",\n        title = \"Likelihood function for $(y) heads in $(n) flips\")\n\n    lines!(ax, θ_grid, likelihood_vals,\n        label = L\"p(\\vec{y} \\mid \\theta)\", linewidth = 2, color = :blue)\n\n    # Mark the MLE\n    mle = y / n\n    vlines!(ax, [mle], color = :red, linestyle = :dash, linewidth = 2,\n        label = L\"$\\hat{\\theta}_\\text{MLE} = %$(round(mle, digits=2))$\")\n\n    # Mark the true value for comparison\n    vlines!(ax, [true_θ], color = :green, linestyle = :dot, linewidth = 2,\n        label = L\"$\\theta_\\text{true}$ = %$(true_θ)\")\n\n    axislegend(ax, position = :lt)\n    return fig\nend\n\nfig_likelihood = plot_likelihood(observed_heads, n_flips)\nfig_likelihood\n\n\n\n\n\n\n\n\nThe likelihood function shows how plausible different parameter values are given our observed data. It peaks at the observed proportion, which becomes our maximum likelihood estimate. Notice how the curve’s width reflects uncertainty—more data would create a sharper peak.\n\n\nMaximum likelihood estimation\nMaximum likelihood estimation (MLE) finds the parameter value that makes the observed data most probable. For the binomial model, this has a simple analytical solution.\n\nfunction analyze_mle_behavior()\n    n = 10\n    possible_heads = [3, 5, 7, 9]\n\n    fig = Figure(size = (800, 600))\n\n    θ_grid = 0.01:0.01:0.99\n\n    for (i, y) in enumerate(possible_heads)\n        row = div(i - 1, 2) + 1\n        col = ((i - 1) % 2) + 1\n\n        ax = Axis(fig[row, col],\n            xlabel = L\"$\\theta$\",\n            ylabel = \"likelihood\",\n            title = \"$(y)/$(n) heads\")\n\n        likelihood = pdf.(Binomial.(n, θ_grid), y)\n\n        lines!(ax, θ_grid, likelihood, linewidth = 2, color = :blue, label = L\"$p(\\vec{y} \\mid \\theta)$\")\n\n        mle = y / n\n        mle = round(mle, digits = 2)\n        vlines!(ax, [mle], color = :red, linestyle = :dash, linewidth = 2, label = L\"$\\hat{\\theta}_{MLE}$\")\n\n        if (row == 1) && (col == 1)\n            axislegend(ax, position = :rt)\n        end\n    end\n\n    return fig\nend\n\nfig_mle = analyze_mle_behavior()\nfig_mle\n\n\n\n\n\n\n\n\nEach panel shows how the likelihood function changes with different observed outcomes. The maximum always occurs at the observed proportion, confirming that MLE equals the sample proportion for binomial data.\nThis relationship holds broadly: MLE provides intuitive estimates that often match simple summary statistics for well-behaved problems.",
    "crumbs": [
      "**Computational Case Studies**",
      "Inference Example: Flipping a Coin ✏️"
    ]
  },
  {
    "objectID": "notebooks/inference-coin-flip.html#bayesian-inference",
    "href": "notebooks/inference-coin-flip.html#bayesian-inference",
    "title": "Inference Example: Flipping a Coin ✏️",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nFirst we define our probabilistic model in Turing.\n\n@model function coin_flip_model(y, n, prior_α, prior_β)\n    θ ~ Beta(prior_α, prior_β)\n    y ~ Binomial(n, θ)\n    return θ\nend\n\ncoin_flip_model (generic function with 2 methods)\n\n\nThen, we compare the analytical posterior (which is available due to conjugacy) to the posterior obtained via MCMC sampling.\n\nfunction compare_analytical_mcmc(y, n; prior_α = 3, prior_β = 3, n_samples = 5000, n_threads = 4)\n\n    # Analytical posterior\n    post_α = prior_α + y\n    post_β = prior_β + n - y\n    analytical_posterior = Beta(post_α, post_β)\n\n    # MCMC sampling\n    model = coin_flip_model(y, n, prior_α, prior_β)\n    chain = sample(\n        model, # the object with model + data\n        NUTS(), # our sampler\n        MCMCThreads(), # to use multiple threads\n        Int(ceil(n_samples / n_threads)), # number of draws total\n        n_threads, # number of chains\n        discard_initial = 1000, # burn-in samples per chain (discarded)\n        progress = false, # disable progress meter\n    )\n    mcmc_samples = vec(chain[:θ]) # convert to Vector (not always helpful)\n\n    # Return data for plotting instead of creating plots\n    return (analytical_posterior = analytical_posterior, mcmc_samples = mcmc_samples, n_samples = n_samples)\nend\n\n# Generate data by calling the same function twice  with different sample sizes\nn_low = 500\nn_high = 50_000\ndata_low = compare_analytical_mcmc(7, 10, n_samples = n_low)\ndata_high = compare_analytical_mcmc(7, 10, n_samples = n_high)\n\nWe can then compare how well we can sample the analytical distribution with different numbers of samples.\n\n# Combine plots\nfig_comparison = Figure(size = (1200, 500))\nax1 = Axis(fig_comparison[1, 1],\n    xlabel = L\"$\\theta$ (probability of heads)\",\n    ylabel = \"Density\",\n    title = \"MCMC with $n_low samples\")\nax2 = Axis(fig_comparison[1, 2],\n    xlabel = L\"$\\theta$ (probability of heads)\",\n    title = \"MCMC with $n_high samples\")\n\n# Link y-axes for comparison\n\nlinkaxes!(ax1, ax2)\n\n# Plot using the same plotting logic for both panels\n\nfor (ax, data) in [(ax1, data_low), (ax2, data_high)]\n    # Plot analytical posterior\n    θ_grid = 0.01:0.01:0.99\n\n    analytical_density = pdf.(data.analytical_posterior, θ_grid)\n    lines!(ax, θ_grid, analytical_density, label = \"Analytical\",\n        linewidth = 3, color = :blue)\n\n    # Plot MCMC histogram\n\n    hist!(ax, vec(data.mcmc_samples), bins = 40, normalization = :pdf,\n        color = (:red, 0.6), label = \"MCMC samples\")\n\n    # Add summary statistics\n    analytical_mean = mean(data.analytical_posterior)\n    mcmc_mean = mean(data.mcmc_samples)\n\n    # Vertical lines for means\n    vlines!(ax, [analytical_mean], color = :blue, linestyle = :dash, alpha = 0.8)\n    vlines!(ax, [mcmc_mean], color = :red, linestyle = :dot, alpha = 0.8)\n\n\n\n    # Add text annotations with statistics  \n    text!(ax, 0.35, maximum(analytical_density) * 0.95,\n        text = L\"\\text{Analytical: Mean} = %$(round(analytical_mean, digits=3))\",\n        fontsize = 10, color = :blue)\n    text!(ax, 0.35, maximum(analytical_density) * 0.85,\n        text = L\"\\text{MCMC: Mean} = %$(round(mcmc_mean, digits=3))\",\n        fontsize = 10, color = :red)\n\n    # Calculate P(theta &lt; 0.25) - tail probability\n    tail = 0.25\n    vlines!(ax, [tail], color = :gray, linestyle = :solid, alpha = 0.6, label = L\"$\\theta = %$(tail)$\")\n    analytical_tail_prob = cdf(data.analytical_posterior, tail)\n    analytical_tail_prob = @sprintf(\"%.2E\", analytical_tail_prob)\n\n    mcmc_tail_prob = mean(data.mcmc_samples .&lt; tail)\n    mcmc_tail_prob = @sprintf(\"%.2E\", mcmc_tail_prob)\n    text!(ax, 0.05, maximum(analytical_density) * 0.55,\n        text = L\"\\text{Analytical}: $P(\\theta &lt; %$(tail))$ = %$(analytical_tail_prob)\",\n        fontsize = 10, color = :blue)\n    text!(ax, 0.05, maximum(analytical_density) * 0.45,\n        text = L\"\\text{MCMC}: $P(\\theta &lt; %$(tail))$ = %$(mcmc_tail_prob)\",\n        fontsize = 10, color = :red)\n\n    axislegend(ax, position = :rt)\nend\n\nfig_comparison\n\n\n\n\n\n\n\n\nWe can see that with a small number of samples, we do a reasonable job of approximating the mean, but the tails are poorly estimated. This is a general principle in Monte Carlo methods: estimating tail probabilities requires many more samples than estimating central tendencies like the mean or median. Sometimes, specialized techniques like importance sampling are needed to accurately capture tail behavior.\n\nSequential Bayesian updating\nThe following example demonstrates how posterior distributions evolve as we observe more coin flips:\n\nfunction demonstrate_bayesian_updating()\n    # True parameter and prior\n    true_θ = 0.55\n    prior_α, prior_β = 3, 3\n\n    # Generate sequence of coin flips\n    Random.seed!(42)\n    n_total = 12\n    flips = rand(n_total) .&lt; true_θ  # true = heads, false = tails\n\n    # Convert to H/T string for display\n    flip_chars = [f ? \"H\" : \"T\" for f in flips]\n\n    # Stages to show (number of flips observed)\n    stages = [1, 3, 5, 8, 12]\n\n    # Create figure with subplots\n    fig = Figure(size = (1000, 600))\n\n    θ_range = 0:0.01:1\n\n    for (i, stage) in enumerate(stages)\n        # Calculate posterior parameters\n        y_obs = sum(flips[1:stage])  # heads observed so far\n        n_obs = stage                # total flips observed\n\n        post_α = prior_α + y_obs\n        post_β = prior_β + n_obs - y_obs\n\n        # Create subplot\n        row = div(i - 1, 3) + 1\n        col = ((i - 1) % 3) + 1\n        ax = Axis(fig[row, col],\n            xlabel = \"θ (probability of heads)\",\n            ylabel = \"Density\",\n            title = \"$(join(flip_chars[1:stage])) ($(y_obs)/$(n_obs) heads)\")\n\n        # Plot prior (only for first subplot)\n        if i == 1\n            prior_density = pdf.(Beta(prior_α, prior_β), θ_range)\n            lines!(ax, θ_range, prior_density,\n                color = :gray, linewidth = 2, linestyle = :dash, label = \"Prior Beta(3,3)\")\n        end\n\n        # Plot posterior\n        posterior = Beta(post_α, post_β)\n        posterior_density = pdf.(posterior, θ_range)\n        lines!(ax, θ_range, posterior_density,\n            color = :blue, linewidth = 3, label = \"Posterior\")\n\n        # Add vertical lines\n        vlines!(ax, [true_θ], color = :green, linewidth = 2, linestyle = :dot,\n            label = i == 1 ? \"True θ = $(true_θ)\" : \"\")\n\n        # MLE estimate\n        mle_θ = stage == 0 ? 0.5 : y_obs / n_obs\n        vlines!(ax, [mle_θ], color = :red, linewidth = 2, linestyle = :dashdot,\n            label = i == 1 ? \"MLE\" : \"\")\n\n        # Posterior mean\n        post_mean = post_α / (post_α + post_β)\n        vlines!(ax, [post_mean], color = :blue, linewidth = 2, alpha = 0.7,\n            label = i == 1 ? \"Posterior mean\" : \"\")\n\n        # Add text with posterior parameters\n        text!(ax, 0.05, maximum(posterior_density) * 0.9,\n            text = \"Beta($(post_α), $(post_β))\",\n            fontsize = 11, color = :blue)\n\n        # Add 95% credible interval\n        ci_lower = quantile(posterior, 0.025)\n        ci_upper = quantile(posterior, 0.975)\n        text!(ax, 0.05, maximum(posterior_density) * 0.75,\n            text = \"95% CI: [$(round(ci_lower, digits=2)), $(round(ci_upper, digits=2))]\",\n            fontsize = 10, color = :black)\n\n        # Set consistent x-axis limits\n        xlims!(ax, 0, 1)\n    end\n\n    # Add legend in the sixth panel (bottom right)\n    ax_legend = Axis(fig[2, 3],\n        xlabel = \"θ (probability of heads)\",\n        ylabel = \"Density\")\n\n    # Create dummy plots for legend\n    lines!(ax_legend, [0, 0], [0, 0], color = :gray, linewidth = 2, linestyle = :dash, label = \"Prior Beta(3,3)\")\n    lines!(ax_legend, [0, 0], [0, 0], color = :blue, linewidth = 3, label = \"Posterior\")\n    vlines!(ax_legend, [0], color = :green, linewidth = 2, linestyle = :dot, label = \"True θ = $(true_θ)\")\n    vlines!(ax_legend, [0], color = :red, linewidth = 2, linestyle = :dashdot, label = \"MLE\")\n    vlines!(ax_legend, [0], color = :blue, linewidth = 2, alpha = 0.7, label = \"Posterior mean\")\n\n    axislegend(ax_legend, position = :cc, framevisible = true)\n    hidespines!(ax_legend)\n    hidedecorations!(ax_legend)\n\n    # Add overall title\n    Label(fig[0, :], \"Bayesian Updating: How Posterior Evolves with New Evidence\",\n        fontsize = 16, font = \"TeX Gyre Heros Bold\")\n\n    return fig\nend\n\nfig_updating = demonstrate_bayesian_updating()\nfig_updating",
    "crumbs": [
      "**Computational Case Studies**",
      "Inference Example: Flipping a Coin ✏️"
    ]
  },
  {
    "objectID": "notebooks/linear-regression-examples.html",
    "href": "notebooks/linear-regression-examples.html",
    "title": "Linear regression: three perspectives on the same problem ✏️",
    "section": "",
    "text": "The regression problem\nThis notebook demonstrates how the same statistical problem can be approached from three different perspectives: curve fitting, maximum likelihood estimation, and Bayesian inference. Linear regression provides an ideal example because it bridges simple parameter estimation with multivariate modeling while maintaining analytical tractability.\nUnderstanding these different approaches illuminates the philosophical foundations underlying statistical methods and their practical implications for uncertainty quantification.\nLinear regression models the relationship between a predictor variable \\(x\\) and a response variable \\(y\\) as: \\[y_i = \\alpha + \\beta x_i + \\epsilon_i\\]\nwhere \\(\\alpha\\) is the intercept, \\(\\beta\\) is the slope, and \\(\\epsilon_i\\) represents random noise.\nThis framework applies broadly in climate science: relating temperature to elevation, precipitation to atmospheric pressure, or sea level rise to global temperature.",
    "crumbs": [
      "**Computational Case Studies**",
      "Linear regression: three perspectives on the same problem ✏️"
    ]
  },
  {
    "objectID": "notebooks/linear-regression-examples.html#the-regression-problem",
    "href": "notebooks/linear-regression-examples.html#the-regression-problem",
    "title": "Linear regression: three perspectives on the same problem ✏️",
    "section": "",
    "text": "Generating synthetic data\nWe create synthetic data from a known linear relationship to evaluate how well different methods recover the true parameters. First, we generate and plot our synthetic data.\n\n# Set reproducible parameters\nRandom.seed!(42)\nn_obs = 30\ntrue_intercept = 2.0\ntrue_slope = 1.5\ntrue_sigma = 1.5\n\n# Generate predictor and response data\nx_data = sort(rand(Uniform(0, 5), n_obs))\ny_data = true_intercept .+ true_slope .* x_data .+ rand(Normal(0, true_sigma), n_obs)\n\nfunction plot_synthetic_data()\n    fig = Figure(size = (700, 500))\n    ax = Axis(fig[1, 1],\n        xlabel = \"x\",\n        ylabel = \"y\",\n        title = \"Synthetic linear regression dataset\")\n\n    # Plot data points\n    scatter!(ax, x_data, y_data, color = :blue, markersize = 8, label = \"Observed data\")\n\n    # Plot true relationship\n    x_line = range(minimum(x_data), maximum(x_data), length = 100)\n    y_true = true_intercept .+ true_slope .* x_line\n    lines!(ax, x_line, y_true, color = :red, linewidth = 3,\n        label = L\"True: $y = %$(true_intercept) + %$(true_slope)x$\")\n\n    # Add uncertainty band\n    y_upper = y_true .+ 2 * true_sigma\n    y_lower = y_true .- 2 * true_sigma\n    band!(ax, x_line, y_lower, y_upper, color = (:red, 0.2), label = L\"±2σ\")\n\n    axislegend(ax, position = :lt)\n    return fig\nend\n\nfig_data = plot_synthetic_data()\nfig_data\n\n\n\n\n\n\n\n\nThe plot shows our synthetic data with the true linear relationship and noise level. The scattered points represent our “observations,” while the red line and shaded band show the true underlying process we’re trying to recover.",
    "crumbs": [
      "**Computational Case Studies**",
      "Linear regression: three perspectives on the same problem ✏️"
    ]
  },
  {
    "objectID": "notebooks/linear-regression-examples.html#approach-1-curve-fitting-least-squares",
    "href": "notebooks/linear-regression-examples.html#approach-1-curve-fitting-least-squares",
    "title": "Linear regression: three perspectives on the same problem ✏️",
    "section": "Approach 1: Curve fitting (least squares)",
    "text": "Approach 1: Curve fitting (least squares)\nThe curve fitting perspective treats regression as an optimization problem: find the line that minimizes prediction errors. This approach focuses on algorithmic solutions without probabilistic interpretation.\n\nMathematical foundation\nLeast squares minimizes the sum of squared residuals: \\[\\text{minimize} \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2\\]\nThis optimization problem has a closed-form analytical solution.\n\nfunction fit_least_squares(x, y)\n    \"\"\"Analytical least squares regression\"\"\"\n    n = length(x)\n    x_bar = mean(x)\n    y_bar = mean(y)\n\n    # Analytical formulas\n    slope = sum((x .- x_bar) .* (y .- y_bar)) / sum((x .- x_bar) .^ 2)\n    intercept = y_bar - slope * x_bar\n\n    # Calculate residuals and fit statistics\n    y_pred = intercept .+ slope .* x\n    residuals = y .- y_pred\n    mse = mean(residuals .^ 2)\n    r_squared = 1 - sum(residuals .^ 2) / sum((y .- y_bar) .^ 2)\n\n    return (\n        intercept = intercept,\n        slope = slope,\n        mse = mse,\n        r_squared = r_squared,\n        predictions = y_pred,\n        residuals = residuals,\n    )\nend\n\n# Fit the model\nols_results = fit_least_squares(x_data, y_data)\n\n# Create results summary\nols_summary = DataFrame(\n    \"parameter\" =&gt; [\"Intercept\", \"Slope\", \"MSE\", \"R²\"],\n    \"estimate\" =&gt; [ols_results.intercept, ols_results.slope, ols_results.mse, ols_results.r_squared],\n    \"true_value\" =&gt; [true_intercept, true_slope, missing, missing],\n)\nols_summary\n\n4×3 DataFrame\n\n\n\nRow\nparameter\nestimate\ntrue_value\n\n\n\nString\nFloat64\nFloat64?\n\n\n\n\n1\nIntercept\n1.85082\n2.0\n\n\n2\nSlope\n1.58365\n1.5\n\n\n3\nMSE\n2.22587\nmissing\n\n\n4\nR²\n0.711843\nmissing\n\n\n\n\n\n\nThe least squares estimates are close to the true values, demonstrating that the method works well for this problem. However, this approach provides no direct way to quantify uncertainty in the parameter estimates.\n\n\nVisualization with residuals\n\nfunction plot_least_squares_fit()\n    fig = Figure(size = (800, 500))\n    ax = Axis(fig[1, 1],\n        xlabel = \"x\",\n        ylabel = \"y\",\n        title = \"Least squares curve fitting\")\n\n    # Plot data points\n    scatter!(ax, x_data, y_data, color = :blue, markersize = 8, label = \"Data\")\n\n    # Plot fitted line\n    x_line = range(minimum(x_data), maximum(x_data), length = 100)\n    y_fitted = ols_results.intercept .+ ols_results.slope .* x_line\n    lines!(ax, x_line, y_fitted, color = :red, linewidth = 2,\n        label = L\"Fitted: $y = %$(round(ols_results.intercept, digits=2)) + %$(round(ols_results.slope, digits=2))x$\")\n\n    # Show residuals as vertical lines\n    for i in eachindex(x_data)\n        lines!(ax, [x_data[i], x_data[i]], [y_data[i], ols_results.predictions[i]],\n            color = :gray, linestyle = :dash, alpha = 0.7, linewidth = 1)\n    end\n\n    # Add true line for comparison\n    y_true_line = true_intercept .+ true_slope .* x_line\n    lines!(ax, x_line, y_true_line, color = :green, linewidth = 2, linestyle = :dot,\n        label = L\"True: $y = %$(true_intercept) + %$(true_slope)x$\")\n\n    axislegend(ax, position = :lt)\n    return fig\nend\n\nfig_ols = plot_least_squares_fit()\nfig_ols\n\n\n\n\n\n\n\n\nThe residual lines (gray dashes) show prediction errors for each data point. Least squares minimizes the sum of squared lengths of these lines, providing the “best fit” in this specific sense.",
    "crumbs": [
      "**Computational Case Studies**",
      "Linear regression: three perspectives on the same problem ✏️"
    ]
  },
  {
    "objectID": "notebooks/linear-regression-examples.html#approach-2-maximum-likelihood-estimation",
    "href": "notebooks/linear-regression-examples.html#approach-2-maximum-likelihood-estimation",
    "title": "Linear regression: three perspectives on the same problem ✏️",
    "section": "Approach 2: Maximum likelihood estimation",
    "text": "Approach 2: Maximum likelihood estimation\nThe probabilistic perspective treats regression as a statistical model with explicitly specified error distributions. This enables likelihood-based inference and uncertainty quantification.\n\nStatistical model specification\nWe assume: \\[y_i \\sim \\text{Normal}(\\alpha + \\beta x_i, \\sigma^2)\\]\nThis specifies that responses are normally distributed around the linear predictor with constant variance.\n\n# Define the regression model - we'll reuse this for Bayesian inference\n@model function linear_regression_model(x, y)\n    n = length(y)\n\n    # Parameters to estimate\n    α ~ Normal(0, 5)      # Intercept\n    β ~ Normal(0, 5)      # Slope\n    σ ~ LogNormal(0, 1)   # Noise standard deviation\n\n    # Likelihood\n    μ = α .+ β .* x\n    for i in 1:n\n        y[i] ~ Normal(μ[i], σ)\n    end\nend\n\n# Find MLE estimates using Turing's mode estimation\nmodel = linear_regression_model(x_data, y_data)\nmle_result = optimize(model, MLE())\nmle_params = mle_result.values\n\n# Extract parameter estimates\nmle_results = (\n    intercept = mle_params[:α],\n    slope = mle_params[:β],\n    sigma = mle_params[:σ],\n)\n\n# Create comparison table\nmle_summary = DataFrame(\n    Parameter = [\"Intercept\", \"Slope\", \"Sigma\"],\n    MLE = [mle_results.intercept, mle_results.slope, mle_results.sigma],\n    OLS = [ols_results.intercept, ols_results.slope, sqrt(ols_results.mse)],\n    True_Value = [true_intercept, true_slope, true_sigma],\n)\n\nmle_summary\n\n3×4 DataFrame\n\n\n\nRow\nParameter\nMLE\nOLS\nTrue_Value\n\n\n\nString\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nIntercept\n1.85082\n1.85082\n2.0\n\n\n2\nSlope\n1.58365\n1.58365\n1.5\n\n\n3\nSigma\n1.49193\n1.49193\n1.5\n\n\n\n\n\n\nTuring’s optimize function with MLE() provides a clean, robust approach to maximum likelihood estimation without requiring custom optimization code. The MLE estimates for intercept and slope match the OLS results exactly, confirming the theoretical equivalence between these approaches for linear regression with normal errors. Additionally, MLE provides an estimate of the noise parameter \\(\\sigma\\).",
    "crumbs": [
      "**Computational Case Studies**",
      "Linear regression: three perspectives on the same problem ✏️"
    ]
  },
  {
    "objectID": "notebooks/linear-regression-examples.html#approach-3-bayesian-inference",
    "href": "notebooks/linear-regression-examples.html#approach-3-bayesian-inference",
    "title": "Linear regression: three perspectives on the same problem ✏️",
    "section": "Approach 3: Bayesian inference",
    "text": "Approach 3: Bayesian inference\nBayesian inference treats all parameters as random variables with probability distributions. It combines prior beliefs with observed data to produce posterior distributions that fully characterize parameter uncertainty.\n\nBayesian inference implementation\n\nfunction run_bayesian_regression(x, y; samples_per_chain = 2000, n_chains = 4)\n    model = linear_regression_model(x, y)\n\n    chains = begin\n        sampler = NUTS()\n        discard_initial = 1_000\n        sample(model, sampler, MCMCThreads(),\n            samples_per_chain, n_chains,\n            discard_initial = discard_initial,\n            verbose = false, progress = false)\n    end\n\n    return chains\nend\n\n# Run Bayesian inference\nbayesian_chains = run_bayesian_regression(x_data, y_data)\nsummary(bayesian_chains)\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/FSyVk/src/sample.jl:382\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.025\n\n\n\n\n\"MCMCChains.Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, @NamedTuple{parameters::Vector{Symbol}, internals::Vector{Symbol}}, @NamedTuple{varname_to_symbol::OrderedCollections.OrderedDict{AbstractPPL.VarName, Symbol}, start_time::Vector{Float64}, stop_time::Vector{Float64}}}\"\n\n\nThe diagnostic table is consistent with, though does not guarantee, MCMC convergence and accurate sampling. Fundamentally, we can check for comon signs of non-convergence and rule them out, but we cannot actually prove that we are sampling from the true posterior distribution. While there are no perfect rules, generally\n\nLow values of \\(\\hat{R}\\) (rhat) close to 1 (e.g., &lt;1.1) are consistent with “good mixing”, meaning that the chains have converged to the target distribution.\nHigh effective sample sizes (ess_bulk, ess_tail) indicate that the samples are more approximately independent, which is desirable for accurate posterior estimation.\n\n\n\nTrace plot\n\n# Extract samples from chains (keep chain structure for trace plots)\nα_samples = vec(Array(bayesian_chains[:α]))  # flattened for summary stats\nβ_samples = vec(Array(bayesian_chains[:β]))\nσ_samples = vec(Array(bayesian_chains[:σ]))\n\nfunction plot_mcmc_trace(chains, title_prefix = \"\")\n    fig = Figure(size = (1200, 800))\n\n    # Extract parameter arrays (chains are organized as samples x variables x chains)\n    α_arr = Array(chains[:α])\n    β_arr = Array(chains[:β])\n    σ_arr = Array(chains[:σ])\n\n    nsamples, nchains = size(α_arr, 1), size(α_arr, 2)\n\n    # Set up all axes\n    ax1 = Axis(fig[1, 1], xlabel = \"Iteration\", ylabel = \"α\", title = \"$(title_prefix)Intercept Parameter Trace\")\n    ax2 = Axis(fig[1, 2], xlabel = \"Iteration\", ylabel = \"β\", title = \"$(title_prefix)Slope Parameter Trace\")\n    ax3 = Axis(fig[1, 3], xlabel = \"Iteration\", ylabel = \"σ\", title = \"$(title_prefix)Noise Parameter Trace\")\n    ax4 = Axis(fig[2, 1], xlabel = \"α\", ylabel = \"Density\", title = \"$(title_prefix)Intercept Posterior\")\n    ax5 = Axis(fig[2, 2], xlabel = \"β\", ylabel = \"Density\", title = \"$(title_prefix)Slope Posterior\")\n    ax6 = Axis(fig[2, 3], xlabel = \"σ\", ylabel = \"Density\", title = \"$(title_prefix)Noise Posterior\")\n\n    # Define colors for chains\n    colors = [:red, :blue, :green, :orange]\n\n    # Plot traces and histograms for each chain\n    for c in 1:nchains\n        # Trace plots\n        lines!(ax1, 1:nsamples, α_arr[:, c], label = \"Chain $(c)\", color = colors[c])\n        lines!(ax2, 1:nsamples, β_arr[:, c], label = \"Chain $(c)\", color = colors[c])\n        lines!(ax3, 1:nsamples, σ_arr[:, c], label = \"Chain $(c)\", color = colors[c])\n\n        # Marginal densities\n        hist!(ax4, α_arr[:, c], bins = 40, normalization = :pdf, color = (colors[c], 0.3), label = \"Chain $(c)\")\n        hist!(ax5, β_arr[:, c], bins = 40, normalization = :pdf, color = (colors[c], 0.3), label = \"Chain $(c)\")\n        hist!(ax6, σ_arr[:, c], bins = 40, normalization = :pdf, color = (colors[c], 0.3), label = \"Chain $(c)\")\n    end\n\n    # Add true value lines\n    vlines!(ax4, [true_intercept], color = :black, linestyle = :dash, linewidth = 2, label = \"True value\")\n    vlines!(ax5, [true_slope], color = :black, linestyle = :dash, linewidth = 2, label = \"True value\")\n    vlines!(ax6, [true_sigma], color = :black, linestyle = :dash, linewidth = 2, label = \"True value\")\n\n    # Add legends to posterior plots\n    axislegend(ax4; position = :rt)\n    axislegend(ax5; position = :rt)\n    axislegend(ax6; position = :rt)\n\n    return fig\nend\n\nfig_trace = plot_mcmc_trace(bayesian_chains, \"Linear Regression: \")\nfig_trace\n\n\n\n\n\n\n\n\nWe can see from this plot that the chains visually appear to be well-mixed and stationary, which again is consistent with, though not definitive proof of, convergence.\n\n\nPosterior predictive distribution\nThe posterior predictive distribution represents our beliefs about new data given what we’ve observed. It’s computed as \\(p(y_{\\text{new}} | y_{\\text{obs}}) = \\int p(y_{\\text{new}} | \\theta) p(\\theta | y_{\\text{obs}}) d\\theta\\).\n\nfunction plot_posterior_predictive()\n    # Generate posterior predictive samples at new x values\n    x_new = range(0, 6, length = 50)\n    n_samples = 200\n    sample_indices = rand(1:length(α_samples), n_samples)\n\n    # For each posterior sample, generate predictions\n    y_pred_samples = []\n    for i in sample_indices\n        α_i = α_samples[i]\n        β_i = β_samples[i]\n        σ_i = σ_samples[i]\n\n        # Mean prediction\n        μ_pred = α_i .+ β_i .* x_new\n        # Add observation noise\n        y_pred = rand.(Normal.(μ_pred, σ_i))\n        push!(y_pred_samples, y_pred)\n    end\n\n    # Calculate prediction quantiles\n    y_lower = [quantile([y_pred_samples[j][i] for j in 1:n_samples], 0.05) for i in 1:length(x_new)]\n    y_upper = [quantile([y_pred_samples[j][i] for j in 1:n_samples], 0.95) for i in 1:length(x_new)]\n    y_median = [quantile([y_pred_samples[j][i] for j in 1:n_samples], 0.5) for i in 1:length(x_new)]\n\n    # Create plot\n    fig = Figure(size = (800, 600))\n    ax = Axis(fig[1, 1],\n        xlabel = \"x\",\n        ylabel = \"y\",\n        title = \"Posterior predictive distribution\")\n\n    # Plot observed data\n    scatter!(ax, x_data, y_data, color = :black, markersize = 8, label = \"Observed data\")\n\n    # Plot prediction interval\n    band!(ax, x_new, y_lower, y_upper, color = (:blue, 0.3), label = \"90% prediction interval\")\n\n    # Plot median prediction\n    lines!(ax, x_new, y_median, color = :blue, linewidth = 2, label = \"Posterior median\")\n\n    # Add true relationship for reference\n    y_true = true_intercept .+ true_slope .* x_new\n    lines!(ax, x_new, y_true, color = :red, linewidth = 2, linestyle = :dash, label = \"True relationship\")\n\n    axislegend(ax, position = :lt)\n    return fig\nend\n\nfig_ppd = plot_posterior_predictive()\nfig_ppd\n\n\n\n\n\n\n\n\nThe posterior predictive distribution incorporates both parameter uncertainty and observation noise. The 90% prediction interval shows where we expect new observations to fall, while the median line shows our best prediction. This is the fundamental quantity for forecasting and decision-making under uncertainty.",
    "crumbs": [
      "**Computational Case Studies**",
      "Linear regression: three perspectives on the same problem ✏️"
    ]
  },
  {
    "objectID": "notebooks/extremes-examples.html",
    "href": "notebooks/extremes-examples.html",
    "title": "Extreme Value Theory Examples ✏️",
    "section": "",
    "text": "Shape parameter implications\nThis notebook demonstrates extreme value analysis concepts using real Houston precipitation data, illustrating the fundamental extrapolation problem in climate risk assessment.\nThe shape parameter ξ in extreme value distributions fundamentally determines how heavy the tail is, which directly affects our extrapolation to rare events. Let’s visualize how different shape parameter values lead to dramatically different tail behavior:\nThe key insight: the shape parameter \\(\\xi\\) controls how rapidly probabilities decrease in the tail.",
    "crumbs": [
      "**Computational Case Studies**",
      "Extreme Value Theory Examples ✏️"
    ]
  },
  {
    "objectID": "notebooks/extremes-examples.html#houston-precipitation-data",
    "href": "notebooks/extremes-examples.html#houston-precipitation-data",
    "title": "Extreme Value Theory Examples ✏️",
    "section": "Houston precipitation data",
    "text": "Houston precipitation data\nLet’s look at this using a realistic example\n\n# Load GHCN daily precipitation data from Houston area station USC00414313\n# Data downloaded from: https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00414313.csv.gz\n\nraw_data = CSV.read(joinpath(dirname(@__FILE__()), \"data/USC00414313.csv\"), DataFrame, header=true)\nstation_id = \"GHCND:USC00414313\"\nstation_name = \"HOUSTON BARKER, TX US\"\n\nprintln(\"Loaded $(nrow(raw_data)) raw GHCN daily observations\")\nprintln(\"First few rows:\")\ndisplay(first(raw_data, 3))\n\nLoaded 70366 raw GHCN daily observations\nFirst few rows:\n\n\n3×8 DataFrame\n\n\n\nRow\nUSC00414313\n19430101\nPRCP\n0\nColumn5\nColumn6\n6\nColumn8\n\n\n\nString15\nInt64\nString7\nInt64\nString1?\nMissing\nString1\nInt64?\n\n\n\n\n1\nUSC00414313\n19430102\nPRCP\n0\nmissing\nmissing\n6\nmissing\n\n\n2\nUSC00414313\n19430103\nPRCP\n0\nmissing\nmissing\n6\nmissing\n\n\n3\nUSC00414313\n19430104\nPRCP\n0\nmissing\nmissing\n6\nmissing\n\n\n\n\n\n\nThis loads the raw GHCN (Global Historical Climatology Network) data file. Each row contains weather measurements with a specific format that we need to decode.",
    "crumbs": [
      "**Computational Case Studies**",
      "Extreme Value Theory Examples ✏️"
    ]
  },
  {
    "objectID": "notebooks/extremes-examples.html#processing-ghcn-format",
    "href": "notebooks/extremes-examples.html#processing-ghcn-format",
    "title": "Extreme Value Theory Examples ✏️",
    "section": "Processing GHCN format",
    "text": "Processing GHCN format\nGHCN daily files use a fixed format where each row represents one measurement at one station on one date:\n\n\nAfter filtering for precipitation (PRCP):\n- Total PRCP observations: 25225\n- Date range (raw): 19430102 to 20131130\n\n\nGHCN includes many weather variables (temperature, snow, etc.), but we only need precipitation (PRCP). The date is stored as YYYYMMDD format and values are in tenths of millimeters.",
    "crumbs": [
      "**Computational Case Studies**",
      "Extreme Value Theory Examples ✏️"
    ]
  },
  {
    "objectID": "notebooks/extremes-examples.html#converting-dates-and-units",
    "href": "notebooks/extremes-examples.html#converting-dates-and-units",
    "title": "Extreme Value Theory Examples ✏️",
    "section": "Converting dates and units",
    "text": "Converting dates and units\n\n\nAfter format conversion:\n- Daily observations: 25225\n- Date range: 1943-01-02 to 2013-11-30\n- Years of data: 71\n- Max daily rainfall: 272.3 mm\n\n\nNow we have properly formatted dates and rainfall measurements in standard millimeter units. This represents over 70 years of daily precipitation measurements - our “high-frequency observations.”",
    "crumbs": [
      "**Computational Case Studies**",
      "Extreme Value Theory Examples ✏️"
    ]
  },
  {
    "objectID": "notebooks/extremes-examples.html#filling-missing-dates-for-complete-time-series",
    "href": "notebooks/extremes-examples.html#filling-missing-dates-for-complete-time-series",
    "title": "Extreme Value Theory Examples ✏️",
    "section": "Filling missing dates for complete time series",
    "text": "Filling missing dates for complete time series\nGHCN data only includes dates where measurements were recorded. For proper time series analysis and plotting, we need to fill in missing dates:\n\n\nDate completeness:\n- GHCN observations: 25225 days\n- Complete date range: 25901 days\n- Missing days: 676\n- Complete time series: 25901 days\n- Missing rainfall values: 676\n\n\nThis creates a complete daily time series from $(minimum(precip_data.date)) to $(maximum(precip_data.date)) with missing values for days without measurements. Missing values are common in historical weather data due to equipment issues, observer absence, or data quality problems.",
    "crumbs": [
      "**Computational Case Studies**",
      "Extreme Value Theory Examples ✏️"
    ]
  },
  {
    "objectID": "notebooks/extremes-examples.html#daily-precipitation-time-series",
    "href": "notebooks/extremes-examples.html#daily-precipitation-time-series",
    "title": "Extreme Value Theory Examples ✏️",
    "section": "Daily precipitation time series",
    "text": "Daily precipitation time series\nNow let’s visualize the daily precipitation data to see the “high-frequency observations” mentioned in our extrapolation problem:\n\n\n\n\n\n\n\n\n\nOr a subset",
    "crumbs": [
      "**Computational Case Studies**",
      "Extreme Value Theory Examples ✏️"
    ]
  },
  {
    "objectID": "notebooks/mosquitos.html",
    "href": "notebooks/mosquitos.html",
    "title": "Mosquito bites and beer consumption: simulation-based inference ✏️",
    "section": "",
    "text": "The research question\nThis notebook demonstrates computational approaches to hypothesis testing using a practical example: whether drinking beer affects mosquito bite frequency. The analysis illustrates how simulation can provide intuitive understanding of statistical concepts without relying on complex mathematical assumptions.\nThis example showcases the power of computational thinking in statistics, where complex theoretical concepts become accessible through direct simulation. The same principles apply to climate data analysis where traditional parametric assumptions may not hold.\nWe investigate a practical question with broader implications for experimental design and causal inference:\nThis question exemplifies common challenges in environmental and health research where controlled experiments must account for multiple confounding factors. The statistical methods demonstrated here apply broadly to climate impact studies and policy evaluation.",
    "crumbs": [
      "**Computational Case Studies**",
      "Mosquito bites and beer consumption: simulation-based inference ✏️"
    ]
  },
  {
    "objectID": "notebooks/mosquitos.html#the-research-question",
    "href": "notebooks/mosquitos.html#the-research-question",
    "title": "Mosquito bites and beer consumption: simulation-based inference ✏️",
    "section": "",
    "text": "Does drinking beer reduce the likelihood of being bitten by mosquitos?\n\n\n\nLearning objectives\nAfter working through this analysis, you should understand: - The logic underlying permutation testing and simulation-based inference - How computational methods can replace complex mathematical derivations - The interpretation of p-values through direct simulation - When simulation-based tests offer advantages over parametric approaches",
    "crumbs": [
      "**Computational Case Studies**",
      "Mosquito bites and beer consumption: simulation-based inference ✏️"
    ]
  },
  {
    "objectID": "notebooks/mosquitos.html#experimental-data",
    "href": "notebooks/mosquitos.html#experimental-data",
    "title": "Mosquito bites and beer consumption: simulation-based inference ✏️",
    "section": "Experimental data",
    "text": "Experimental data\nThe dataset comes from a controlled experiment where participants were randomly assigned to consume either beer or water, then exposed to mosquitos in a controlled environment. Researchers counted the number of mosquito bites received by each participant.\n\n# Beer drinkers group\nbeer_bites = [\n    27, 20, 21, 26, 27, 31, 24, 21, 20, 19, 23, 24, 28,\n    19, 24, 29, 18, 20, 17, 31, 20, 25, 28, 21, 27\n]\n\n# Water drinkers group (control)\nwater_bites = [\n    21, 22, 15, 12, 21, 16, 19, 15, 22, 24, 19, 23,\n    13, 22, 20, 24, 18, 20\n]\n\n# Create summary table\ndata_summary = DataFrame(\n    Group = [\"Beer drinkers\", \"Water drinkers\"],\n    N_participants = [length(beer_bites), length(water_bites)],\n    Mean_bites = [mean(beer_bites), mean(water_bites)],\n    Std_bites = [std(beer_bites), std(water_bites)],\n    Min_bites = [minimum(beer_bites), minimum(water_bites)],\n    Max_bites = [maximum(beer_bites), maximum(water_bites)]\n)\n\ndata_summary\n\n2×6 DataFrame\n\n\n\nRow\nGroup\nN_participants\nMean_bites\nStd_bites\nMin_bites\nMax_bites\n\n\n\nString\nInt64\nFloat64\nFloat64\nInt64\nInt64\n\n\n\n\n1\nBeer drinkers\n25\n23.6\n4.1332\n17\n31\n\n\n2\nWater drinkers\n18\n19.2222\n3.67112\n12\n24\n\n\n\n\n\n\nThe descriptive statistics reveal that beer drinkers received more mosquito bites on average. However, we need statistical analysis to determine whether this difference could reasonably be attributed to random variation.",
    "crumbs": [
      "**Computational Case Studies**",
      "Mosquito bites and beer consumption: simulation-based inference ✏️"
    ]
  },
  {
    "objectID": "notebooks/mosquitos.html#initial-analysis-observed-difference",
    "href": "notebooks/mosquitos.html#initial-analysis-observed-difference",
    "title": "Mosquito bites and beer consumption: simulation-based inference ✏️",
    "section": "Initial analysis: observed difference",
    "text": "Initial analysis: observed difference\nThe most direct approach compares the average number of bites between groups. This test statistic captures the core research question in a single number.\n\n# Calculate the observed difference in means\nobserved_diff = mean(beer_bites) - mean(water_bites)\n\n# Create results summary\ninitial_analysis = DataFrame(\n    Statistic = [\"Beer group mean\", \"Water group mean\", \"Difference (Beer - Water)\"],\n    Value = [mean(beer_bites), mean(water_bites), observed_diff]\n)\n\ninitial_analysis\n\n3×2 DataFrame\n\n\n\nRow\nStatistic\nValue\n\n\n\nString\nFloat64\n\n\n\n\n1\nBeer group mean\n23.6\n\n\n2\nWater group mean\n19.2222\n\n\n3\nDifference (Beer - Water)\n4.37778\n\n\n\n\n\n\nBeer drinkers received approximately 5.1 more bites on average than water drinkers. But is this difference statistically meaningful, or could it arise from random variation alone?",
    "crumbs": [
      "**Computational Case Studies**",
      "Mosquito bites and beer consumption: simulation-based inference ✏️"
    ]
  },
  {
    "objectID": "notebooks/mosquitos.html#the-null-hypothesis-and-permutation-testing",
    "href": "notebooks/mosquitos.html#the-null-hypothesis-and-permutation-testing",
    "title": "Mosquito bites and beer consumption: simulation-based inference ✏️",
    "section": "The null hypothesis and permutation testing",
    "text": "The null hypothesis and permutation testing\nThe skeptic’s position forms our null hypothesis: drinking beer has no effect on mosquito bite frequency. Under this assumption, the group assignment (beer vs. water) is irrelevant, and the observed difference arose by chance.\nWe can test this hypothesis through permutation testing, which simulates what would happen if the null hypothesis were true.\n\nTwo approaches to hypothesis testing\nParametric approach (t-test): - Assumes data follow specific distributions (normal) - Uses mathematical theory for p-value calculation - Fast but relies on assumptions that may not hold\nSimulation approach (permutation test): - Makes minimal distributional assumptions - Uses computational power instead of mathematical theory - More intuitive and flexible\n\n\nImplementing permutation testing\nThe logic is straightforward: if treatment assignment doesn’t matter, we can randomly shuffle group labels and recalculate the difference.\n\nfunction calculate_permuted_difference(group1_data, group2_data)\n    \"\"\"Calculate difference in means after random permutation of group labels\"\"\"\n    # Combine all data\n    all_data = vcat(group1_data, group2_data)\n\n    # Randomly shuffle all observations\n    shuffled_data = shuffle(all_data)\n\n    # Reassign to groups with original group sizes\n    n1 = length(group1_data)\n    new_group1 = shuffled_data[1:n1]\n    new_group2 = shuffled_data[(n1+1):end]\n\n    # Calculate difference in means\n    return mean(new_group1) - mean(new_group2)\nend\n\n# Test the function\nexample_permuted_diff = calculate_permuted_difference(beer_bites, water_bites)\n@printf(\"Example permuted difference: %.2f\\n\", example_permuted_diff)\n\nExample permuted difference: 0.46\n\n\nThis function simulates one possible outcome under the null hypothesis. By repeating this process thousands of times, we can characterize the full distribution of differences we’d expect by chance alone.\n\n\nGenerating the null distribution\n\n# Set seed for reproducibility\nRandom.seed!(42)\n\n# Generate many permuted differences\nn_permutations = 50_000\npermuted_differences = [\n    calculate_permuted_difference(beer_bites, water_bites)\n    for _ in 1:n_permutations\n]\n\n# Summarize the null distribution\nnull_summary = DataFrame(\n    Statistic = [\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"2.5th percentile\", \"97.5th percentile\"],\n    Value = [\n        mean(permuted_differences),\n        std(permuted_differences),\n        minimum(permuted_differences),\n        maximum(permuted_differences),\n        quantile(permuted_differences, 0.025),\n        quantile(permuted_differences, 0.975)\n    ]\n)\n\nnull_summary\n\n6×2 DataFrame\n\n\n\nRow\nStatistic\nValue\n\n\n\nString\nFloat64\n\n\n\n\n1\nMean\n0.00119\n\n\n2\nStd Dev\n1.38007\n\n\n3\nMin\n-5.36889\n\n\n4\nMax\n5.14222\n\n\n5\n2.5th percentile\n-2.69333\n\n\n6\n97.5th percentile\n2.65778\n\n\n\n\n\n\nThe null distribution shows what group differences we’d expect if treatment assignment were random. Most permuted differences cluster around zero, as expected when there’s no true effect.\n\n\nVisualizing the results\nThe key insight comes from comparing our observed difference to the null distribution. How extreme is our observed value relative to what we’d expect by chance?\n\nfunction create_permutation_plot(null_distribution, observed_value)\n    \"\"\"Create visualization comparing observed value to null distribution\"\"\"\n    fig = Figure(size = (800, 500))\n    ax = Axis(fig[1, 1],\n        xlabel = \"Difference in means (Beer - Water)\",\n        ylabel = \"Density\",\n        title = \"Permutation test results\")\n\n    # Plot null distribution\n    hist!(ax, null_distribution, bins = 40, normalization = :pdf,\n        color = (:blue, 0.6), label = \"Null distribution\")\n\n    # Mark observed value\n    vlines!(ax, [observed_value], color = :red, linewidth = 3,\n        label = \"Observed difference\")\n\n    # Add text annotation\n    text!(ax, observed_value + 0.5, 0.15,\n        text = @sprintf(\"Observed\\n%.2f\", observed_value),\n        color = :red, fontsize = 12)\n\n    axislegend(ax, position = :rt)\n    return fig\nend\n\nfig_permutation = create_permutation_plot(permuted_differences, observed_diff)\nfig_permutation\n\n\n\n\n\n\n\n\nThe visualization clearly shows that our observed difference lies in the extreme tail of the null distribution. This suggests that the observed difference is unlikely to have occurred by chance alone.\n\n\nStatistical significance assessment\n\n# Calculate p-value: proportion of permuted differences ≥ observed\np_value_one_sided = mean(permuted_differences .&gt;= observed_diff)\np_value_two_sided = mean(abs.(permuted_differences) .&gt;= abs(observed_diff))\n\n# Effect size (Cohen's d)\npooled_std = sqrt(((length(beer_bites) - 1) * var(beer_bites) +\n                   (length(water_bites) - 1) * var(water_bites)) /\n                  (length(beer_bites) + length(water_bites) - 2))\ncohens_d = observed_diff / pooled_std\n\n# Summary results\ntest_results = DataFrame(\n    Measure = [\n        \"Observed difference\",\n        \"One-sided p-value\",\n        \"Two-sided p-value\",\n        \"Effect size (Cohen's d)\",\n        \"Interpretation\"\n    ],\n    Value = [\n        @sprintf(\"%.2f bites\", observed_diff),\n        @sprintf(\"%.4f\", p_value_one_sided),\n        @sprintf(\"%.4f\", p_value_two_sided),\n        @sprintf(\"%.2f\", cohens_d),\n        p_value_two_sided &lt; 0.05 ? \"Statistically significant\" : \"Not significant\"\n    ]\n)\n\ntest_results\n\n5×2 DataFrame\n\n\n\nRow\nMeasure\nValue\n\n\n\nString\nString\n\n\n\n\n1\nObserved difference\n4.38 bites\n\n\n2\nOne-sided p-value\n0.0003\n\n\n3\nTwo-sided p-value\n0.0008\n\n\n4\nEffect size (Cohen's d)\n1.11\n\n\n5\nInterpretation\nStatistically significant\n\n\n\n\n\n\n\n\nComparison with parametric testing\nFor completeness, we can compare our simulation-based results with traditional parametric tests.\n\n# Equal variance t-test\nt_test_equal = EqualVarianceTTest(beer_bites, water_bites)\n# Unequal variance t-test (Welch's t-test)\nt_test_unequal = UnequalVarianceTTest(beer_bites, water_bites)\n\n# Comparison summary\ncomparison_results = DataFrame(\n    Method = [\"Permutation test\", \"Equal variance t-test\", \"Unequal variance t-test\"],\n    P_value = [\n        p_value_two_sided,\n        pvalue(t_test_equal),\n        pvalue(t_test_unequal)\n    ],\n    Test_statistic = [\n        \"Difference in means\",\n        @sprintf(\"t = %.3f\", t_test_equal.t),\n        @sprintf(\"t = %.3f\", t_test_unequal.t)\n    ]\n)\n\ncomparison_results\n\n3×3 DataFrame\n\n\n\nRow\nMethod\nP_value\nTest_statistic\n\n\n\nString\nFloat64\nString\n\n\n\n\n1\nPermutation test\n0.00082\nDifference in means\n\n\n2\nEqual variance t-test\n0.000883127\nt = 3.587\n\n\n3\nUnequal variance t-test\n0.000747402\nt = 3.658\n\n\n\n\n\n\nAll three methods reach similar conclusions, but the permutation test requires fewer assumptions about the underlying data distribution.",
    "crumbs": [
      "**Computational Case Studies**",
      "Mosquito bites and beer consumption: simulation-based inference ✏️"
    ]
  },
  {
    "objectID": "notebooks/mosquitos.html#key-insights-and-broader-applications",
    "href": "notebooks/mosquitos.html#key-insights-and-broader-applications",
    "title": "Mosquito bites and beer consumption: simulation-based inference ✏️",
    "section": "Key insights and broader applications",
    "text": "Key insights and broader applications\nThis analysis demonstrates several important statistical concepts:\nSimulation-based inference: Complex statistical concepts become intuitive when approached through simulation rather than mathematical theory.\nAssumption-free testing: Permutation tests work without requiring specific distributional assumptions, making them robust for real-world data.\nP-value interpretation: The p-value directly represents the probability of observing such an extreme difference under the null hypothesis, clarified through simulation.\nEffect size matters: Statistical significance alone doesn’t indicate practical importance—effect size measures like Cohen’s d provide additional context.\n\nClimate science applications\nThese simulation-based methods prove particularly valuable in climate research where:\n\nNon-normal data: Climate variables often have skewed or heavy-tailed distributions\nSmall sample sizes: Paleoclimate records or extreme event counts may have limited observations\nComplex dependencies: Traditional parametric assumptions may not hold for climate time series\nPolicy decisions: Robust statistical inference supports high-stakes climate adaptation decisions\n\nThe computational approach demonstrated here scales naturally to more complex problems while maintaining the same intuitive logic: simulate what would happen under different assumptions and compare with observed data.",
    "crumbs": [
      "**Computational Case Studies**",
      "Mosquito bites and beer consumption: simulation-based inference ✏️"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References 🎯",
    "section": "",
    "text": "Abernathey, Ryan. 2024. An Introduction to\nEarth and Environmental Data Science. https://earth-env-data-science.github.io/intro.html.\n\n\nApplegate, Patrick, and Klaus Keller. 2015. Risk\nAnalysis in the Earth Sciences. Leanpub.\nhttps://leanpub.next/raes.\n\n\nBastani, Hamsa, Osbert Bastani, Alp Sungu, Haosen Ge, Özge Kabakcı, and\nRei Mariman. 2025. “Generative AI Without Guardrails\nCan Harm Learning: Evidence from High School\nMathematics.” Proceedings of the National Academy of\nSciences 122 (26): e2422633122. https://doi.org/10.1073/pnas.2422633122.\n\n\nBishop, Christopher M., and Hugh Bishop. 2024. Deep\nLearning: Foundations and\nConcepts. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-45468-4.\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to\nProbability, Second Edition. 2nd Edition.\nBoca Raton: Chapman and Hall/CRC. http://probabilitybook.net.\n\n\nColes, Stuart. 2001. An Introduction to Statistical Modeling of\nExtreme Values. Springer Series in Statistics. London: Springer.\n\n\nCressie, Noel A. C., and Christopher K. Wikle. 2011. Statistics for\nSpatio-Temporal Data. Hoboken, N.J.: Wiley.\n\n\nDowney, Allen B. 2021. Think Bayes. \"O’Reilly\nMedia, Inc.\". https://allendowney.github.io/ThinkBayes2/.\n\n\nFarnham, David J, James Doss-Gollin, and Upmanu Lall. 2018.\n“Regional Extreme Precipitation Events: Robust Inference from\nCredibly Simulated GCM Variables.” Water\nResources Research 54 (6). https://doi.org/10.1002/2017wr021318.\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The\nElements of Statistical Learning. Vol. 1.\nSpringer series in statistics Springer, Berlin.\n\n\nGelman, Andrew. 2021. Regression and Other Stories. Analytical\nMethods for Social Research. Cambridge, United Kingdom ; Cambridge\nUniversity Press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 2014.\nBayesian Data Analysis. 3rd ed. Chapman &\nHall/CRC Boca Raton, FL, USA.\n\n\nGhil, M, P Yiou, S Hallegatte, B D Malamud, P Naveau, A Soloviev, P\nFriederichs, et al. 2011. “Extreme Events: Dynamics, Statistics\nand Prediction.” Nonlinear Processes in Geophysics 18\n(3): 295–350. https://doi.org/10/fvzxvv.\n\n\nHelsel, Dennis R., Robert M. Hirsch, Karen R. Ryberg, Stacey A.\nArchfield, and Edward J. Gilroy. 2020. Statistical Methods in Water\nResources. Techniques and Methods. U.S. Geological Survey.\nhttps://doi.org/10.3133/tm4A3.\n\n\nHerman, Jon, and Will Usher. 2017. “SALib:\nAn Open-Source Python Library for\nSensitivity Analysis.” Journal of Open Source\nSoftware 2 (9): 97. https://doi.org/10.21105/joss.00097.\n\n\nJaynes, Edwin T. 2003. Probability Theory: The Logic of\nScience. New York, NY: Cambridge University Press.\n\n\nKosmyna, Nataliya, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ,\nXian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, and Pattie\nMaes. 2025. “Your Brain on ChatGPT:\nAccumulation of Cognitive Debt When\nUsing an AI Assistant for Essay Writing\nTask.” June 10, 2025. https://doi.org/10.48550/arXiv.2506.08872.\n\n\nLafferty, David C., and Ryan L. Sriver. 2023. “Downscaling and\nBias-Correction Contribute Considerable Uncertainty to Local Climate\nProjections in CMIP6.” Npj Climate and\nAtmospheric Science 6 (1, 1): 1–13. https://doi.org/10.1038/s41612-023-00486-0.\n\n\nLanzante, John R, Keith W Dixon, Mary Jo Nath, Carolyn E Whitlock, and\nDennis Adams-Smith. 2018. “Some Pitfalls in\nStatistical Downscaling of Future\nClimate.” Bulletin of the American Meteorological\nSociety 99 (4): 791–803. https://doi.org/10.1175/bams-d-17-0046.1.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. Second edition. Texts in Statistical Science\nSeries. Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nMerz, Bruno, Jeroen C J H Aerts, Karsten Arnbjerg-Nielsen, M Baldi, A\nBecker, A Bichet, Günter Blöschl, et al. 2014. “Floods and\nClimate: Emerging Perspectives for Flood Risk Assessment and\nManagement.” Natural Hazards and Earth System Science 14\n(7): 1921–42. https://doi.org/10/gb9nzm.\n\n\nMignan, Arnaud. 2024. Introduction to Catastrophe Risk\nModelling: A Physics-based\nApproach. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781009437370.\n\n\nMudelsee, Manfred. 2020. “Statistical Analysis of Climate Extremes\n/ Manfred Mudelsee.” In Statistical Analysis of\nClimate Extremes. Cambridge, United Kingdom ; Cambridge University\nPress.\n\n\nNaghettini, Mauro, ed. 2017. Fundamentals of Statistical\nHydrology. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-43561-9.\n\n\nPiironen, Juho, and Aki Vehtari. 2017. “Comparison of\nBayesian Predictive Methods for Model Selection.”\nStatistics and Computing 27 (3): 711–35. https://doi.org/10.1007/s11222-016-9649-y.\n\n\nPyrcz, Michael J. 2024. Applied Machine Learning in\nPython: A Hands-on Guide with\nCode. https://geostatsguy.github.io/MachineLearningDemos_Book.\n\n\nRackauckas, Christopher, Yingbo Ma, Julius Martensen, Collin Warner,\nKirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and Alan\nEdelman. 2020. “Universal Differential Equations for\nScientific Machine Learning.” 2020. https://doi.org/10.48550/ARXIV.2001.04385.\n\n\nSaltelli, Andrea, Marco Ratto, Terry Andres, Francesca Campolongo,\nJessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano\nTarantola. 2008. Global Sensitivity Analysis: The Primer. John\nWiley & Sons, Ltd. http://onlinelibrary.wiley.com/doi/abs/10.1002/9780470725184.ch1.\n\n\nThuerey, N., B. Holzschuh, P. Holl, G. Kohl, M. Lino, Q. Liu, P.\nSchnell, and F. Trost. 2024. Physics-Based Deep Learning. https://physicsbaseddeeplearning.org.",
    "crumbs": [
      "**References**"
    ]
  },
  {
    "objectID": "chapters/appendices/software.html",
    "href": "chapters/appendices/software.html",
    "title": "Appendix A — Software Setup ✏️",
    "section": "",
    "text": "A.1 Quick start\nIf you want to run the computational notebooks in this book, or apply a similar workflow, then these instructions are for you.\nThis section provides step-by-step instructions to get your development environment set up and running.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Software Setup ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/software.html#quick-start",
    "href": "chapters/appendices/software.html#quick-start",
    "title": "Appendix A — Software Setup ✏️",
    "section": "",
    "text": "A.1.1 Installation steps\n\nInstall Visual Studio Code - your code editor\n\nThere are other good IDEs out there, and you can absolutely use one.\nVS Code is a good and well-supported starting point\n\nInstall Quarto - for creating documents with code\n\nFor step 1, choose your operating system\nFor step 2, choose VS Code as your tool\n\nInstall Julia using JuliaUp - the programming language\n\nFollow the directions on the GitHub page based on your operating system\nDon’t worry about the Continuous Integration (CI) section or anything below it\nInstall Julia 1.11 using juliaup add 1.11\nSet this to be your default version using juliaup default 1.11\nYou should get a message that says something like Configured the default Julia version to be '1.11'\n\nIn VS Code: Install extensions from the Extensions marketplace\n\nInstall the Julia extension (provides syntax highlighting, code completion, and integrated REPL)\nInstall the Quarto extension (provides syntax highlighting and preview capabilities for .qmd files)\n\nInstall GitHub Desktop - for version control\n\nThis is optional if you prefer to use git through the command line or another app, but GitHub Desktop is a good default recommendation\n\n\n\n\nA.1.2 Verification\nAfter installation, you should be able to:\n\nOpen VS Code and see the Julia and Quarto extensions listed\nOpen a terminal and type julia to start the Julia REPL\nCreate a new Quarto document (.qmd file) in VS Code with syntax highlighting",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Software Setup ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/software.html#dig-deeper",
    "href": "chapters/appendices/software.html#dig-deeper",
    "title": "Appendix A — Software Setup ✏️",
    "section": "A.2 Dig deeper",
    "text": "A.2 Dig deeper\n\nA.2.1 Julia\nJulia is a fast, modern programming language designed for scientific computing. Its syntax closely mirrors mathematical notation, making it intuitive for researchers while delivering performance comparable to C and Fortran.\nJuliaUp is the official Julia version manager. It simplifies installation, allows you to maintain multiple Julia versions simultaneously, and keeps your installation current with the latest releases. This is especially useful as the Julia ecosystem evolves rapidly.\nSee the Julia page for more.\n\n\nA.2.2 Quarto\nQuarto is a scientific publishing system that enables you to combine code, results, and narrative text in reproducible documents. Think of it as the next generation of R Markdown, but with multi-language support (Julia, Python, R, and more).\nThis textbook is written in Quarto. Unlike traditional notebooks, Quarto documents are plain text files that render to multiple output formats (HTML, PDF, Word, presentations) while maintaining computational reproducibility.\nYou can learn more at:\n\nOfficial Tutorial: Hello, Quarto - basic document creation\nOfficial Tutorial: Computations - integrating code\nComprehensive Quarto documentation\n\n\nA.2.2.1 Writing with Markdown and math\nQuarto uses Markdown syntax with LaTeX math notation. Essential references:\n\nMarkdown Cheatsheet - basic text formatting\nLaTeX Cheatsheet - mathematical notation\nMathpix Snip - convert equation images to LaTeX code (free tier available)\nDetexify - draw symbols to find LaTeX commands\n\n\n\n\nA.2.3 Visual Studio Code\nVisual Studio Code is a free, open-source code editor developed by Microsoft. Its strength lies in its extensibility—thousands of extensions add language support, debugging capabilities, and productivity tools.\nFor our workflow, the Julia extension transforms VS Code into a full Julia development environment with syntax highlighting, intelligent code completion, integrated debugging, and a built-in REPL. The Quarto extension provides similar capabilities for computational documents, including live preview and cell execution.\nYou can learn more at the official tutorial.\n\n\nA.2.4 Git and GitHub\nGit is a distributed version control system that tracks changes in your code over time. GitHub is a cloud-based platform that hosts Git repositories and adds collaboration features like issue tracking, pull requests, and project management.\nVersion control is essential for reproducible research—it allows you to track changes, collaborate with others, recover from mistakes, and share your work publicly. This textbook itself is maintained on GitHub.\nYou can learn more at:\n\nGit and GitHub for Poets - beginner-friendly video series\nGitHub Hello World – official docs\nVersion Control - comprehensive guide from MIT’s “Missing Semester”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Software Setup ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/julia.html",
    "href": "chapters/appendices/julia.html",
    "title": "Appendix B — Julia Learning Resources ✏️",
    "section": "",
    "text": "B.1 Why Julia?\nThe computational examples in this textbook use the Julia programming language.\nJulia is a fast, modern, open-source programming language designed for scientific and numerical computing. The language is designed to be fast, dynamic, and easy to use and maintain.\nKey advantages for this textbook include:\nWhile Julia is powerful for computational thinking and research, many ecosystems remain stronger in other languages (like Python’s deep learning and climate data analysis tools), so a well-rounded programmer benefits from learning multiple languages.\nYou can read more about Julia’s design philosophy:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Julia Learning Resources ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/julia.html#why-julia",
    "href": "chapters/appendices/julia.html#why-julia",
    "title": "Appendix B — Julia Learning Resources ✏️",
    "section": "",
    "text": "High-Level Syntax: Julia has a clean and expressive syntax that closely parallels mathematical notation.\nPerformance: Julia compiles to efficient machine code, achieving speeds comparable to low-level languages like C and Fortran. This solves the “two-language problem,” where you might prototype in a high-level language but need to rewrite for performance.\nSimplified Dependencies: Eliminates or reduces the need for dependencies on C and Fortran libraries, which simplifies installation and maintenance.\nOpen-Source and Shareable: Julia is completely open-source with excellent package management for reproducible research environments.\nStrong Ecosystem: Despite being newer, Julia has a rapidly growing ecosystem of high-quality libraries for scientific domains.\n\n\n\n\nJulia Data Science textbook is didactic and clear\nWhy We Created Julia from the founders\nWhy Julia Manifesto is more comprehensive",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Julia Learning Resources ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/julia.html#learning-resources",
    "href": "chapters/appendices/julia.html#learning-resources",
    "title": "Appendix B — Julia Learning Resources ✏️",
    "section": "B.2 Learning resources",
    "text": "B.2 Learning resources\nThis textbook aims to never reinvent the wheel. There are lots of exceptional resources for learning Julia, or for learning computational concepts with Juila. Here are some favorites:\n\nMIT’s Introduction to Computational Thinking: Julia-based course covering applied mathematics and computational thinking\nJulia for Nervous Beginners: free course for people hesitant but curious about learning Julia\nJulia Data Science: comprehensive introduction to data science with Julia\nFastTrack to Julia cheatsheet\nComprehensive Julia Tutorials: YouTube playlist covering Julia topics\nMatlab-Python-Julia Cheatsheet: helpful if you’re experienced in one of these languages\n\n\nB.2.1 Specialized topics\nHere are some additional resources for specific Julia tools and packages developed in this class\n\nPlotting: Makie Tutorials and MakieCon 2023 YouTube Channel\nStatistical Modeling: Turing.jl tutorials has detailed examples of using Turing for modeling",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Julia Learning Resources ✏️</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/llm.html",
    "href": "chapters/appendices/llm.html",
    "title": "Appendix C — Large Language Models (“AI”) ✏️",
    "section": "",
    "text": "Coding is an integral part of real-world climate-risk analysis, and large language models (LLMs; often referred to as “AI” models) are rapidly changing how some kinds of coding happen. Beyond web-based chatbots, you may have useed tools like GitHub Copilot (free for students and educators) or Claude Code (see free Deeplearning.AI Course). LLMs use powerful new technologies that can support learning and replace tedious tasks, but they can also threaten your intellectual growth and skill development (Kosmyna et al. 2025; Bastani et al. 2025).\nIt is clear that there are some tasks that should be delegated to these models and some tasks that must remain human-driven. However, there are tremendous differences of opinion about how most tasks in the middle can or should be allocated. As you wrestle with these questions for yourself, you should explore resources like:\n\nAI Snake Oil is a blog that seeks to dispel hype, remove misconceptions, and clarify the limits of AI. The authors are in the Princeton University Department of Computer Science.\nAI software assistants make the hardest kinds of bugs to spot from Pluralistic is a thoughtful and deep blog post about the perils of (mis)using LLMs for coding.\nOne Useful Thing is a newsletter about AI focused on implications for work and education. The authors’ prompt library is also a good resource for working with LLMs.\nEd Zitron’s Where’s Your Ed At is a newsletter that takes a critical perspective on the business models and hype narratives around AI.\n\n\n\n\n\nBastani, Hamsa, Osbert Bastani, Alp Sungu, Haosen Ge, Özge Kabakcı, and Rei Mariman. 2025. “Generative AI Without Guardrails Can Harm Learning: Evidence from High School Mathematics.” Proceedings of the National Academy of Sciences 122 (26): e2422633122. https://doi.org/10.1073/pnas.2422633122.\n\n\nKosmyna, Nataliya, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, and Pattie Maes. 2025. “Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task.” June 10, 2025. https://doi.org/10.48550/arXiv.2506.08872.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Large Language Models (\"AI\") ✏️</span>"
    ]
  }
]