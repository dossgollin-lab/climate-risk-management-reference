---
title: "Julia ✏️"
---

The computational examples in this textbook use the Julia programming language.

## Why Julia?

Julia offers a unique combination of features that make it particularly well-suited for scientific and numerical computing. As outlined in the ["Why Julia? - A Manifesto"](https://github.com/Datseris/whyjulia-manifesto), the language is designed to be **fast**, **dynamic**, and **easy to use**, addressing many of the common challenges faced by researchers and developers.

### Key Advantages:

* **High-Level Syntax:** Like Python, R, and Matlab, Julia has a clean and expressive syntax that is readable to both humans and computers, and it closely parallels mathematical notation.
* **Performance:** Unlike those languages, however, Julia is fast. It compiles to efficient machine code, achieving speeds comparable to low-level languages like C and Fortran. This solves the "two-language problem," where a researcher might prototype a method in a high-level language but then need to rewrite it in a low-level one for performance.
* **Extensibility and Composability:** Julia's multiple dispatch paradigm allows for exceptional code re-use and seamless integration between different packages, making it easy to extend existing libraries.
* **Strong Ecosystem:** Despite being a newer language, Julia has a rapidly growing ecosystem of high-quality libraries for various scientific domains, from deep learning to climate data analysis.
* **Accessibility and Reproducibility:** Julia is straightforward to install and manage. Its built-in package manager simplifies the process of sharing and reproducing research environments, which is crucial for collaborative and verifiable science.

The methods discussed are applicable in other programming languages, such as Python, R, and C++. And while Julia is great, many relevant software ecosystems are stronger in other languages. For example, while Julia has great libraries for deep learning, industry and research tend to rely on the PyTorch/TensorFlow/JAX ecosystem and their Python interfaces. Similarly, while tooling for reading large climate data files in Julia is improving, the Python-based Pangeo ecosystem remains more mature. Therefore, while Julia is a powerful tool for computational thinking and research, a well-rounded programmer will likely benefit from learning other languages as well.
## Julia Resources

There are lots of great resources on programming and Julia.
Here is a curated list of some particularly helpful tools.

### Getting Started

* MIT's [Introduction to Computational Thinking](https://computationalthinking.mit.edu): a great Julia-based course at MIT covering applied mathematics and computational thinking
* [Julia for Nervous Begineers](https://juliaacademy.com/p/julia-programming-for-nervous-beginners): a free course on JuliaAcademy for people who are hesitant but curious about learning to write code in Julia. 
* [FastTrack to Julia cheatsheet](https://juliadocs.github.io/Julia-Cheat-Sheet/)
* [Comprehensive Julia Tutorials](https://www.youtube.com/playlist?list=PLCXbkShHt01seTlnlVg6O7f6jKGTguFi7): YouTube playlist covering a variety of Julia topics, starting with an introduciton to the language.
* [Matlab-Python-Julia Cheatsheet](https://cheatsheets.quantecon.org/): if you are experienced in one of these languages, this cheatsheet can help you learn the basics of Julia.

### Plotting with Makie

* [Makie Tutorials](https://docs.makie.org/stable/tutorials/)
* [MakieCon 2023 YouTube Channel](https://youtube.com/playlist?list=PLP8iPy9hna6TXEn99mhG5KaTgjsrCkDzQ&feature=shared)

### Climate Risk Analysis in Julia

- The documentation for the [Turing (Julia)](https://turing.ml/dev/tutorials/), [PyMC (Python)](https://www.pymc.io/projects/docs/en/v3/nb_examples/index.html), and (especially) [stan (multi-language)](https://mc-stan.org/users/documentation/) probabilistic programming languages offer outstanding tutorials on statistical modeling, and you can learn a lot by going through their examples and references.
- Extremes.jl

## Computational Efficiency Tips

### Caching MCMC Chains

Most Bayesian models in this textbook are computationally inexpensive to run.
However, for complex models or when sharing results with collaborators, you may want to save posterior samples to disk rather than regenerating them each time.

Here's a complete workflow for caching MCMC chains using HDF5 storage:

```julia
using Distributions
using DynamicHMC
using HDF5
using MCMCChains
using MCMCChainsStorage
using Random
using Turing

"""Write a MCMC Chain to disk"""
function write_chain(chain::MCMCChains.Chains, fname::AbstractString)
    mkpath(dirname(fname))
    HDF5.h5open(fname, "w") do f
        write(f, chain)
    end
end

"""Read a MCMCChain from disk"""
function read_chain(fname::AbstractString)
    HDF5.h5open(fname, "r") do f
        read(f, MCMCChains.Chains)
    end
end

"""User-facing interface"""
function get_posterior(
    model::DynamicPPL.Model, # the model to sample
    fname::String; # where to save it
    n_samples::Int=2_000, # number of samples per chain
    n_chains::Int=1, # how many chains to run?
    overwrite::Bool=false,
    kwargs...,
)
    # unless we're overwriting, try to load from file
    if !overwrite
        try
            samples = read_chain(fname)
            return samples
        catch
        end
    end

    # if we're here, we didn't want to or weren't able to
    # read the chain in from file. Generate the samples and
    # write them to disk.
    chn = let
        rng = Random.MersenneTwister(1041)
        sampler = externalsampler(DynamicHMC.NUTS())
        n_per_chain = n_samples
        nchains = n_chains
        sample(rng, model, sampler, MCMCThreads(), n_per_chain, nchains; kwargs...)
    end
    write_chain(chn, fname)
    return chn
end
```

**Example Usage:**

```julia
@model function BayesGEV(x)
    μ ~ Normal(0, 10)
    σ ~ InverseGamma(2, 3)
    ξ ~ Normal(0, 0.5)
    return x ~ Normal(μ, σ)
end

x = rand(GeneralizedExtremeValue(6, 1, 0.2), 100)
model = BayesGEV(x)

# First run: generates and saves samples
@time posterior = get_posterior(model, "bayes_gev.h5"; n_samples=10_000, n_chains=4)

# Second run: loads saved samples from disk (much faster)
@time posterior = get_posterior(model, "bayes_gev.h5"; n_samples=10_000, n_chains=4)
```

::: {.callout-tip}
## Don't commit `.h5` files

You generally shouldn't share `.h5` files in your repository since they can be large and your version history tracks changes.
Add `*.h5` to your `.gitignore` file to keep them out of version control.
:::

::: {.callout-tip}
## Alternative Approach

[`Arviz.jl`](https://arviz-devs.github.io/ArviZ.jl/stable/) offers a more sophisticated solution for posterior analysis and storage, but requires learning its own conventions.
:::

## Running the Notebooks

This course comes with a series of computational notebooks written in Julia.
