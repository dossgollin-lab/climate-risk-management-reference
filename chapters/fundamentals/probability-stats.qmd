---
title: Probability and Statistics ✏️
---

## Learning Objectives {.unnumbered}

After reading this chapter, you should be able to:

- Understand foundational probability concepts (random variables, distributions, moments).
- Apply descriptive and inferential statistical methods to climate-related datasets.
- Recognize when and why certain probability models are appropriate for climate variables.

## Probability

Probability is a mathematical framework for quantifying uncertainty.
In this textbook we will not get into measure theory.

### Axiomatic definition

- Non-negativity: $\Pr(A) \geq 0$ for any event $A$
- Normalization: $\Pr(S) = 1$ where $S$ is the sample space
- Additivity: $\Pr(A \cup B) = \Pr(A) + \Pr(B)$ for any two mutually exclusive events $A$ and $B$

### CDFs, PDFs, and PMFs

If $F_X$ is the *cumulative distribution function* (CDF) of $X$ and $f_X$ is the {{< glossary pdf >}} of $X$, then:
$$
F_X ( x ) = \int_{-\infty}^x f_X(u) \, du,
$$
and (if $f_X$ is continuous at $x$ which it typically will be)
$$
f_{X}(x)={\frac {d}{dx}}F_{X}(x).
$$
A useful property is
$$
\Pr[a\leq X\leq b]=\int _{a}^{b}f_{X}(x)\,dx
$$

::: {.callout-important}
We can only talk about the probability that $y$ is in some interval $[a, b]$, which is given by the integral of the PDF over that interval.
The probability that $y$ takes on the value $y^*$, written $p(y=y^*)$, is zero.
:::

- Discrete distributions (like the Poisson) have a *probability mass function* (PMF) instead of a PDF
- For PMFs, $p(y=y^*)$ is the probability that $y$ takes on the value $y^*$, and is defined

### Marginal, conditional, and joint distributions

Understanding relationships between multiple random variables is essential for climate risk modeling, where we often need to consider how different variables interact.

#### Marginal probability

Probability of event $A$: $\Pr(A)$

We write the marginal probability density function as:
$$
p(\theta) \quad \text{or} \quad p(y)
$$

#### Joint probability  

Probability of events $A$ and $B$ both occurring: $\Pr(A \cap B)$

The joint probability density function is written as:
$$
p(\theta, y)
$$

#### Conditional probability

Probability of event $A$ given that event $B$ has occurred: $\Pr(A | B)$

The conditional probability density function is:
$$
p(\theta | y) \quad \text{or} \quad p(y | \theta)
$$

#### Example: Two-Dice Wager

A gambler presents you with an even-money wager. You will roll two dice, and if the highest number showing is one, two, three or four, then you win. If the highest number on either die is five or six, then she wins. Should you take the bet?

This problem illustrates how joint and conditional probabilities help us evaluate complex scenarios involving multiple random events.

### Bayes' theorem

Bayes' theorem provides the fundamental relationship between joint, marginal, and conditional probabilities:

$$
p(\theta, y) = p(\theta) p(y | \theta)
$$

and thus:
$$
p(\theta | y) = \frac{p(\theta, y)}{p(y)} = \frac{p(\theta) p(y | \theta)}{p(y)}
$$

In many applications, we work with the proportional form:
$$
p(\theta | y) \propto p(\theta) p(y | \theta)
$$

This theorem is fundamental to Bayesian inference and updating beliefs based on new evidence.

### Example: Regression as a Conditional Distribution

Standard linear regression model, let's assume $x \in \mathbb{R}$ for simplicity (1 predictor):
$$
y_i = ax_i + b + \epsilon_i
$$
where $\epsilon_i \sim N(0, \sigma^2)$.

The conditional probability density of $y_i$ given $x_i$ is
$$
p(y_i | x_i, a, b, \sigma) = N(ax_i + b, \sigma^2)
$$
which is a shorthand for writing out the full equation for the Normal PDF.
We can (and often will) write this as
$$
y_i \sim \mathcal{N}(ax_i + b, \sigma^2)
$$
Finally, we will sometimes write $p(y_i | x_i)$ as a shorthand for $p(y_i | x_i, a, b, \sigma)$.
While fine in many circumstances, we should take care to make sure we are extremely clear about what parameters we are conditioning on.

The marginal probability density of $y_i$ is
$$
p(y_i | a, b, \sigma) = \int p(y_i | x_i, a, b, \sigma) p(x_i) \, dx_i
$$
where $p(x_i)$ is the probability density of $x_i$.

The joint probability density of $y_i$ and $x_i$ is
$$
p(y_i, x_i | a, b, \sigma) = p(y_i | x_i, a, b, \sigma) p(x_i)
$$
where $p(x_i)$ is the probability density of $x_i$.


## Statistics

### Expectations

### Descriptive statistics (mean, variance, quantiles)

#### Mean

The mean of a sample is the sample average:
$$
\bar{y} = \frac{1}{N} \sum_{i=1}^N y_i
$$

The mean of a distribution is the expected value:
$$
\mathbb{E}(u) = \int u p(u) \, du
$$

#### Variance

Variance measures how points differ from the mean. 

Sample variance:
$$
S^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n - 1}
$$

For a distribution:
$$
\mathbb{V}(u) = \int (u - \mathbb{E}(u))^2 p(u) \, du
$$
or, for a vector
$$
\mathbb{V}(u) = \int (u - \mathbb{E}(u)) (u - \mathbb{E}(u))^T p(u) \, du
$$

### Central Limit Theorem

The *central limit theorem* says that the sum of many independent random variables is approximately normally distributed.

```{julia}
#| code-fold: true
#| label: fig-sample-means
#| fig-cap: "Illustration of the central limit theorem"
using CairoMakie
using Distributions
using LaTeXStrings

dist = Gamma(2, 1)
N = 10_000 # number of samples
J = 300 # draws per sample
ȳ = [
    mean(rand(dist, J)) for i in 1:N
]
fig = hist(
    ȳ;
    bins=30,
    axis=(
        xlabel=L"$\bar{y}$",
    ),
    normalization=:probability,
    label="$N sample means"
)
vlines!(
    [mean(dist)],
    color=:red,
    linestyle=:dash,
    label="True mean",
    linewidth=2,
)
axislegend()
fig
```

### Common Probability Distributions

#### Normal Distribution

The Normal (Gaussian) distribution has probability density function:

$$
p(y | \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp
\left(
-\frac{1}{2}\left(
\frac{y-\mu}{\sigma}
\right)^{\!2}
\,
\right)
$$

- Mean $\mu$
- Variance $\sigma^2$ 
- Symmetric
- Often written as shorthand: $y \sim \mathcal{N}(\mu, \sigma^2)$

#### Bernoulli Distribution

Models a coin flip with probability $p$ of success.

```{julia}
p = 0.5 # probability of heads
rand(Bernoulli(p), 5)
```

#### Binomial Distribution

Models the number of successes in $n$ consecutive trials with probability $p$.

```{julia}
p = 0.5
N = 5
rand(Binomial(N, p), 5)
```

#### Multinomial Distribution

Extends the Binomial to multiple categories. Note that `p` is a *vector*.

```{julia}
p = [0.5, 0.3, 0.2]
N = 5
dist = Multinomial(N, p)
rand(dist, 5)
```

#### Poisson Distribution

Used to model count data. It is the limit of a Binomial distribution with $p=\lambda/N$, as $N \rightarrow \infty$.
A Poisson distribution has mean and variance equal to $\lambda$.

```{julia}
dist = Poisson(2.5)
rand(dist, 10)
```

#### Negative Binomial Distribution

The Negative Binomial distribution relaxes the Poisson's assumption that $\text{mean} = \text{variance}$.
This distribution models the number of successes before a specified number of failures occurs.

#### Mixture models

The Negative Binomial distribution (see last lecture) can be interpreted as a Gamma-Poisson *mixture*:

$$
\begin{aligned}
y &\sim \textrm{Poisson}(\lambda) \\
\lambda &\sim \textrm{Gamma}\left(r, \frac{p}{1-p} \right)
\end{aligned}
$$

We can show mathematically that if $y ~ \textrm{Negative Binomial}(r, p)$, that is equivalent to the mixture model $y ~ \textrm{Poisson}(\lambda)$ and $\lambda ~ \textrm{Gamma}(r, p / (1 - p))$.
$$
\begin{aligned}
& \int_0^{\infty} f_{\text {Poisson }(\lambda)}(y) \times f_{\operatorname{Gamma}\left(r, \frac{p}{1-p}\right)}(\lambda) \mathrm{d} \lambda \\
& = \int_0^{\infty} \frac{\lambda^y}{y !} e^{-\lambda} \times \frac{1}{\Gamma(r)}\left(\frac{p}{1-p} \lambda\right)^{r-1} e^{-\frac{p}{1-p} \lambda}\left(\frac{p}{1-p} \mathrm{~d} \lambda\right) \\
\ldots \\
&= f_{\text {Negative Binomial }(r, p)}(y)
\end{aligned}
$$
For all the steps see [Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture){target=_blank}.

### Hypothesis Testing

A common goal of statistical analysis is to quantify how consistent a set of observations is with a given hypothesis.

#### Case Study: Statistics Without the Agonizing Pain

See [the notebook](/notebooks/mosquitos.qmd).)

## Further Reading {.unnumbered}

If most of this content is unfamiliar to you, it may be worth reviewing a more comprehensive introduction to probability and statistics before proceeding.
While introductory statistics is often taught in an exhaustingly dry way, there are a few excellent resources

- **@blitzstein_probability:2019** rovides a thorough introduction to key concepts and ideas in probability. The book accompanies a free online course, [Stat 110](https://projects.iq.harvard.edu/stat110/home), which is a great resource for learning probability and statistics. Practice problems and solutions, handouts, and lecture videos are all available online.
- **@downey_thinkbayes:2021**  offers an introduction to Bayesian statistics using computational methods. It’s not focused on environmental variables or extremes but provides code and a clear explanation of core concepts.
- **@gelman_regression:2021** is an examples-centric introduction to probability, statistics, and statistical modeling. Andrew Gelman is the professor who got me passionate about statistics, and this is an excellent resource.