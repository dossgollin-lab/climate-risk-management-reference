---
title: Probability and inference ✏️

engine: julia
---


This chapter covers the fundamental concepts of probabilistic modeling and statistical inference.
Data analysis proceeds by building a generative model—a formal, probabilistic hypothesis about how data are created.
Inference is the inverse problem of using observed data to learn about model parameters.
Computational methods make complex inference problems tractable.

### Learning objectives {.unnumbered}

After reading this chapter, you should be able to:

- Build generative models using probability distributions.
- Apply maximum likelihood and Bayesian inference to estimate parameters.
- Use Monte Carlo methods when analytical solutions don't exist.
- Recognize when computational methods are required versus analytical approaches.

## Probability theory

The concepts in this section provide the mathematical language for describing uncertainty.
These mathematical tools support practical modeling applications.

### Basic concepts

#### Random variables

A random variable is a function that assigns numerical values to the outcomes of a random experiment.
Random variables provide the mathematical foundation for describing uncertainty.

- Discrete random variables take on countable values (e.g., number of floods per year)
- Continuous random variables take on uncountable values (e.g., temperature, precipitation amount)

#### Notation conventions

- Random variables: Capital letters ($X$, $Y$, $Z$)
- Realizations (specific values): Lowercase letters ($x$, $y$, $z$)
- Parameters: Greek letters ($\theta$, $\mu$, $\sigma$)
- Observed data: $y$ (following Bayesian convention)
- Predictions: $\tilde{y}$ (y-tilde)

### Distribution functions

The foundation of probability theory rests on three fundamental functions that describe random variables.

#### Probability Mass Function (PMF)

For discrete random variables, $P(X = x)$ gives the probability that the variable takes on a specific value $x$.
The PMF satisfies $\sum_x P(X = x) = 1$.

#### Probability Density Function (PDF)

For continuous random variables, $p(x)$ describes the relative likelihood of different values.

PDF $p(x)$ is not a probability but a **density**.
Since probability is density multiplied by a (potentially very small) interval of $x$, the value of $p(x)$ itself can exceed 1 without violating the laws of probability.
Probabilities are areas under the curve: $P(a \leq X \leq b) = \int_a^b p(x) \, dx$.
PDFs are sometimes written as $f(x)$ or $f_X(x)$ and must satisfy $\int_{-\infty}^{\infty} p(x) \, dx = 1$.

#### Cumulative Distribution Function (CDF)

$F(x) = P(X \leq x)$ gives the probability that a random variable is less than or equal to $x$.
The CDF is the most fundamental descriptor, defined for all random variables (discrete and continuous).
It unifies probability concepts and is essential for quantiles and return periods.

#### Quantile function

The quantile function $Q(p) = F^{-1}(p)$ is the inverse of the CDF.
It takes a probability $p \in [0,1]$ and returns the value $x$ such that $P(X \leq x) = p$.

For example, the median is $Q(0.5)$.
Then 99th percentile is $Q(0.99)$.

#### Examples of key distribution functions

These examples illustrate the relationships between probability density/mass functions, cumulative distribution functions, and quantiles using two fundamental distributions.
Each example shows both the forward operation (finding probabilities from values) and the inverse operation (finding values from probabilities).

The normal distribution demonstrates these concepts for continuous random variables, where probabilities correspond to areas under smooth curves.

{{< embed ../../notebooks/probability-stats-examples.qmd#normal-distribution-example >}}

The Poisson distribution shows the analogous concepts for discrete random variables, where probabilities correspond to point masses and CDFs are step functions.

{{< embed ../../notebooks/probability-stats-examples.qmd#poisson-distribution-example >}}

### Multiple variables

#### Joint, marginal, and conditional distributions

Real systems involve multiple random variables, requiring tools to describe their relationships.
This machinery allows construction of complex models from simpler components.

The **joint distribution** $p(x,y)$ for continuous or $P(X=x, Y=y)$ for discrete gives the probability of events occurring together.

The **marginal distribution** $p(x)$ or $P(X=x)$ gives the probability of an event, irrespective of other variables.
Calculated by summing or integrating over the other variables: $p(x) = \int p(x,y) \, dy$.

The **conditional distribution** $p(y \mid x)$ or $P(Y=y \mid X=x)$ gives the probability of an event given that another event has occurred.
Conditional distributions describe how variables depend on each other.

#### Visualizing joint, marginal, and conditional distributions

Understanding the relationships between joint, marginal, and conditional distributions becomes clearer with visualization.
The following example shows a bivariate normal distribution with marginal histograms and conditional distributions:

{{< embed ../../notebooks/probability-stats-examples.qmd#joint-marginal-conditional-example >}}

#### Independence

Two random variables $X$ and $Y$ are independent if their joint distribution is the product of their marginal distributions: $p(x,y) = p(x)p(y)$ for continuous variables, and $P(X=x, Y=y) = P(X=x)P(Y=y)$ for discrete variables.
For example, temperature and rainfall on a given day are typically **not** independent—hot days often have lower rainfall probability.

#### IID (Independent and Identically Distributed)

A sequence of random variables that are independent and have the same distribution.
Many statistical models assume IID data points, which enables powerful analytical and computational techniques.

#### Bayes' rule

The mechanical relationship between joint, marginal, and conditional distributions:

$$p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}$$

Bayes' rule is a consequence of the definition of conditional probability.
It becomes a tool for inference when interpreted probabilistically.

## Statistical foundations

This section provides the mathematical toolkit that underlies statistical inference.
These results justify why statistical methods work and enable practical computation.

### Summary statistics

#### Expectation (Expected Value)

The expectation is the formal definition of the quantity we approximate with the sample mean in a Monte Carlo simulation.
The expectation of a function $g(X)$ is:

$$\mathbb{E}[g(X)] = \int g(x) p(x) \, dx$$

for continuous variables, or 
$$\mathbb{E}[g(X)] = \sum_x g(x) P(X = x)$$

for discrete variables.

#### Moments of a distribution

Probability distributions are completely described by their PDF/PMF and CDF, but we often need summary statistics that capture essential properties.

- Mean: $\mu = \mathbb{E}[X]$ measures central tendency
- Variance: $\sigma^2 = \mathbb{E}[(X - \mu)^2]$ measures spread or scale; standard deviation is $\sigma = \sqrt{\sigma^2}$
- Higher-order moments: Skewness measures asymmetry, kurtosis measures tail weight

For certain heavy-tailed distributions, some higher-order moments (or even the variance) may not exist because the defining integrals diverge.

### Fundamental theorems

Two fundamental theorems provide the mathematical foundation for both statistical estimation and computational methods.
These results justify why statistical methods work and when we can trust their results.

#### Law of Large Numbers

Subject to mild conditions, the sample mean converges to the expected value as the number of samples increases:

$$\frac{1}{N} \sum_{i=1}^N X_i \to \mathbb{E}[X]$$

This theorem underlies both parameter estimation and Monte Carlo simulation.
It guarantees that maximum likelihood estimates become accurate with sufficient data, and that Monte Carlo approximations become precise with enough samples.

#### Central Limit Theorem

The distribution of a sample mean approaches a Normal distribution as the sample size increases, regardless of the underlying distribution shape:

$$\frac{\sqrt{N}(\bar{X} - \mu)}{\sigma} \to N(0,1)$$

where $\bar{X}$ is the sample mean, $\mu = \mathbb{E}[X]$, and $\sigma^2 = \text{Var}(X)$.

This enables uncertainty quantification through confidence intervals and justifies the widespread use of normal approximations in statistical inference.
The CLT explains why many phenomena follow normal distributions—they arise from sums of many small, independent effects.

### Monte Carlo Expectations

Most decision-relevant quantities can be expressed as expectations.
When analytical calculation is impossible, simulation provides a practical approximation method.

The basic Monte Carlo estimate of an expectation is:

$$\mathbb{E}[g(X)] \approx \frac{1}{N} \sum_{i=1}^N g(x_i)$$

where $x_1, x_2, \ldots, x_N$ are independent samples from the distribution of $X$.
More sophisticated methods (importance sampling, MCMC) exist for drawing samples from more complex distributions.

The Law of Large Numbers guarantees convergence, while the Central Limit Theorem provides the convergence rate.
The Monte Carlo standard error is approximately $\sigma/\sqrt{N}$, where $\sigma$ is the standard deviation of $g(X)$.
This means that to halve the error, we need four times as many samples.

Monte Carlo methods become essential when dealing with high-dimensional integrals that arise in Bayesian inference and uncertainty propagation through complex models.

### Transformation of variables

A core task in probabilistic modeling is understanding how randomness propagates through a system.
If we have a random variable $X$ with PDF $p_X(x)$ and create $Y = g(X)$, what is the PDF of $Y$?

Simply substituting $x = g^{-1}(y)$ in the original PDF is incorrect because functions can stretch or compress the probability space.
We must account for this distortion.

The general change of variables formula is:

$$p_Y(y) = p_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$

The term $\left| \frac{d}{dy} g^{-1}(y) \right|$ is the Jacobian—a "stretching factor" ensuring probability mass is conserved.
When a function stretches a region, density decreases proportionally to keep total probability equal to 1.
When it compresses a region, density increases.

This formula derives from working through CDFs and applying the chain rule, but the key insight is that transformations distort the coordinate system and we must adjust densities accordingly.


## Likelihood and maximum likelihood estimation

The probability theory and statistical foundations we've covered provide the mathematical language for uncertainty and the tools for computation.
We now turn from describing uncertainty to learning from data.
The first major approach to statistical inference connects data to parameters through likelihood functions and optimization.

### The likelihood function

The central tool for connecting data to parameters is the likelihood function.
The likelihood is the conditional probability $p(y \mid \theta)$, where $y$ represents our observed data.

#### Definition

The likelihood tells us how likely we are to see the observed data $y$ for some value of the parameters $\theta$.

#### Crucial distinction

The likelihood is not the probability of the parameters.
It's the probability (or probability density) of the data given the parameters.

This confusion is common: $p(\text{data}|\text{parameters})$ tells us about data likelihood, not parameter probability.
`MLE` provides point estimates of the most likely parameter values, while Bayesian inference provides probability distributions over parameters.
Only Bayesian inference gives us $p(\text{parameters}|\text{data})$.

For continuous variables, since we're dealing with a density, the probability of getting exactly that value is zero, but the probability of getting near it is the integral of the PDF over a small interval.

#### Independence and the product form

If we assume our data points are independent and identically distributed (IID), then by the definition of independence:
$$p(y_1, y_2, \ldots, y_n \mid \theta) = \prod_{i=1}^n p(y_i \mid \theta)$$

#### The log-likelihood

Products are numerically unstable and difficult to work with.
Since the logarithm is monotonic, 
$$
\arg \max_\theta p(y \mid \theta) = \arg \max_\theta \log p(y \mid \theta)
$$
although
$$
\max_\theta p(y \mid \theta) \neq \max_\theta \log p(y \mid \theta)
$$
in general.

For independent data, this gives us:
$$
\log p(y \mid \theta) = \sum_{i=1}^n \log p(y_i \mid \theta)
$$

### Maximum likelihood estimation

Maximum likelihood estimation (MLE) finds the parameter values that maximize the likelihood function:
$$\hat{\theta}_{\text{MLE}} = \arg\max_\theta p(y \mid \theta)$$

#### Why maximum likelihood makes sense

The likelihood function $p(y \mid \theta)$ gives the probability of observed data under different parameter values.
Maximum likelihood estimation finds the parameter values that maximize the probability of the observed data.
The approach selects parameters that best explain the observations.

In practical applications, `MLE` estimates parameters of distributions describing observed phenomena by finding values that maximize the probability of historical observations.
The estimates inform subsequent analysis and decision-making.

#### Implementation

We find the actual parameter values using optimization approaches.
This may involve analytical differentiation (setting derivatives to zero) or numerical optimization methods when closed-form solutions don't exist.

This reframes the statistical problem of inference as a numerical problem of optimization.

#### Properties and theoretical foundations

Understanding when and why MLE works requires defining estimator quality.
An estimator should be consistent (converge to the true value as sample size increases), efficient (achieve low variance), and unbiased (correct on average).

Under regularity conditions, MLE estimators have desirable asymptotic properties.
As the sample size $n$ grows large, the MLE estimator $\hat{\theta}_{\text{MLE}}$ becomes consistent—it converges to the true parameter value $\theta_0$.

#### Computational considerations

Finding maximum likelihood estimates requires different approaches depending on the complexity of the model.

Analytical solutions exist when we can solve 
$$
\frac{d}{d\theta} \log p(y \mid \theta) = 0
$$
in closed form.
This works for simple models like Normal distributions with known variance, or the coin flip example we examine below.
These cases provide valuable intuition and serve as building blocks for more complex problems.

Numerical optimization becomes necessary when no closed-form solution exists.
Practical challenges arise in applications.
The likelihood surface may contain multiple local maxima, requiring different starting values to find the global optimum.
Numerical stability requires working with log-likelihoods rather than products of small probabilities.
Flat likelihood surfaces indicate that data contain limited information about parameters.
All methods assume correct model specification -- poor model approximations yield misleading results regardless of optimization quality.

### Example: coin flip maximum likelihood estimation

The coin flip example demonstrates both analytical MLE derivation and the principles behind maximum likelihood estimation.

A series of coin flips are independent Bernoulli trials with fixed probability $\theta$ of heads.
Given $y$ heads in $n$ flips, we want to estimate $\theta$ using maximum likelihood.

**Likelihood function**: For $n$ independent coin flips, the likelihood follows the Binomial distribution:
$$
p(y \mid \theta, n) = \binom{n}{y} \theta^y (1-\theta)^{n-y}
$$
where $\binom{n}{y} = \frac{n!}{y!(n-y)!}$ is the binomial coefficient.

Since we condition on the observed data, the binomial coefficient is constant for inference purposes:

$$p(y \mid \theta, n) \propto \theta^y (1-\theta)^{n-y}$$

**Analytical MLE derivation**: We maximize the log-likelihood:

$$\log p(y \mid \theta) = y \log \theta + (n-y) \log(1-\theta) + \text{const.}$$

Taking the derivative and setting to zero:

$$\frac{d}{d\theta} \log p(y \mid \theta) = \frac{y}{\theta} - \frac{n-y}{1-\theta} = 0$$

Solving: $\frac{y}{\theta} = \frac{n-y}{1-\theta} \implies \hat{\theta}_{\text{MLE}} = \frac{y}{n}$

The maximum likelihood estimate is simply the observed proportion of heads.
This intuitive result demonstrates how MLE selects parameters that make the observed data most probable.

{{< embed ../../notebooks/probability-stats-examples.qmd#coin-flip-likelihood >}}

{{< embed ../../notebooks/probability-stats-examples.qmd#coin-flip-mle >}}

Just as we infer $\theta$ from coin flips, climate scientists use the same principles to estimate the probability of extreme events from historical data.

### Linear regression example

Linear regression extends the coin flip example to multiple parameters, demonstrating both analytical solutions and the connection between probabilistic and optimization approaches.

Given $n$ observations of response variable $y_i$ and predictor variable $x_i$, the goal is to understand the relationship between $x$ and $y$.

In linear regression, we assume that the response follows a linear relationship with added noise:
$$
y_i = a + b x_i + \epsilon_i
$$
where:

- $a$ is the intercept (value of $y$ when $x = 0$)
- $b$ is the slope (change in $y$ per unit change in $x$)
- $\epsilon_i \sim \text{Normal}(0, \sigma^2)$ is independent random noise

We can equivalently write this as
$$
y_i \mid x_i, a, b, \sigma^2 \sim \text{Normal}(a + b x_i, \sigma^2).
$$

#### Maximum likelihood solution

The likelihood for a single observation is given by the Normal PDF:
$$
p(y_i \mid x_i, a, b, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{(y_i - a - b x_i)^2}{2\sigma^2}\right\}
$$
By independence, the likelihood for all observations is:
$$
p(\mathbf{y} \mid \mathbf{x}, a, b, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{(y_i - a - b x_i)^2}{2\sigma^2}\right\}
$$
and the log-likelihood is:
$$
\log p(\mathbf{y} \mid \mathbf{x}, a, b, \sigma^2) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - a - b x_i)^2.
$$
If we take the derivative of the log-likelihood with respect to $a$ and $b$ and set it equal to zero, we find that we need to minimize:

$$\sum_{i=1}^n (y_i - a - b x_i)^2$$

This is equivalent to minimizing the mean squared error between the observed and predicted values.
This illustrates how "ordinary least squares" regression can be derived from a probabilistic model using maximum likelihood estimation or from an optimization perspective by minimizing squared errors.

The following examples show both the curve fitting and MLE approaches:

{{< embed ../../notebooks/probability-stats-examples.qmd#linear-regression-data-plot >}}

{{< embed ../../notebooks/probability-stats-examples.qmd#linear-regression-curve-fitting >}}

{{< embed ../../notebooks/probability-stats-examples.qmd#linear-regression-mle >}}

Similar approaches can model relationships between climate variables and their impacts.

## Bayesian inference

Maximum likelihood estimation provides point estimates of parameters by finding values that maximize the probability of observed data.
Bayesian inference takes a fundamentally different approach: it treats parameters as random variables and computes full probability distributions that quantify uncertainty.
In other words, rather than searching for $\hat{\theta}$ that maximizes the likelihood, Bayesian inference seeks to estimate the entire distribution $p(\theta \mid y)$.

### Motivation and overview

Real-world risk assessment relies on multiple, imperfect data sources: short instrumental records, longer regional records, qualitative historical accounts, and physical constraints from models.
Traditional statistical methods often struggle to formally integrate these different types of information into a single analysis.
The Bayesian framework provides a principled solution by treating all knowledge—both prior beliefs and new data—as probability distributions that can be mathematically combined.

The core relationship is Bayes' rule:
$$
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)}
$$

This deceptively simple equation describes how we update our beliefs:

- **Prior distribution** $p(\theta)$: Quantifies existing knowledge about parameters before analyzing the current dataset
- **Likelihood function** $p(y \mid \theta)$: The engine for learning from data (identical to the likelihood used in maximum likelihood estimation)
- **Posterior distribution** $p(\theta \mid y)$: Our updated beliefs after combining prior knowledge with observed data
- **Marginal likelihood** $p(y)$: A normalizing constant ensuring the posterior integrates to 1

Since $p(y)$ doesn't depend on $\theta$ for a fixed dataset, we often write:
$$
p(\theta \mid y) \propto p(y \mid \theta) p(\theta)
$$

The result is fundamentally different from maximum likelihood estimation: instead of a single "best" parameter estimate, we obtain a full probability distribution that naturally quantifies uncertainty.
This enables direct probabilistic statements like "there is a 95% probability that the parameter lies between these values."

### Maximum A Posteriori: a bridge to optimization

Before exploring full Bayesian inference, we can find the single most probable parameter value given the data.
Maximum A Posteriori (MAP) estimation finds the mode of the posterior distribution:

$$\hat{\theta}_{\text{MAP}} = \arg\max_\theta p(\theta \mid y) = \arg\max_\theta p(y \mid \theta) p(\theta)$$

Taking logarithms (since log is monotonic):

$$\hat{\theta}_{\text{MAP}} = \arg\max_\theta [\log p(y \mid \theta) + \log p(\theta)]$$

This reveals an elegant connection to machine learning and optimization.
The log-posterior decomposes into the familiar log-likelihood plus a log-prior term.
The log-prior acts as a regularization penalty, preventing overfitting by favoring certain parameter values.

When the prior is uniform (non-informative), the log-prior is constant and MAP reduces to maximum likelihood estimation.
When the prior is informative, it regularizes the estimate by pulling it toward prior beliefs.
This is mathematically identical to penalized likelihood methods like Ridge regression (with Gaussian priors) or Lasso regression (with Laplace priors).

However, MAP provides only a point estimate and discards uncertainty information.
To fully leverage the Bayesian framework, we need the entire posterior distribution.

### Analytic solutions and conjugate priors

In special cases, we can compute the posterior distribution analytically using conjugate priors.
A prior is conjugate to a likelihood if the posterior has the same functional form as the prior.
This mathematical convenience allows us to update our beliefs with a simple algebraic formula.

The coin flip example demonstrates this perfectly.
For the Binomial likelihood, the Beta distribution is conjugate.
To see why, let's work through the mathematics.

Starting with our prior and likelihood:

- **Prior**: $\theta \sim \text{Beta}(\alpha, \beta)$ with PDF $p(\theta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}$
- **Likelihood**: $y \mid \theta \sim \text{Binomial}(n, \theta)$ with $p(y \mid \theta) \propto \theta^y(1-\theta)^{n-y}$

The posterior is proportional to the product of prior and likelihood:
$$
p(\theta \mid y) \propto p(y \mid \theta) \cdot p(\theta) \propto \theta^y(1-\theta)^{n-y} \cdot \theta^{\alpha-1}(1-\theta)^{\beta-1}
$$
Combining the powers:
$$
p(\theta \mid y) \propto \theta^{(\alpha + y) - 1}(1-\theta)^{(\beta + n - y) - 1}
$$
This is exactly the kernel of a $\text{Beta}(\alpha + y, \beta + n - y)$ distribution!
Therefore:
$$
\theta \mid y \sim \text{Beta}(\alpha + y, \beta + n - y)
$$

The posterior parameters are intuitive: prior "successes" ($\alpha$) plus observed successes ($y$), and prior "failures" ($\beta$) plus observed failures ($n-y$).
This demonstrates the core Bayesian principle: new data updates our beliefs in a mathematically principled way.

The following example shows this updating process in action, demonstrating how the posterior distribution evolves with each new coin flip:

{{< embed ../../notebooks/probability-stats-examples.qmd#beta-binomial-updating >}}

As data accumulate, the influence of the prior diminishes relative to that of the likelihood.
With sufficient data, Bayesian and maximum likelihood estimates converge regardless of the prior choice.

In practice, conjugate priors exist for only a limited set of models.

### Markov Chain Monte Carlo

For any non-trivial model, analytically computing the posterior distribution becomes mathematically intractable.
A brute-force approach of evaluating the posterior on a grid fails catastrophically: with $k$ parameters and $n$ grid points per parameter, we need $n^k$ evaluations.
For even modest problems (say, 10 parameters with 100 grid points each), this requires $100^{10} = 10^{20}$ calculations—computationally impossible.

This "curse of dimensionality" means that analytical approaches work only for the simplest models.
Real scientific applications require computational methods.

Markov Chain Monte Carlo methods provide a computational workaround.
Instead of calculating the posterior distribution everywhere, `MCMC` algorithms generate samples from it.

An `MCMC` algorithm creates a Markov chain: a sequence of parameter values where each new value depends only on the current value.
The algorithm is carefully designed so that the chain's stationary distribution is the target posterior distribution.
This means the algorithm spends time in different regions of parameter space in proportion to their posterior probability.

The key insight is that we only need the posterior up to a constant of proportionality: $p(\theta \mid y) \propto p(y \mid \theta) p(\theta)$.
Since `MCMC` algorithms don't require the normalizing constant, they can handle arbitrarily complex models.

After running the chain for thousands of iterations, the resulting samples serve as a high-fidelity numerical approximation of the true posterior distribution.
We can use these samples to approximate any posterior quantity:
$$
\mathbb{E}[g(\theta) \mid y] \approx \frac{1}{N} \sum_{i=1}^N g(\theta^{(i)})
$$
where $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(N)}$ are our `MCMC` samples.
Modern `MCMC` algorithms include:

- **Metropolis-Hastings**: A general-purpose algorithm using proposal distributions and acceptance criteria
- **Gibbs sampling**: Efficient for multivariate problems with available conditional distributions
- **Hamiltonian Monte Carlo**: Uses gradient information for efficient exploration, forming the basis of modern probabilistic programming frameworks like `Turing.jl` and `stan`

Many other modern approaches to sampling exist.

### Example: coin flip Bayesian analysis

The coin flip example demonstrates the complete Bayesian workflow, from prior specification through posterior computation and interpretation.

Building on the maximum likelihood analysis from the previous section, we now treat the parameter $\theta$ as a random variable.
This is a helpful example, because we can compare to the known and exact analytical solution to validate the computational approach.

Starting with a $\text{Beta}(1,1)$ uniform prior (representing no initial preference for heads or tails), observing 7 heads in 10 flips yields the posterior $\text{Beta}(8,4)$.
This analytical result can be compared directly with `MCMC` sampling to demonstrate computational accuracy:

{{< embed ../../notebooks/probability-stats-examples.qmd#coin-flip-bayesian >}}

The analytical and `MCMC` results match closely, demonstrating that:

- The Beta-Binomial conjugacy gives exact results
- `MCMC` provides a general computational approach when analytical solutions don't exist
- Both approaches quantify uncertainty through the full posterior distribution
- Increasing the number of flips improves the approximation of the posterior

`MCMC` accuracy varies across the distribution.
The method provides better estimates in the center of the distribution than in the tails.
For example, computing $\mathbb{E}[\theta]$ requires fewer samples than computing the probability that $\theta \leq 0.25$, since the latter depends on accurately characterizing the probability mass in the tail.
This reflects a general principle: quantities that depend on rare events or extreme values require more samples and/or more sophisticated sampling strategies for accurate estimation.

### Example: linear regression Bayesian analysis

Linear regression demonstrates Bayesian inference for multivariate problems where analytical solutions don't exist.
We specify prior distributions for all parameters ($a$, $b$, $\sigma$) and use `MCMC` to sample from the joint posterior distribution.

The Bayesian approach provides full posterior distributions for each parameter, enabling:
- Direct probabilistic statements about parameter values
- Credible intervals that quantify parameter uncertainty
- Prediction intervals that account for both parameter and observation uncertainty

The following example shows the complete Bayesian workflow and compares results with maximum likelihood estimation:

{{< embed ../../notebooks/probability-stats-examples.qmd#linear-regression-bayesian >}}

{{< embed ../../notebooks/probability-stats-examples.qmd#linear-regression-comparison >}}

All three approaches (least squares, `MLE`, Bayesian) provide similar point estimates, but differ in their treatment of uncertainty:
- Least squares provides point estimates only
- `MLE` provides point estimates with asymptotic standard errors
- Bayesian inference provides full posterior distributions and credible intervals

Notice that the key difference between `MLE` and Bayesian estimates is the influence of prior information.
When priors are weak (uninformative), Bayesian and `MLE` estimates converge.
When priors are informative, they pull the estimates toward the prior beliefs, demonstrating how Bayesian inference formally incorporates existing knowledge.

The Bayesian approach excels when we need to propagate parameter uncertainty through subsequent calculations or decision processes.
The credible interval demonstrates how parameter uncertainty translates into prediction uncertainty—a crucial consideration for risk assessment and decision-making under uncertainty.

Just as we infer $\theta$ from coin flips, climate scientists estimate the probability of extreme events from historical data.
The same Bayesian principles apply whether analyzing controlled experiments or complex climate observations, providing a unified framework for uncertainty quantification across scales and applications.

## Further reading {.unnumbered}

For deeper study of probability and statistics:

- @blitzstein_probability:2019 provides excellent intuition with computational examples
- @downey_thinkbayes:2021 emphasizes Bayesian thinking with practical applications
- @gelman_regression:2021 connects regression to broader statistical modeling
- @gelman_bda3:2014 comprehensive treatment of Bayesian computation
- [Computational Examples](../../notebooks/probability-stats-examples.qmd) demonstrates all methods with working code