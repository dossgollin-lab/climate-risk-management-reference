---
title: Probability and Statistics
---

::: {.callout-note}
## Under Construction
This chapter is still under active construction. Please check back regularly for updates and new content.
:::

This textbook assumes that you have taken not only an introductory course in probability and statistics, but also some more advanced courses on statistics, machine learning, or data analysis.
For this reason, this chapter will briefly summarize some of the key concepts in probability theory that we will use throughout the book, but is by no means a comprehensive reference.

::: {.callout-tip}
## Learning Objectives

- Understand foundational probability concepts (random variables, distributions, moments).
- Apply descriptive and inferential statistical methods to climate-related datasets.
- Recognize when and why certain probability models are appropriate for climate variables.
:::

## Probability

Probability is a mathematical framework for quantifying uncertainty.
In this textbook we will not get into measure theory.

### Axiomatic definition

- Non-negativity: $\Pr(A) \geq 0$ for any event $A$
- Normalization: $\Pr(S) = 1$ where $S$ is the sample space
- Additivity: $\Pr(A \cup B) = \Pr(A) + \Pr(B)$ for any two mutually exclusive events $A$ and $B$

### CDFs, PDFs, and PMFs

If $F_X$ is the *cumulative distribution function* (CDF) of $X$ and $f_X$ is the {{< acr pdf >}} of $X$, then:
$$
F_X ( x ) = \int_{-\infty}^x f_X(u) \, du,
$$
and (if $f_X$ is continuous at $x$ which it typically will be)
$$
f_{X}(x)={\frac {d}{dx}}F_{X}(x).
$$
A useful property is
$$
\Pr[a\leq X\leq b]=\int _{a}^{b}f_{X}(x)\,dx
$$

::: {.callout-important}
We can only talk about the probability that $y$ is in some interval $[a, b]$, which is given by the integral of the PDF over that interval.
The probability that $y$ takes on the value $y^*$, written $p(y=y^*)$, is zero.
:::

- Discrete distributions (like the Poisson) have a *probability mass function* (PMF) instead of a PDF
- For PMFs, $p(y=y^*)$ is the probability that $y$ takes on the value $y^*$, and is defined

### Marginal, conditional, and joint distributions

#### Marginal probability

Probability of event $A$: $\Pr(A)$


We will write the marginal probability density function as
$$
p(\theta) \quad \text{or} \quad p(y)
$$

#### Joint probability

Probability of events $A$ and $B$: $\Pr(A  \& B)$

$$
p(\theta, y)
$$

#### Conditional probability

Probability of event $A$ given event $B$: $\Pr(A | B)$

$$
p(\theta | y) \quad \text{or} \quad p(y | \theta)
$$


### Bayes' theorem


$$
p(\theta, y) = p(\theta) p(y | \theta)
$$
and thus
$$
p(\theta | y) = \frac{p(\theta, y)}{p(y)} = \frac{p(\theta) p(y | \theta)}{p(y)}
$$
generally:
$$
p(\theta | y) \propto p(\theta) p(y | \theta)
$$

### Example: Regression as a Conditional Distribution

Standard linear regression model, let's assume $x \in \mathbb{R}$ for simplicity (1 predictor):
$$
y_i = ax_i + b + \epsilon_i
$$
where $\epsilon_i \sim N(0, \sigma^2)$.

The conditional probability density of $y_i$ given $x_i$ is
$$
p(y_i | x_i, a, b, \sigma) = N(ax_i + b, \sigma^2)
$$
which is a shorthand for writing out the full equation for the Normal PDF.
We can (and often will) write this as
$$
y_i \sim \mathcal{N}(ax_i + b, \sigma^2)
$$
Finally, we will sometimes write $p(y_i | x_i)$ as a shorthand for $p(y_i | x_i, a, b, \sigma)$.
While fine in many circumstances, we should take care to make sure we are extremely clear about what parameters we are conditioning on.

The marginal probability density of $y_i$ is
$$
p(y_i | a, b, \sigma) = \int p(y_i | x_i, a, b, \sigma) p(x_i) \, dx_i
$$
where $p(x_i)$ is the probability density of $x_i$.

The joint probability density of $y_i$ and $x_i$ is
$$
p(y_i, x_i | a, b, \sigma) = p(y_i | x_i, a, b, \sigma) p(x_i)
$$
where $p(x_i)$ is the probability density of $x_i$.


## Statistics

### Expectations

### Descriptive statistics (mean, variance, quantiles)

### Central Limit Theorem

### Common Probability Distributions

#### Mixture models

The Negative Binomial distribution (see last lecture) can be interpreted as a Gamma-Poisson *mixture*:

$$
\begin{aligned}
y &\sim \textrm{Poisson}(\lambda) \\
\lambda &\sim \textrm{Gamma}\left(r, \frac{p}{1-p} \right)
\end{aligned}
$$

We can show mathematically that if $y ~ \textrm{Negative Binomial}(r, p)$, that is equivalent to the mixture model $y ~ \textrm{Poisson}(\lambda)$ and $\lambda ~ \textrm{Gamma}(r, p / (1 - p))$.
$$
\begin{aligned}
& \int_0^{\infty} f_{\text {Poisson }(\lambda)}(y) \times f_{\operatorname{Gamma}\left(r, \frac{p}{1-p}\right)}(\lambda) \mathrm{d} \lambda \\
& = \int_0^{\infty} \frac{\lambda^y}{y !} e^{-\lambda} \times \frac{1}{\Gamma(r)}\left(\frac{p}{1-p} \lambda\right)^{r-1} e^{-\frac{p}{1-p} \lambda}\left(\frac{p}{1-p} \mathrm{~d} \lambda\right) \\
\ldots \\
&= f_{\text {Negative Binomial }(r, p)}(y)
\end{aligned}
$$
For all the steps see [Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture){target=_blank}.

### Hypothesis Testing

A common goal of statistical analysis is to quantify how consistent a set of observations is with a given hypothesis.

#### Case Study: Statistics Without the Agonizing Pain

See [the notebook](/notebooks/mosquitos.qmd).)

## Further Reading {.unnumbered}

If most of this content is unfamiliar to you, it may be worth reviewing a more comprehensive introduction to probability and statistics before proceeding.
While introductory statistics is often taught in an exhaustingly dry way, there are a few excellent resources

- **@blitzstein_probability:2019** rovides a thorough introduction to key concepts and ideas in probability. The book accompanies a free online course, [Stat 110](https://projects.iq.harvard.edu/stat110/home), which is a great resource for learning probability and statistics. Practice problems and solutions, handouts, and lecture videos are all available online.
- **@downey_thinkbayes:2021**  offers an introduction to Bayesian statistics using computational methods. Itâ€™s not focused on environmental variables or extremes but provides code and a clear explanation of core concepts.
- **@gelman_regression:2021** is an examples-centric introduction to probability, statistics, and statistical modeling. Andrew Gelman is the professor who got me passionate about statistics, and this is an excellent resource.