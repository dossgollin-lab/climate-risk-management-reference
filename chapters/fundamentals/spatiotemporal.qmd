---
title: Spatiotemporal Processes ✏️
status: "draft"
---

## See First {.unnumbered}

This chapter builds on concepts from:
- [Probability and Statistics](/chapters/fundamentals/probability-stats.qmd)
- [Machine Learning and Nonparametric Methods](/chapters/fundamentals/ml-nonparametric.qmd)

## Learning Objectives {.unnumbered}

- Model and interpret spatial dependence in climate fields
- Perform kriging and other geostatistical interpolations for mapping hazards
- Understand the fundamentals of spatial autocorrelation and variogram analysis
- Apply time series analysis methods to detect trends and patterns in climate data
- Use dimension reduction techniques for high-dimensional climate datasets
- Integrate spatial and temporal methods for spatiotemporal climate analysis

## Spatial Statistics

### Spatial Autocorrelation and Variograms

### Interpolation Methods

### Kriging

- (ordinary, universal)

### Gaussian Processes in 1D

### Gaussian Processes in Multiple Dimensions

## Time Series Analysis

### Autocorrelation Functions

### Frequency Analysis

### Wavelets

### Autoregressive Models

Sometimes used in practice, but most useful as a didactic tool.

### Trends

We are often interested in the following sorts of trends:

- Changing Mean
    - Shifts
    - Smooth Changes
        - Monotonic
        - Cyclical
- Changing Variance
- Changing Event Frequency
- Changing Seasonality

Probability Distribution of the Process changes slowly with time: due to identifiable or unknown causes

Key questions:

- is change "natural" or can it be attributed to specific human activity?
- did change in recording method or station location change the "record"
- are policies working? 
- if trend is removed, are underlying causal factors revealed? 
- are  time series models valid if process has  trends?

#### Example

TODO: add the Folosm River data with a vertical line at 1945

This is the annual maximum flood time series from the Fair Oaks station on the American River above Sacramento. Folsom Dam was built as a multi-purpose structure in 1945. The system of dam and dikes was designed to provide Sacramento with flood protection at a level between the 200- to 500-year return period flood as estimated from the 1912 to 1942 annual maximum flood data. There have now been 6 annual maximum floods larger than the largest observed in the pre-dam construction period. As a result, if the recent data is used, the estimated flood protection is below FEMA's nominal level of 100 years, which makes the property in Sacramento uninsurable for flood risk. Moving windows of two different lengths are used to estimate the 100-year flood using the Log-Normal distribution. The estimate is reported at the center of each moving window. The 21-year moving window shows nearly a four-fold increase in the magnitude of the 100-year flood over time, with most of the increase coming in the period after 1940. The 51-year window shows a 1.8-fold increase in the estimated 100-year flood magnitude, with a nearly monotonic increase starting post-dam construction. Are the changes just due to chance? Do they reflect changes in the basin, leading to higher floods with the same rainfall? Do they reflect changes in global climate?

#### Mann-Kendall

There's an example in @helsel_waterresources:2020 for the Potomac River that we can work through

### Other Time Series Models

- ARIMA
- Generalized Additive Models
- State Space Models
- LSTMs
- and beyond

## High-Dimensional Methods

### Covariance and Correlation

### Structured Variability

- e.g., model grid

### Dimension Reduction

::: {.callout-note}
## Motivate with some example scatterplots

What is the true dimensionality of this data?
:::

- Goal: summarize data with many ($p$) variables by a smaller set of $k$ derived (synthetic, composite) variables
- Start with $A_{n \times p}$, $n$ samples of $p$ variables. Get $X_{n \times k}$, $k < p$.
- You will lose _some_ information in $A$ that is not in $X$
- balancing act betwen clarity of represenation (ease of understanding) vs loss of relevant information

#### Principal Component Analysis (PCA)

**Overview and Motivation:**

High-dimensional data presents several challenges:
- Difficult to visualize and interpret
- Contains redundant or irrelevant dimensions
- Computational challenges in high dimensions ("curse of dimensionality")
- Need to identify meaningful patterns in complex datasets

PCA addresses these challenges by finding low-dimensional representations that preserve as much variation as possible in the original data.

**Mathematical Foundation:**

For $n$ observations and $p$ features $X_1, X_2, \ldots, X_p$, PCA finds a low-dimensional representation that captures maximum variance.

The first **principal component** is the **linear combination** of features that maximizes variance:
$$Z_1  = \phi_{11}X_1 + \phi_{21}X_2 + \ldots + \phi_{p1}X_p$$

with normalization constraint: $\sum_{j=1}^p \phi_{j1}^2 = 1$

The **loading vector** $\phi_1 = (\phi_{11}, \phi_{21}, \ldots, \phi_{p1})^T$ defines a direction in feature space along which the data vary the most.

**PCA as an Optimization Problem:**

We can formulate PCA as finding the best linear approximation to the data:
$$f(Z) = \mu + \phi_q Z$$

where:
- $\mu$ is a location vector (typically zero for centered data)
- $\phi_q$ is a $p \times q$ matrix with $q$ **orthogonal** unit vectors as columns
- $Z$ is a $q$-dimensional vector of **coefficients** or **scores**

The optimization problem becomes:
$$\min \sum_{i=1}^n \| X_i - \phi_q Z_i \|_2^2$$

**Connection to Singular Value Decomposition (SVD):**

The solution can be written as a singular value decomposition of the empirical covariance matrix.
This assumes that variance is an appropriate measure of variability in the data.

**Important caveat:** This assumption may be poor when:
- Variables have very different scales
- Outliers are present
- Non-Gaussian distributions where variance doesn't capture the relevant structure

**Uniqueness and Interpretation:**

Each principal component loading vector is unique up to a sign flip.
The choice of sign is arbitrary and doesn't affect the interpretation.

**Climate Science Applications:**

In climate science, we commonly work with space-time data where PCA components can be interpreted as:

1. **Spatial patterns** ($Z$): The "EOFs" (Empirical Orthogonal Functions) or principal components represent dominant spatial patterns of variability
2. **Time series** ($\phi$): The loadings represent time evolution of each spatial pattern
3. **Data reconstruction**: Can reconstruct the full field at time $t$ from the EOFs and their time series

**Preprocessing Considerations:**

**Centering:**
- Variables should have mean zero for standard PCA
- Variance is the average squared deviation from the mean
- Centered and non-centered data will have different covariance matrices

**Standardization:**
- Optional: standardize variables to unit variance
- Important when variables have different units or scales
- Equivalent to weighting variables by inverse of their variances

**Climate Anomalies:**
In climate science, it's common to decompose time series into climatology and anomalies:
$$x(t) = \overline{x}(t) + x'(t)$$
where $\overline{x}(t)$ is the **climatology** and $x'(t)$ is the **anomaly**.

Common approaches for defining climatology:
1. **Time-mean** over a reference period
2. **Seasonal climatology** computed separately for each month or season (DJF, MAM, JJA, SON)
3. **Harmonic climatology** with seasonal cycle removed using sinusoidal terms

**Spatial Data Considerations:**

**Weighting:**
- **Area weighting**: Use $\sqrt{\cos(\phi)}$ where $\phi$ is latitude to weight by grid cell area
- **Variance weighting**: Weight by inverse variance (equivalent to standardization)
- **Physical considerations**: Weight variables based on measurement uncertainty or physical importance

**Model Selection:**

**How Many Components to Retain?**
- **Scree plot**: Look for "elbow" where eigenvalues level off
- **Cumulative variance**: Retain components explaining desired percentage (e.g., 90%)
- **Cross-validation**: Choose number based on out-of-sample performance
- **Physical interpretation**: Retain components with clear physical meaning

**Computational Implementation:**

**Julia Packages:**
- [`EmpiricalOrthogonalFunctions.jl`](https://kmarkert.github.io/EmpiricalOrthogonalFunctions.jl/dev/): Climate-focused PCA/EOF analysis, based on Python's EOFS library
- [`MultivariateStats.jl`](https://juliastats.org/MultivariateStats.jl/dev/pca/): General-purpose multivariate statistics including PCA

**Practical Workflow:**
```julia
using EmpiricalOrthogonalFunctions

# Assume data matrix with time × space structure
solver = EmpiricalOrthogonalFunctions.Eof(data)

# Get EOFs (spatial patterns)
eofs = solver.eofs()

# Get principal components (time series)  
pcs = solver.pcs()

# Get explained variance
variance = solver.varianceFraction()
```

**Extensions and Advanced Methods:**

**Probabilistic PCA**: Provides probabilistic framework with uncertainty quantification
**Robust PCA**: Less sensitive to outliers and non-Gaussian distributions  
**Sparse PCA**: Produces interpretable loadings with many zero entries
**Canonical Correlation Analysis**: Analyzes relationships between two sets of variables

**Climate Applications:**

**Mode Analysis**: Identify dominant climate variability patterns (ENSO, NAO, etc.)
**Model Evaluation**: Compare spatial patterns between observations and climate models
**Dimension Reduction**: Reduce high-dimensional climate model output for analysis
**Data Compression**: Efficiently store and transmit large climate datasets

**Further Reading:**

- [SVD Mathematical Overview](https://youtube.com/watch?v=nbBvuuNVfco&feature=shared) by Steven Brunton provides conceptual SVD overview
- [MIT Computational Thinking PCA Lecture](https://computationalthinking.mit.edu/Fall23/data_science/pca/)
- Chapter 10.2 of @james_statlearn:2013 for statistical learning perspective
- [NCAR EOF Tutorial](https://climatedataguide.ucar.edu/climate-tools/empirical-orthogonal-function-eof-analysis-and-rotated-eof-analysis) for climate applications

### Copulas

### Deep Models

Many generative AI models are fundamentally sampling from high-dimensional conditional distributions.
Handling the high dimensionality of these distributions is a key challenge.

- GAN
- cGAN
- Diffusion Models

## Spatiotemporal Integration

### Spatiotemporal Covariance Structures

### Spatiotemporal Kriging

### State-Space Models for Climate Data

### Climate Field Reconstruction

### Empirical Orthogonal Functions (EOFs)

### Machine Learning for Spatiotemporal Climate Data

## Applications to Climate Data

### Gridded Climate Data Analysis

Almost all the high-dimensional data we deal with in climate science are spatial (e.g., gridded climate model output, reanalysis products, satellite observations).

### Climate Mode Analysis

### Extreme Event Detection in Spatiotemporal Fields

### Model Evaluation and Intercomparison

## Further Reading {.unnumbered}