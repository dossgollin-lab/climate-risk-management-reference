---
title: Spatiotemporal and High-Dimensional Methods ðŸš§
---

## See First {.unnumbered}

This chapter builds on concepts from:
- [Probability and Statistics](/chapters/fundamentals/probability-stats.qmd)
- [Machine Learning and Nonparametric Methods](/chapters/fundamentals/ml-nonparametric.qmd)

## Learning Objectives {.unnumbered}

- Model and interpret spatial dependence in climate fields
- Perform kriging and other geostatistical interpolations for mapping hazards
- Understand the fundamentals of spatial autocorrelation and variogram analysis
- Apply time series analysis methods to detect trends and patterns in climate data
- Use dimension reduction techniques for high-dimensional climate datasets
- Integrate spatial and temporal methods for spatiotemporal climate analysis

## Spatial Statistics

### Spatial Autocorrelation and Variograms

### Interpolation Methods

### Kriging

- (ordinary, universal)

### Gaussian Processes in 1D

### Gaussian Processes in Multiple Dimensions

## Time Series Analysis

### Autocorrelation Functions

### Frequency Analysis

### Wavelets

### Autoregressive Models

Sometimes used in practice, but most useful as a didactic tool.

### Trends

We are often interested in the following sorts of trends:

- Changing Mean
    - Shifts
    - Smooth Changes
        - Monotonic
        - Cyclical
- Changing Variance
- Changing Event Frequency
- Changing Seasonality

Probability Distribution of the Process changes slowly with time: due to identifiable or unknown causes

Key questions:

- is change "natural" or can it be attributed to specific human activity?
- did change in recording method or station location change the "record"
- are policies working? 
- if trend is removed, are underlying causal factors revealed? 
- are  time series models valid if process has  trends?

#### Example

TODO: add the Folosm River data with a vertical line at 1945

This is the annual maximum flood time series from the Fair Oaks station on the American River above Sacramento. Folsom Dam was built as a multi-purpose structure in 1945. The system of dam and dikes was designed to provide Sacramento with flood protection at a level between the 200- to 500-year return period flood as estimated from the 1912 to 1942 annual maximum flood data. There have now been 6 annual maximum floods larger than the largest observed in the pre-dam construction period. As a result, if the recent data is used, the estimated flood protection is below FEMA's nominal level of 100 years, which makes the property in Sacramento uninsurable for flood risk. Moving windows of two different lengths are used to estimate the 100-year flood using the Log-Normal distribution. The estimate is reported at the center of each moving window. The 21-year moving window shows nearly a four-fold increase in the magnitude of the 100-year flood over time, with most of the increase coming in the period after 1940. The 51-year window shows a 1.8-fold increase in the estimated 100-year flood magnitude, with a nearly monotonic increase starting post-dam construction. Are the changes just due to chance? Do they reflect changes in the basin, leading to higher floods with the same rainfall? Do they reflect changes in global climate?

#### Mann-Kendall

There's an example in @helsel_waterresources:2020 for the Potomac River that we can work through

### Other Time Series Models

- ARIMA
- Generalized Additive Models
- State Space Models
- LSTMs
- and beyond

## High-Dimensional Methods

### Covariance and Correlation

### Structured Variability

- e.g., model grid

### Dimension Reduction

::: {.callout-note}
## Motivate with some example scatterplots

What is the true dimensionality of this data?
:::

- Goal: summarize data with many ($p$) variables by a smaller set of $k$ derived (synthetic, composite) variables
- Start with $A_{n \times p}$, $n$ samples of $p$ variables. Get $X_{n \times k}$, $k < p$.
- You will lose _some_ information in $A$ that is not in $X$
- balancing act betwen clarity of represenation (ease of understanding) vs loss of relevant information

#### PCA

- Widely used, well-studied
- Takes data matrix $A$ and finds $k$ orthogonal linear combinations of the columns of $A$ that maximize variance (the first $k$ components display the maximum possible variance)

### Copulas

### Deep Models

Many generative AI models are fundamentally sampling from high-dimensional conditional distributions.
Handling the high dimensionality of these distributions is a key challenge.

- GAN
- cGAN
- Diffusion Models

## Spatiotemporal Integration

### Spatiotemporal Covariance Structures

### Spatiotemporal Kriging

### State-Space Models for Climate Data

### Climate Field Reconstruction

### Empirical Orthogonal Functions (EOFs)

### Machine Learning for Spatiotemporal Climate Data

## Applications to Climate Data

### Gridded Climate Data Analysis

Almost all the high-dimensional data we deal with in climate science are spatial (e.g., gridded climate model output, reanalysis products, satellite observations).

### Climate Mode Analysis

### Extreme Event Detection in Spatiotemporal Fields

### Model Evaluation and Intercomparison

## Further Reading {.unnumbered}