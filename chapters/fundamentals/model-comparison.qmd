---
title: Model Validation and Comparison ðŸš§
---

## See first {.unnumbered}

This chapter builds on concepts from:
- [Probability and Statistics](/chapters/fundamentals/probability-stats.qmd) (including maximum likelihood estimation)
- [Monte Carlo Methods](/chapters/fundamentals/monte-carlo.qmd) (Bayesian inference)

## Learning objectives {.unnumbered}

After reading this chapter, you should be able to:

- Apply graphical diagnostic methods to assess model fit quality
- Use information criteria (AIC, DIC, BIC) for quantitative model comparison
- Understand the relationship between model selection and predictive accuracy
- Recognize the subjective nature of model selection and make transparent choices

## Overview

Model validation and comparison are critical steps in any statistical analysis.
We need methods to:

1. **Assess model fit**: Does our model adequately capture the patterns in the data?
2. **Compare alternatives**: Which model should we choose among competing options?
3. **Predict future observations**: How well will our model perform on new data?

These challenges are particularly acute in climate science because Earth systems are high-dimensional, multi-scale, and nonlinear.

## Graphical diagnostic methods

### The Power of Visualization

Before diving into numerical criteria, graphical methods provide intuitive ways to assess model performance.
"Vibe checks with plots" - as they say - can be more informative than numerical summaries alone.

Key diagnostic plots include:

#### Histogram and Density Plot
- **What we plot**: Histogram of the data and the probability density function
- **Ideal case**: The histogram and the PDF appear to show the same distribution
- **Warnings**: Systematic deviations between empirical and fitted distributions
- **Limitations**: Hard to learn much about the tails of the distribution from this plot

#### Probability Plot (P-P Plot)
- **What we plot**: Empirical CDF (1 - AEP) against the fitted distribution's CDF
- **Ideal case**: A straight line, indicating perfect agreement between the empirical CDF and the fitted CDF
- **Warnings**: Curvature or systematic deviations from the line, especially in the tails
- **Limitations**: Sampling uncertainty affects interpretation

#### Q-Q Plot
- **What we plot**: Quantiles (i.e., return levels) of the data against quantiles of the fitted distribution
- **Ideal case**: A straight line through the data points
- **Warnings**: Curvature or systematic deviations from the line, especially in the tails
- **Limitations**: Sampling uncertainty, especially for extreme quantiles

#### Calibration Histogram
For spatial analyses with many locations:
- **What we plot**: Histogram where each observation is the observed quantile of the data, given the conditional model at that location/year
- **Ideal case**: A uniform distribution
- **Warnings**: Systematic deviations from uniformity
- **Limitations**: Aggregating over sites can hide local issues

### Model Adequacy vs Perfect Fit

The goal is not to find a "perfect" model (which doesn't exist), but rather a model that is:
- **Adequate** for the intended purpose
- **Interpretable** given the context
- **Robust** to reasonable changes in assumptions
- **Predictively useful** for decision-making

**Remember**: You cannot look at a single criterion and decide whether a model is "good" or not.

## Information criteria

### The Fundamental Problem

We want to make probabilistic predictions about **unobserved** data $\tilde{y}$.
The challenge is balancing:
- **Complexity**: More parameters can better fit observed data
- **Overfitting**: Complex models may perform poorly on new data
- **Parsimony**: Simpler models are more interpretable and robust

### Predictive Accuracy Framework

We define predictive performance using a utility function, commonly the log predictive density:
$$
\overline{u}(M) = \mathbb{E}[\log p(\tilde{y} | D, M)] = \int p_t(\tilde{y}) \log p(\tilde{y} | D, M) d\tilde{y}
$$
where $p_t(\tilde{y})$ is the true data generating distribution (unknown!).

Maximizing this expected utility is equivalent to minimizing the Kullback-Leibler divergence between our model and the true data-generating process.

#### Kullback-Leibler Divergence

The KL divergence measures how similar two distributions are:
$$
D_\text{KL} (P \parallel Q) = \sum_{x \in \mathcal{X}} P(x) \log \left[ \frac{P(x)}{Q(x)} \right]
$$

One interpretation of $D_\text{KL} (P \parallel Q)$ is the measure of information gained by revising one's beliefs from the prior distribution $Q$ to the posterior distribution $P$.
Another interpretation is the amount of information lost when $Q$ is used to approximate $P$.

#### Working with Estimates

In practice, we don't know the true distribution, so we approximate the log pointwise predictive density (lppd):
$$
\begin{aligned}
\text{lppd} &= \log \prod_{i=1}^N p_\text{post}(y_i) = \sum_{i=1}^N \log \int p(y_i | \theta) p_\text{post} (\theta) d \theta \\
&\approx \sum_{i=1}^N \log \left[ \frac{1}{S} \sum_{s=1}^S p(y_i | \theta^s) \right]
\end{aligned}
$$
where we have approximated the posterior with $S$ simulations from the posterior (e.g., using MCMC).

The LPPD of observed data $y$ is an overestimate of the expected LPPD for future data, which is why information criteria include bias corrections.

### Akaike Information Criterion (AIC)

For models with $k$ parameters estimated by maximum likelihood:
$$
\text{AIC} = 2k - 2\ln\hat{\mathcal{L}}
$$

where $\hat{\mathcal{L}}$ is the maximized likelihood.

**Key assumptions**:
- Parameters are asymptotically normal
- Residuals are independent given parameter estimates
- Sample size is large relative to number of parameters

### Deviance Information Criterion (DIC)

Extends AIC to Bayesian settings by:
1. Replacing MLE with posterior mean: $\hat{\theta}_{\text{Bayes}} = \mathbb{E}[\theta | y]$
2. Using data-based bias correction for effective parameters

$$
\text{DIC} = -2\log p(y | \hat{\theta}_{\text{Bayes}}) + 2p_{\text{DIC}}
$$

### Bayesian Information Criterion (BIC)

Takes a different approach, approximating the marginal likelihood:
$$
\text{BIC} = k\ln(n) - 2\ln\hat{\mathcal{L}}
$$

Assuming a "true model" exists, BIC tends to select simpler models than AIC.

### Significance Criteria

Use Null Hypothesis Significance Testing (NHST) to decide whether to include a variable.
For example, should we add a trend term in our regression?

1. Form a null hypothesis: $\beta = 0$
2. Test statistics â‡’ $p$-value
3. If $p < \alpha$ then use $M_2$ else use $M_1$

Note that:
- This is equivalent to Bayes factor in certain contexts
- Still assumes existence of a true model (hence the many problems with NHST)
- **This is widely used in practice, often without justification**

## Model selection philosophy

### No Magic Numbers

**Critical insight**: You cannot look at a single criterion and decide whether a model is "good" or not.
Model selection involves subjective choices about:
- Which criteria to prioritize
- How to balance fit vs complexity
- What constitutes "adequate" performance
- Which aspects of the data are most important to capture

### Transparency Over False Objectivity

Rather than pretending model selection is purely objective:
1. **Make assumptions explicit** so others can follow and critique
2. **Consider multiple criteria** and understand their trade-offs
3. **Use domain knowledge** to inform choices
4. **Test sensitivity** to key assumptions

## Model combination and ensemble methods

### Bayesian Model Averaging

Instead of selecting a single "best" model, we can average across models:
$$
p(\tilde{y} | D) = \sum_{\ell=1}^L p(\tilde{y} | D, M_\ell) p(M_\ell | D)
$$

This approach:
- Accounts for model uncertainty
- Often improves predictive performance
- Provides more realistic uncertainty estimates

### Model Stacking

Alternative ensemble approach that optimizes predictive performance by finding optimal weights for combining models without assuming any single model is "true."

::: {.callout-tip}
## Computational examples

For detailed computational examples of model validation and comparison, see the companion file: [Model Validation Examples: Diagnostic Plots and Information Criteria](/chapters/fundamentals/model-comparison-examples.qmd).

Examples include:
- Extreme value model fitting with diagnostic plots (using Extremes.jl)
- `Extremes.histplot()`, `Extremes.probplot()`, `Extremes.qqplot()`, `Extremes.qqplotci()`, and `Extremes.diagnosticplots()` functions
- Information criteria comparison across model families
- Bayesian model averaging implementation
- Model validation for Houston precipitation data and HOUSTON HOBBY AP station
:::

## Further reading {.unnumbered}

For more accessible discussion, see Chapter 7 of @mcelreath_rethinking2:2020.
For more technical treatment, see @piironen_comparison:2017.
For practical guidance on extreme value model selection, see @coles_extremes:2001.