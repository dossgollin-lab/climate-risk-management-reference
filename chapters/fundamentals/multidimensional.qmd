---
title: High-Dimensional Statistics
---

::: {.callout-note}
## Under Construction
This chapter is still under active construction. Please check back regularly for updates and new content.
:::

::: {.callout-tip}
## Learning Objectives
:::

## Covariance and correlation

## Structured variability

- e.g., model grid

## Dimension Reduction

::: {.callout-note}
## Motivate with some example scatterplots

What is the true dimensionality of this data?
:::

- Goal: summarize data with many ($p$) variables by a smaller set of $k$ derived (synthetic, composite) variables
- Start with $A_{n \times p}$, $n$ samples of $p$ variables. Get $X_{n \times k}$, $k < p$.
- You will lose _some_ information in $A$ that is not in $X$
- balancing act betwen clarity of represenation (ease of understanding) vs loss of relevant information

### PCA

- Widely used, well-studied
- Takes data matrix $A$ and finds $k$ orthogonal linear combinations of the columns of $A$ that maximize variance (the first $k$ components display the maximum possible variance)
- 


## Deep Models

Many generative AI models are fundamentally samplingÂ from high-dimensional conditional distributions.
Handling the high dimensionality of these distributions is a key challenge.

- GAN
- cGAN
- Diffusion Models
