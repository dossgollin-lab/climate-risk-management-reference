{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Inference and Optimization\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Learning Objectives {.unnumbered}\n",
        "\n",
        "## Optimization: calculus perspective\n",
        "\n",
        "## Optimization: overview of methods\n",
        "\n",
        "## Curse of dimensionality\n",
        "\n",
        "## Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "We have some *parametric statistical model* with unknown parameters.\n",
        "We want to evaluate how consistent the data are with different values of the parameters, and to find the values of the parameters that are most consistent with the data.\n",
        "\n",
        "### Likelihood function for one observation\n",
        "\n",
        "The likelihood is the probability of the **data** given some **parameters**:\n",
        "$$\n",
        "p(y | \\theta)\n",
        "$$\n",
        "\n",
        "Often, we want to study how the likelihood changes for different values of $\\theta$, holding $y$ fixed.\n",
        "This is just $p(y | \\theta)$ for a range of $\\theta$.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "You will sometimes see this referred to as $\\mathcal{L}(\\theta)$, which is a  confusing notation...\n",
        ":::\n",
        "\n",
        "### Likelihood function for multiple observations\n",
        "\n",
        "Independent and identically distributed (i.i.d.) assumption:\n",
        "$$\n",
        "\\begin{align}\n",
        "p(y_1, y_2, \\ldots, y_n) &= p(y_1) p(y_2) \\times \\ldots \\times p(y_n)\\\\\n",
        " &= \\prod_{i=1}^n p(y_i)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "Usually we have more than one data point.\n",
        "Say we measure $y = y_1, y_2, \\ldots, y_n$:\n",
        "$$\n",
        "p(y | \\theta) = \\prod_{i=1}^n p(y_i | \\theta)\n",
        "$$\n",
        "\n",
        "## Log trick\n",
        "\n",
        "Recall: $\\log(AB) = \\log(A) + \\log(B)$ or, more generally,\n",
        "$$\n",
        "\\log \\left( \\prod_{i=1}^n f_i \\right) = \\sum_{i=1}^n \\log(f_i)\n",
        "$$\n",
        "\n",
        "Thus, we can work with the \"log likelihood\":\n",
        "$$\n",
        "\\log p(y | \\theta) =  \\log \\left( \\prod_{i=1}^n p(y_i | \\theta) \\right) = \\sum_{i=1}^n \\log \\left( p(y_i | \\theta) \\right)\n",
        "$$\n",
        "\n",
        "Adding small numbers is also more numerically stable than multiplying them\n",
        "\n",
        "### Maximum likelihood estimation\n",
        "\n",
        "## Regression models (inference)\n",
        "\n",
        "### GLMs\n",
        "\n",
        "## Old\n",
        "\n",
        "\n",
        "Can we find the parameters $\\theta^*$ that maximize the likelihood $p(y | \\theta)$?\n",
        "\n",
        "## Log likelihood\n",
        "\n",
        "We can use the log likelihood $\\log p(y | \\theta)$ instead of the likelihood $p(y | \\theta)$.\n",
        "\n",
        "The log likelihood is monotonic with the likelihood, so \n",
        "$$\n",
        "\\arg \\max \\log p(y | \\theta) = \\arg \\max p(y | \\theta)\n",
        "$$\n",
        "\n",
        "## Analytic solution {.scrollable .smaller}\n",
        "\n",
        "Solving things analytically takes time up-front, but can be much faster to run because you can avoid the optimization step.\n",
        "Consider the (potentially multivariate) Gaussian example with known covariance matrix $\\Sigma$.\n",
        "We want to *maximize* the likelihood\n",
        "$$\n",
        "\\sum_{i=1}^n p(y_i | \\mu, \\Sigma)\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "To maximize, we set its derivative with respect to $\\mu$, which we'll denote with $\\nabla_\\mu$, to zero:\n",
        "$$\n",
        "\\sum_{i=1}^n \\nabla_\\mu \\log p(y_i | \\mu, \\Sigma) = 0\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "Substituting in the multivariate Gaussian likelihood we get:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "0 & =\\sum_{i=1}^n \\nabla_\\mu \\log \\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}} \\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right) \\\\\n",
        "& =\\sum_{i=1}^n \\nabla_\\mu\\left(\\log \\left(\\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}}\\right)\\right)+\\log \\left(\\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\right) \\\\\n",
        "& =\\sum_{i=1}^n \\nabla_\\mu\\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\\\\n",
        "&=\\sum_{i=1}^n \\Sigma^{-1}\\left(x_i-\\mu\\right) \\\\\n",
        "0 &= \\sum_{i=1}^n (x_i - \\mu) \\\\\n",
        "\\mu &= \\frac{1}{n} \\sum_{i=1}^n x_i\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "::: {.callout-note}\n",
        "You are not expected to remember the above equations and I won't ask you to do this derivation in a time-constrained exam.\n",
        "You should understand the general procedure:\n",
        "\n",
        "1. write down log likelihood for all data points\n",
        "    1. write down likelihood for one data point\n",
        "    1. write down log likelihood for one data point\n",
        "    1. sum over all data points\n",
        "1. take $\\frac{d}{d\\theta}$ and set equal to zero to maximize\n",
        "1. solve for $\\theta^*$.\n",
        "\n",
        ":::\n",
        "\n",
        "## Numerical approach I\n",
        "\n",
        "We can use the `optimize` function from the `Optim.jl` package to find the maximum likelihood estimate.\n",
        "First, we need to define the function to be optimized.\n",
        "`optimize` will minimize the function, so we need to define the *negative* log likelihood.\n",
        "We'll call this the \"loss\" function.\n"
      ],
      "id": "68b5fadf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss(θ) = -normal_log_lik(y_multi, θ[1], θ[2]); # <1>"
      ],
      "id": "603b0fd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Note that this function takes in **a single argument** which is a vector of parameters. We'll call this vector `θ` but it doesn't matter what we call it.\n",
        "\n",
        "## Numerical approach II {.scrollable .smaller}\n",
        "\n",
        "Now we can run the optimization.\n",
        "Since $\\sigma > 0$ always, we will pass along bounds.\n",
        "We could alternatively do something clever like work with $\\log \\sigma$ instead of $\\sigma$.\n"
      ],
      "id": "1c248aad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lower = [0.0001, 0.0001] # <1>\n",
        "upper = [Inf, Inf] # <2>\n",
        "guess = [1.0, 1.0] # <3>\n",
        "\n",
        "res = optimize(loss, lower, upper, guess) # <4>\n",
        "θ_MLE = Optim.minimizer(res) # <5>"
      ],
      "id": "42075b60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. The lower bound is actually zero, but we just set it to a \"pretty small\" number.\n",
        "2. The upper bound is infinity, we can pass in `Inf`\n",
        "3. We need to pass in a guess for the parameters. We'll just use $\\mu = \\sigma = 1$.\n",
        "4. This will actually run the optimization\n",
        "5. This will extract the parameters that minimize the loss function.\n",
        "\n",
        "We could convert this to a `Distributions` object as\n"
      ],
      "id": "3ad8afc8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dist_MLE = Normal(θ_MLE[1], θ_MLE[2])"
      ],
      "id": "66f82b6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression example\n",
        "\n",
        "## Overview {.scrollable}\n",
        "\n",
        "Let's consider the generic regression probelem where we have paired observations $\\left\\{x_i, y_i\\right\\}_{i=1}^n$.\n",
        "In general, we can write this regression as\n",
        "$$\n",
        "y_i | \\alpha, \\beta, \\epsilon \\sim \\mathcal{N}(\\alpha + x_i \\beta, \\sigma^2)\n",
        "$$\n",
        "where $x_i$ and $\\beta$ may be vectors.\n",
        "\n",
        ". . .\n",
        "\n",
        "We can create some raw data (click to \"unfold\" the code)\n"
      ],
      "id": "b903885d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "X = [\n",
        "    9.4,\n",
        "    11.2,\n",
        "    18.5,\n",
        "    5.7,\n",
        "    6.4,\n",
        "    4.4,\n",
        "    10.3,\n",
        "    16.0,\n",
        "    7.8,\n",
        "    12.2,\n",
        "    12.3,\n",
        "    15.1,\n",
        "    10.3,\n",
        "    12.4,\n",
        "    8.2,\n",
        "    11.5,\n",
        "    9.0,\n",
        "    11.3,\n",
        "    9.4,\n",
        "    8.5,\n",
        "]\n",
        "y = [\n",
        "    19.4,\n",
        "    25.0,\n",
        "    41.6,\n",
        "    11.9,\n",
        "    10.6,\n",
        "    8.0,\n",
        "    21.8,\n",
        "    33.8,\n",
        "    15.4,\n",
        "    24.9,\n",
        "    27.4,\n",
        "    31.8,\n",
        "    18.6,\n",
        "    30.1,\n",
        "    18.1,\n",
        "    25.8,\n",
        "    18.8,\n",
        "    24.0,\n",
        "    17.4,\n",
        "    14.7,\n",
        "]\n",
        "scatter(X, y; xlabel=L\"$X$\", ylabel=L\"$y$\", label=\"\")"
      ],
      "id": "83ec7da6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anlytic approach\n",
        "\n",
        "We can use the same approach to derive the maximum likelihood estimate for linear regression:\n",
        "\n",
        "1. Write the likelihood for one data point\n",
        "1. Write the log likelihood for one data point\n",
        "1. Write the log likelihood for all data points\n",
        "1. Take $\\frac{d}{d\\theta}$ and set equal to zero to maximize\n",
        "\n",
        "If you want a walkthrough, see [Ryan Adams's lecture notes](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf) starting at about equation 11.\n",
        "\n",
        "## Numerical optimization I\n",
        "\n",
        "As before, we need to write down a (log) likelihood function\n"
      ],
      "id": "cdbb8a79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function reg_loglik(xi::T, yi::T, α::T, β::T, σ::T) where {T<:Real}\n",
        "    μ = α + xi * β\n",
        "    dist = Normal(μ, σ)\n",
        "    return logpdf(dist, yi)\n",
        "end;"
      ],
      "id": "65b575b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ". . .\n"
      ],
      "id": "993004e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function reg_loglik(X::Vector{T}, y::Vector{T}, α::T, β::T, σ::T) where {T<:Real}\n",
        "    return sum([reg_loglik(xi, yi, α, β, σ) for (xi, yi) in zip(X, y)])\n",
        "end;"
      ],
      "id": "2bd3bf37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical optimization II\n"
      ],
      "id": "2a7fde36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss(θ) = -reg_loglik(X, y, θ[1], θ[2], θ[3]); # <1>\n",
        "lower = [-Inf, -Inf, 0.0001] # <2>\n",
        "upper = [Inf, Inf, Inf]\n",
        "guess = [1.0, 1.0, 1.0]\n",
        "res = optimize(loss, lower, upper, guess)\n",
        "round.(Optim.minimizer(res); digits=3) # <3>"
      ],
      "id": "105e489e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. This is the loss function, which is the negative log likelihood. The first value of $\\theta$ is the intercept, the second is the slope, and the third is the standard deviation.\n",
        "2. We need to pass in bounds for the parameters. The standard deviation is always positive, so we set the lower bound to a small number.\n",
        "3. This will extract the parameters that minimize the loss function and round to show three decimal places.\n",
        "\n",
        "## Parallel: least squares {.scrollable .smaller}\n",
        "\n",
        "If we work through the math, we can show that the log likelihood for the linear regression problem is\n",
        "$$\n",
        "\\log p(y | X, \\beta, \\sigma) = \\frac{N}{2} \\log (2 \\sigma^2 \\pi)  - \\frac{1}{2 \\sigma^2} \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n",
        "$$\n",
        "\n",
        "::: {.callout-note}\n",
        "## Linear algebra notation\n",
        "\n",
        "There is no intercept here!\n",
        "This is a common notation and assumes that the first column of $X$ is all ones.\n",
        "That is equivalent to writing down an intercept, but lets us use linear algebra notation and keep track of fewer variable names\n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "From this, we can show that terms drop out and\n",
        "$$\n",
        "\\beta^\\text{MLE} = \\arg \\min_\\beta \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n",
        "$$\n",
        "which is exactly the least squares problem (minimize squared error):\n",
        "$$\n",
        "\\min_{\\theta} \\sum_{i=1}^n (y_i - y_i^\\text{pred})^2\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "::: {.callout-important}\n",
        "## Key point\n",
        "\n",
        "\"Least squares can be interpreted as assuming Gaussian noise, and particular choices of likelihood can be interpreted directly as (usually exponentiated) loss functions\" --[Adams](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf)\n",
        ":::\n",
        "\n",
        "If we then want to estimate $\\sigma$, we can estimate the standard deviation of the residuals.\n",
        "\n",
        "# Wrapup\n",
        "\n",
        "## Don't get it twisted\n",
        "\n",
        "::: {.note}\n",
        "Many people get this backwards!\n",
        ":::\n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "- The likelihood is the probability of the data given the parameters: $p(y | \\theta)$.\n",
        "- We often plot the likelihood for many different $\\theta$\n",
        "    - $p(y | \\theta)$ for many different $\\theta$\n",
        "- Don't confuse this with the posterior, which is the probability of the parameters given the data: $p(\\theta | y)$\n",
        "\n",
        ":::\n",
        "\n",
        "## Logistics\n",
        "\n",
        "- Friday:\n",
        "    - Lab 03 in class -- look for GH Classroom link on Canvas\n",
        "    - Lab 02 due\n",
        "- Next week:\n",
        "    - Bayesian inference\n",
        "\n",
        "## References"
      ],
      "id": "7ada97fe"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.11",
      "language": "julia",
      "display_name": "Julia 1.11.3",
      "path": "/Users/jamesdoss-gollin/Library/Jupyter/kernels/julia-1.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}