---
title: Nonparametric and Semiparametric Methods
---

## Learning Objectives {.unnumbered}


## Overview

From Bayes' rule,
$$
f(y | \vec{x}) = \frac{f(y, \vec{x})}{f(\vec{x})}
$$
if we can build reliable multivariate probability distributions, we can build a general framework for conditional distributions


## Density estimation

::: {.callout-note}
## Credit

Pulling some bits from https://vita.had.co.nz/papers/density-estimation.pdf, be sure to cite correctly.
:::

Density estimation builds an estimate of some underlying probability density function using an observed data sample. Density estimation can either be parametric, where the data is from a known family, or nonparametric, which attempts to flexibly estimate an unknown distribution.

A simple approach is a histogram (refer to grad class notes).
The histogram requires two parameters: bin width and starting position of the first bin.

::: {.callout-note}
REFER TO Ricardo Gutierrez-Osuna notes
:::

Another approach is kernel density estimation (KDE), which uses a kernel function
$$
\hat{f}_{\text{KDE}} (x) = \frac{1}{n} \sum_{i=1}^n K(\frac{x - x_i}{h})
$$
where $K$ is the kernel function, $h$ is the bandwidth, and $x_i$ are the data points.

The main challenge to the kde approach is varying data density: regions of high data density could have small bandwidths, but regions with sparse data need large bandwidths. 

## Regression

## Neighborhood methods

## Bootstrapping

## Semi-parametric methods
