# Model Validation Examples: Diagnostic Plots and Information Criteria ✏️ {.unnumbered}

This companion file provides computational examples for model validation and comparison concepts covered in the [Model Validation and Comparison](/chapters/fundamentals/model-comparison.qmd) chapter.

```{julia}
#| echo: false
#| output: false
using CairoMakie
using CSV
using DataFrames
using Distributions
using Extremes
using LaTeXStrings
using StatsBase
using Random
Random.seed!(123)
CairoMakie.activate!(; type="svg")
```

## Case Study: Extreme Value Model Selection

This example demonstrates model validation and comparison using precipitation data, showing how different extreme value models perform and how to assess their adequacy.

### Data: Houston Precipitation Extremes

```{julia}
# Create synthetic annual maximum precipitation data
# (representing Houston Hobby Airport pattern)
Random.seed!(789)
n_years = 40

# Generate realistic extreme precipitation data
true_μ = 3.2
true_σ = 0.4
true_ξ = 0.1

# Use GEV distribution for synthetic data
true_dist = GeneralizedExtremeValue(true_μ, true_σ, true_ξ)
precip_data = rand(true_dist, n_years)

# Create DataFrame
houston_data = DataFrame(
    year=1980:(1980+n_years-1),
    precip_in=precip_data,
)

# Plot the data
fig = Figure(size=(800, 400))

ax1 = Axis(fig[1, 1], xlabel="Year", ylabel="Annual Max Precipitation (in)",
    title="Houston Annual Maximum Precipitation")
scatter!(ax1, houston_data.year, houston_data.precip_in, markersize=8)

ax2 = Axis(fig[1, 2], xlabel="Precipitation (in)", ylabel="Frequency",
    title="Histogram of Annual Maxima")
hist!(ax2, houston_data.precip_in, bins=12)

fig
```

## Model Fitting and Comparison

Let's fit several competing models and compare their performance:

1. **Normal Distribution** (clearly wrong for extremes, but useful comparison)
2. **Log-Normal Distribution** (common alternative)  
3. **Generalized Extreme Value (GEV)** (theoretically justified for block maxima)
4. **GEV with trend** (allowing for climate change)

### Fit Competing Models

```{julia}
# Model 1: Normal distribution
normal_fit = fit_mle(Normal, houston_data.precip_in)

# Model 2: Log-Normal distribution  
lognormal_fit = fit_mle(LogNormal, houston_data.precip_in)

# Model 3: GEV distribution (stationary)
gev_fit = fit_mle(GeneralizedExtremeValue, houston_data.precip_in)

# Model 4: GEV with linear trend (non-stationary)
# For demonstration, we'll fit a simple trend model
years_centered = houston_data.year .- mean(houston_data.year)

# Simple trend fitting (in practice, would use specialized EVT packages)
function gev_trend_loglik(params, data, years)
    μ₀, μ₁, σ, ξ = params
    loglik = 0.0
    for (i, (x, t)) in enumerate(zip(data, years))
        μ_t = μ₀ + μ₁ * t
        try
            dist = GeneralizedExtremeValue(μ_t, σ, ξ)
            loglik += logpdf(dist, x)
        catch
            return -Inf
        end
    end
    return loglik
end

# Simple optimization for trend model (normally would use EVT-specific tools)
using Optim

function fit_gev_trend(data, years)
    function objective(params)
        -gev_trend_loglik(params, data, years)
    end

    # Initial guess
    x0 = [mean(data), 0.0, std(data) * 0.8, 0.1]

    # Bounds to ensure valid parameters
    lower = [-Inf, -0.1, 0.001, -0.5]
    upper = [Inf, 0.1, Inf, 0.5]

    result = optimize(objective, lower, upper, x0, Fminbox(LBFGS()))
    return Optim.minimizer(result), -Optim.minimum(result)
end

gev_trend_params, gev_trend_loglik = fit_gev_trend(houston_data.precip_in, years_centered)

println("Model Parameter Estimates:")
println("Normal: μ=$(round(normal_fit.μ, digits=2)), σ=$(round(normal_fit.σ, digits=2))")
println("LogNormal: μ=$(round(lognormal_fit.μ, digits=2)), σ=$(round(lognormal_fit.σ, digits=2))")
println("GEV: μ=$(round(gev_fit.μ, digits=2)), σ=$(round(gev_fit.σ, digits=2)), ξ=$(round(gev_fit.ξ, digits=3))")
println("GEV Trend: μ₀=$(round(gev_trend_params[1], digits=2)), μ₁=$(round(gev_trend_params[2], digits=4)), σ=$(round(gev_trend_params[3], digits=2)), ξ=$(round(gev_trend_params[4], digits=3))")
```

## Graphical Model Validation

### Histogram with Density Overlays

```{julia}
# Plot histogram with fitted densities
fig = hist(houston_data.precip_in, bins=15, normalization=:pdf,
    label="Data", alpha=0.7,
    axis=(xlabel="Precipitation (in)", ylabel="Density",
        title="Model Comparison: Probability Density Functions"))

# Add fitted distributions
x_range = range(1, 8, length=200)

lines!(x_range, pdf.(normal_fit, x_range),
    linewidth=3, label="Normal", color=:red)

lines!(x_range, pdf.(lognormal_fit, x_range),
    linewidth=3, label="LogNormal", color=:blue)

lines!(x_range, pdf.(gev_fit, x_range),
    linewidth=3, label="GEV", color=:green)

axislegend()
fig
```

### Probability Plots (P-P plots)

Compare empirical and fitted CDFs:

```{julia}
# Calculate empirical CDF using plotting positions
function empirical_cdf(data)
    sorted_data = sort(data)
    n = length(data)
    # Weibull plotting positions
    plotting_positions = [(i - 0.5) / n for i in 1:n]
    return sorted_data, plotting_positions
end

sorted_data, emp_probs = empirical_cdf(houston_data.precip_in)

# Calculate fitted CDFs at data points
normal_cdf = cdf.(normal_fit, sorted_data)
lognormal_cdf = cdf.(lognormal_fit, sorted_data)
gev_cdf = cdf.(gev_fit, sorted_data)

fig = Figure(size=(800, 600))

ax = Axis(fig[1, 1], xlabel="Fitted CDF", ylabel="Empirical CDF",
    title="Probability Plots: Empirical vs Fitted CDFs")

# Add perfect fit line
lines!(ax, [0, 1], [0, 1], color=:black, linestyle=:dash, linewidth=2, label="Perfect Fit")

# Plot each model
scatter!(ax, normal_cdf, emp_probs, color=:red, markersize=8, label="Normal")
scatter!(ax, lognormal_cdf, emp_probs, color=:blue, markersize=6, label="LogNormal")
scatter!(ax, gev_cdf, emp_probs, color=:green, markersize=6, label="GEV")

axislegend()
fig
```

### Q-Q Plots (Quantile-Quantile)

Compare quantiles of data vs fitted distributions:

```{julia}
# Calculate quantiles for Q-Q plots
quantile_probs = emp_probs

normal_quantiles = quantile.(normal_fit, quantile_probs)
lognormal_quantiles = quantile.(lognormal_fit, quantile_probs)
gev_quantiles = quantile.(gev_fit, quantile_probs)

fig = Figure(size=(800, 600))

ax = Axis(fig[1, 1], xlabel="Fitted Quantiles", ylabel="Data Quantiles",
    title="Q-Q Plots: Data vs Fitted Quantiles")

# Perfect fit line
min_val = minimum([sorted_data; normal_quantiles; lognormal_quantiles; gev_quantiles])
max_val = maximum([sorted_data; normal_quantiles; lognormal_quantiles; gev_quantiles])
lines!(ax, [min_val, max_val], [min_val, max_val],
    color=:black, linestyle=:dash, linewidth=2, label="Perfect Fit")

# Plot each model
scatter!(ax, normal_quantiles, sorted_data, color=:red, markersize=8, label="Normal")
scatter!(ax, lognormal_quantiles, sorted_data, color=:blue, markersize=6, label="LogNormal")
scatter!(ax, gev_quantiles, sorted_data, color=:green, markersize=6, label="GEV")

axislegend()
fig
```

### Return Period Plots

```{julia}
# Calculate return periods and levels
return_periods = [2, 5, 10, 25, 50, 100, 200]
exceedance_probs = 1 .- 1 ./ return_periods

# Return levels for each model
normal_levels = quantile.(normal_fit, exceedance_probs)
lognormal_levels = quantile.(lognormal_fit, exceedance_probs)
gev_levels = quantile.(gev_fit, exceedance_probs)

# Empirical return periods
emp_return_periods = 1 ./ (1 .- emp_probs)

fig = scatter(emp_return_periods, sorted_data,
    label="Observed", markersize=8, alpha=0.7,
    axis=(xlabel="Return Period (years)", ylabel="Precipitation (in)",
        title="Return Period Analysis", xscale=log10))

lines!(return_periods, normal_levels, linewidth=3, color=:red, label="Normal")
lines!(return_periods, lognormal_levels, linewidth=3, color=:blue, label="LogNormal")
lines!(return_periods, gev_levels, linewidth=3, color=:green, label="GEV")

axislegend()
fig
```

## Information Criteria Comparison

### Calculate Log-Likelihoods

```{julia}
# Calculate log-likelihood for each model
data = houston_data.precip_in

normal_loglik = sum(logpdf.(normal_fit, data))
lognormal_loglik = sum(logpdf.(lognormal_fit, data))
gev_loglik = sum(logpdf.(gev_fit, data))

# Number of parameters
normal_k = 2  # μ, σ
lognormal_k = 2  # μ, σ  
gev_k = 3  # μ, σ, ξ
gev_trend_k = 4  # μ₀, μ₁, σ, ξ

n = length(data)

println("Log-Likelihood Values:")
println("Normal: $(round(normal_loglik, digits=2))")
println("LogNormal: $(round(lognormal_loglik, digits=2))")
println("GEV: $(round(gev_loglik, digits=2))")
println("GEV Trend: $(round(gev_trend_loglik, digits=2))")
```

### Information Criteria

```{julia}
# Calculate AIC: AIC = 2k - 2*log_likelihood
normal_aic = 2 * normal_k - 2 * normal_loglik
lognormal_aic = 2 * lognormal_k - 2 * lognormal_loglik
gev_aic = 2 * gev_k - 2 * gev_loglik
gev_trend_aic = 2 * gev_trend_k - 2 * gev_trend_loglik

# Calculate BIC: BIC = k*log(n) - 2*log_likelihood
normal_bic = normal_k * log(n) - 2 * normal_loglik
lognormal_bic = lognormal_k * log(n) - 2 * lognormal_loglik
gev_bic = gev_k * log(n) - 2 * gev_loglik
gev_trend_bic = gev_trend_k * log(n) - 2 * gev_trend_loglik

# Create summary table
results_df = DataFrame(
    Model=["Normal", "LogNormal", "GEV", "GEV Trend"],
    Parameters=[normal_k, lognormal_k, gev_k, gev_trend_k],
    LogLik=[normal_loglik, lognormal_loglik, gev_loglik, gev_trend_loglik],
    AIC=[normal_aic, lognormal_aic, gev_aic, gev_trend_aic],
    BIC=[normal_bic, lognormal_bic, gev_bic, gev_trend_bic],
)

# Round for display
for col in [:LogLik, :AIC, :BIC]
    results_df[!, col] = round.(results_df[!, col], digits=2)
end

println("Information Criteria Comparison:")
println(results_df)

# Calculate relative performance (AIC differences)
min_aic = minimum(results_df.AIC)
results_df[!, :ΔAIC] = results_df.AIC .- min_aic

min_bic = minimum(results_df.BIC)
results_df[!, :ΔBIC] = results_df.BIC .- min_bic

println("\nRelative Performance (Δ from best model):")
println(select(results_df, :Model, :ΔAIC, :ΔBIC))
```

### Interpretation Guidelines

```{julia}
println("=== Information Criteria Interpretation ===")
println("\nAIC Differences (ΔAIC):")
println("  0-2: Substantial support (models are comparable)")
println("  2-4: Less support for higher AIC model")
println("  4-7: Considerably less support")
println("  >10: Essentially no support")

println("\nBIC tends to favor simpler models more strongly than AIC")
println("due to stronger penalty for additional parameters.")

println("\nModel Ranking by AIC:")
sorted_aic = sort(results_df, :AIC)
for (i, row) in enumerate(eachrow(sorted_aic))
    println("  $i. $(row.Model) (ΔAIC = $(row.ΔAIC))")
end

println("\nModel Ranking by BIC:")
sorted_bic = sort(results_df, :BIC)
for (i, row) in enumerate(eachrow(sorted_bic))
    println("  $i. $(row.Model) (ΔBIC = $(row.ΔBIC))")
end
```

## Model Averaging Example

Instead of selecting a single "best" model, we can average across models weighted by their relative support:

```{julia}
# Calculate AIC weights (Burnham & Anderson approach)
function calculate_aic_weights(aic_values)
    min_aic = minimum(aic_values)
    delta_aic = aic_values .- min_aic
    rel_likelihood = exp.(-0.5 .* delta_aic)
    weights = rel_likelihood ./ sum(rel_likelihood)
    return weights
end

aic_weights = calculate_aic_weights(results_df.AIC)
results_df[!, :AIC_Weight] = round.(aic_weights, digits=3)

println("AIC Model Weights:")
for row in eachrow(results_df)
    println("$(row.Model): $(row.AIC_Weight)")
end

# Model-averaged return level prediction
return_period_target = 50
exceedance_prob = 1 - 1 / return_period_target

# Get return level from each model
normal_rl = quantile(normal_fit, exceedance_prob)
lognormal_rl = quantile(lognormal_fit, exceedance_prob)
gev_rl = quantile(gev_fit, exceedance_prob)

# For trend model, use current year (simplified)
current_year = 0  # centered
μ_current = gev_trend_params[1] + gev_trend_params[2] * current_year
gev_trend_current = GeneralizedExtremeValue(μ_current, gev_trend_params[3], gev_trend_params[4])
gev_trend_rl = quantile(gev_trend_current, exceedance_prob)

# Model-averaged return level
return_levels = [normal_rl, lognormal_rl, gev_rl, gev_trend_rl]
averaged_rl = sum(return_levels .* aic_weights)

println("\n$(return_period_target)-Year Return Level Estimates:")
for (i, model) in enumerate(results_df.Model)
    println("$(model): $(round(return_levels[i], digits=2)) in (weight: $(aic_weights[i]))")
end
println("Model Average: $(round(averaged_rl, digits=2)) in")
```

## Key Insights from Model Comparison

```{julia}
println("=== Model Comparison Summary ===")

println("\n1. Graphical Diagnostics:")
println("   - P-P and Q-Q plots reveal systematic deviations from Normal assumption")
println("   - GEV distribution shows better fit to tail behavior")
println("   - Return period plots highlight differences in extreme value predictions")

println("\n2. Information Criteria:")
best_aic_model = results_df[argmin(results_df.AIC), :Model]
best_bic_model = results_df[argmin(results_df.BIC), :Model]
println("   - AIC favors: $best_aic_model")
println("   - BIC favors: $best_bic_model")
println("   - Both criteria strongly reject Normal distribution")

println("\n3. Practical Implications:")
println("   - Choice of distribution significantly affects return level estimates")
println("   - Model uncertainty should be considered in risk assessment")
println("   - GEV distribution is theoretically justified for annual maxima")

println("\n4. Model Selection Philosophy:")
println("   - No single criterion determines the 'best' model")
println("   - Consider multiple diagnostics and criteria")
println("   - Account for physical understanding and context")
println("   - Model averaging can improve robustness")
```

## Advanced Topics Preview

This example focused on basic model comparison. Advanced topics include:

1. **Cross-validation**: Split data to test predictive performance
2. **Information-theoretic model averaging**: More sophisticated weighting schemes  
3. **Model stacking**: Optimize predictive performance without assuming a "true" model
4. **Bayesian model selection**: Use Bayes factors and posterior model probabilities
5. **Physics-informed model selection**: Incorporate physical constraints and understanding

The key principle remains: model selection is inherently subjective, but this doesn't mean it should be arbitrary. 
Make assumptions explicit, consider multiple criteria, and focus on the decision-making context.