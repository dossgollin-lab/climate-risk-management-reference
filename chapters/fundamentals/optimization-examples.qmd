---
title: "Maximum Likelihood Estimation: Computational Examples ✏️"
---

This companion file provides computational examples for maximum likelihood estimation concepts covered in the [Optimization](/chapters/fundamentals/optimization.qmd) chapter.

```{julia}
#| echo: false
#| output: false
using CairoMakie
using Distributions
using LaTeXStrings
using Optim
CairoMakie.activate!(; type="svg")
```

## Likelihood Examples

### Single Data Point Example

We can plot the likelihood function $p(y | \theta)$ for different values of $\theta$.
Consider $y \sim \mathcal{N}(\mu, \sigma)$ with $\theta = \{\mu, \sigma\}$.

```{julia}
function normal_lik(y::T, μ::T, σ::T) where {T<:Real}
    dist = Normal(μ, σ)
    return pdf(dist, y)
end

# Plot likelihood as function of μ
μ_try = range(-6, 8; length=500)
y = 2.0
σ = 1.0
μ_lik = normal_lik.(y, μ_try, σ)

fig = lines(
    μ_try, μ_lik;
    axis=(
        xlabel=L"$\mu$",
        ylabel=L"$p(y=2 | \mu, \sigma=1)$",
        title="Likelihood Function",
    ),
    label="Likelihood",
)
vlines!([y]; color=:red, linestyle=:dash, label="Observed y", linewidth=2)
axislegend()
fig
```

Notice the likelihood function is maximized at $\mu = y$.

### Multiple Data Points

For multiple i.i.d. observations, the likelihood becomes:
$$p(y | \theta) = \prod_{i=1}^n p(y_i | \theta)$$

```{julia}
function normal_lik(y::Vector{T}, μ::T, σ::T) where {T<:Real}
    dist = Normal(μ, σ)
    log_liks = logpdf.(dist, y)
    return exp(sum(log_liks))  # Product of likelihoods = exp(sum of log-likelihoods)
end

# Multiple data points
y_multi = [2.7, 0.6, 2.7, 3.2, 1.7, 1.0, 2.1, 1.8, 1.6, 2.3]
μ_lik_multi = [normal_lik(y_multi, μi, σ) for μi in μ_try]

fig = lines(
    μ_try, μ_lik_multi;
    axis=(
        xlabel=L"$\mu$",
        ylabel="Likelihood",
        title="Likelihood with Multiple Observations",
    ),
    label="Likelihood",
)
vlines!(y_multi; color=:red, alpha=0.5, label="Observed data")
axislegend()
fig
```

## Poisson Example

Consider modeling the annual number of tropical cyclones making landfall in the continental United States as Poisson($\lambda$).

```{julia}
function poiss_lik(y::Vector{Int}, λ::T) where {T<:Real}
    return exp(sum([logpdf(Poisson(λ), yi) for yi in y]))
end

# Simulated annual cyclone counts
y_poiss = [6, 7, 8, 6, 4, 7, 5, 4, 7, 5]
λ_try = range(1, 13; length=500)
λ_lik = [poiss_lik(y_poiss, λi) for λi in λ_try]

fig = lines(
    λ_try, λ_lik;
    axis=(
        xlabel=L"$\lambda$",
        ylabel="Likelihood",
        title="Poisson Likelihood: Annual Cyclone Counts",
    ),
    linewidth=2,
)
```

## Multivariate Likelihood Surface

For multiple parameters, we can visualize the likelihood as a surface or contour plot.

```{julia}
function normal_log_lik(y::Vector{T}, μ::T, σ::T) where {T<:Real}
    dist = Normal(μ, σ)
    return sum(logpdf.(dist, y))
end

# Create parameter grids
μ_plot = range(-1, 4; length=100)
σ_plot = range(0.5, 3; length=100)

# Calculate log-likelihood surface
log_lik = [normal_log_lik(y_multi, μ, σ) for μ in μ_plot, σ in σ_plot]
lik_plot = exp.(log_lik)

fig = heatmap(
    μ_plot, σ_plot, lik_plot';
    axis=(
        xlabel=L"$\mu$",
        ylabel=L"$\sigma$",
        title="Likelihood Surface",
    ),
    colorbar_title="Likelihood",
)
```

The small high-likelihood region shows parameter combinations most consistent with the data.

## Numerical Optimization

We can use `Optim.jl` to find maximum likelihood estimates numerically.

```{julia}
# Define negative log-likelihood (loss function)
loss(θ) = -normal_log_lik(y_multi, θ[1], θ[2])

# Set bounds and initial guess
lower = [0.0001, 0.0001]  # σ > 0
upper = [Inf, Inf]
guess = [1.0, 1.0]

# Optimize
res = optimize(loss, lower, upper, guess)
θ_MLE = Optim.minimizer(res)

println("MLE estimates:")
println("μ̂ = $(round(θ_MLE[1], digits=3))")
println("σ̂ = $(round(θ_MLE[2], digits=3))")

# Convert to distribution
dist_MLE = Normal(θ_MLE[1], θ_MLE[2])
```

## Linear Regression Example

Maximum likelihood estimation for linear regression is equivalent to least squares under Gaussian noise assumptions.

```{julia}
# Generate example data
X = [9.4, 11.2, 18.5, 5.7, 6.4, 4.4, 10.3, 16.0, 7.8, 12.2]
y = [19.4, 25.0, 41.6, 11.9, 10.6, 8.0, 21.8, 33.8, 15.4, 24.9]

fig = scatter(
    X, y;
    axis=(
        xlabel="X",
        ylabel="y",
        title="Regression Data",
    ),
    markersize=8,
)
```

### Likelihood Functions

```{julia}
# Likelihood for single point
function reg_loglik(xi::T, yi::T, α::T, β::T, σ::T) where {T<:Real}
    μ = α + xi * β
    dist = Normal(μ, σ)
    return logpdf(dist, yi)
end

# Likelihood for all data
function reg_loglik(X::Vector{T}, y::Vector{T}, α::T, β::T, σ::T) where {T<:Real}
    return sum([reg_loglik(xi, yi, α, β, σ) for (xi, yi) in zip(X, y)])
end

# Optimize
loss(θ) = -reg_loglik(X, y, θ[1], θ[2], θ[3])
lower = [-Inf, -Inf, 0.0001]  # σ > 0
upper = [Inf, Inf, Inf]
guess = [1.0, 1.0, 1.0]

res = optimize(loss, lower, upper, guess)
θ_reg = Optim.minimizer(res)

println("Regression MLE estimates:")
println("α̂ (intercept) = $(round(θ_reg[1], digits=3))")
println("β̂ (slope) = $(round(θ_reg[2], digits=3))")
println("σ̂ = $(round(θ_reg[3], digits=3))")

# Add fitted line to plot
x_pred = range(minimum(X), maximum(X), length=100)
y_pred = θ_reg[1] .+ θ_reg[2] .* x_pred

lines!(fig, x_pred, y_pred; color=:red, linewidth=2, label="MLE fit")
axislegend()
fig
```

This demonstrates that MLE for linear regression with Gaussian noise gives the same result as least squares fitting.