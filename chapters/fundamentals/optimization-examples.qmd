# Maximum Likelihood Estimation: Computational Examples ðŸš§ {.unnumbered}

This companion file provides computational examples for maximum likelihood estimation concepts covered in the [Optimization](/chapters/fundamentals/optimization.qmd) chapter.

```{julia}
#| echo: false
#| output: false
using CairoMakie
using Distributions
using LaTeXStrings
using Optim
CairoMakie.activate!(; type="svg")
```

## Likelihood examples

### Single data point example

We can plot the likelihood function $p(y | \theta)$ for different values of $\theta$.
Consider $y \sim \mathcal{N}(\mu, \sigma)$ with $\theta = \{\mu, \sigma\}$.

```{julia}
function normal_lik(y::T, Î¼::T, Ïƒ::T) where {T<:Real}
    dist = Normal(Î¼, Ïƒ)
    return pdf(dist, y)
end

# Plot likelihood as function of Î¼
Î¼_try = range(-6, 8; length=500)
y = 2.0
Ïƒ = 1.0
Î¼_lik = normal_lik.(y, Î¼_try, Ïƒ)

fig = lines(
    Î¼_try, Î¼_lik;
    axis=(
        xlabel=L"$\mu$",
        ylabel=L"$p(y=2 | \mu, \sigma=1)$",
        title="Likelihood Function",
    ),
    label="Likelihood",
)
vlines!([y]; color=:red, linestyle=:dash, label="Observed y", linewidth=2)
axislegend()
fig
```

Notice the likelihood function is maximized at $\mu = y$.

### Multiple data points

For multiple i.i.d. observations, the likelihood becomes:
$$p(y | \theta) = \prod_{i=1}^n p(y_i | \theta)$$

```{julia}
function normal_lik(y::Vector{T}, Î¼::T, Ïƒ::T) where {T<:Real}
    dist = Normal(Î¼, Ïƒ)
    log_liks = logpdf.(dist, y)
    return exp(sum(log_liks))  # Product of likelihoods = exp(sum of log-likelihoods)
end

# Multiple data points
y_multi = [2.7, 0.6, 2.7, 3.2, 1.7, 1.0, 2.1, 1.8, 1.6, 2.3]
Î¼_lik_multi = [normal_lik(y_multi, Î¼i, Ïƒ) for Î¼i in Î¼_try]

fig = lines(
    Î¼_try, Î¼_lik_multi;
    axis=(
        xlabel=L"$\mu$",
        ylabel="Likelihood",
        title="Likelihood with Multiple Observations",
    ),
    label="Likelihood",
)
vlines!(y_multi; color=:red, alpha=0.5, label="Observed data")
axislegend()
fig
```

## Poisson example

Consider modeling the annual number of tropical cyclones making landfall in the continental United States as Poisson($\lambda$).

```{julia}
function poiss_lik(y::Vector{Int}, Î»::T) where {T<:Real}
    return exp(sum([logpdf(Poisson(Î»), yi) for yi in y]))
end

# Simulated annual cyclone counts
y_poiss = [6, 7, 8, 6, 4, 7, 5, 4, 7, 5]
Î»_try = range(1, 13; length=500)
Î»_lik = [poiss_lik(y_poiss, Î»i) for Î»i in Î»_try]

fig = lines(
    Î»_try, Î»_lik;
    axis=(
        xlabel=L"$\lambda$",
        ylabel="Likelihood",
        title="Poisson Likelihood: Annual Cyclone Counts",
    ),
    linewidth=2,
)
```

## Multivariate likelihood surface

For multiple parameters, we can visualize the likelihood as a surface or contour plot.

```{julia}
function normal_log_lik(y::Vector{T}, Î¼::T, Ïƒ::T) where {T<:Real}
    dist = Normal(Î¼, Ïƒ)
    return sum(logpdf.(dist, y))
end

# Create parameter grids
Î¼_plot = range(-1, 4; length=100)
Ïƒ_plot = range(0.5, 3; length=100)

# Calculate log-likelihood surface
log_lik = [normal_log_lik(y_multi, Î¼, Ïƒ) for Î¼ in Î¼_plot, Ïƒ in Ïƒ_plot]
lik_plot = exp.(log_lik)

fig = heatmap(
    Î¼_plot, Ïƒ_plot, lik_plot';
    axis=(
        xlabel=L"$\mu$",
        ylabel=L"$\sigma$",
        title="Likelihood Surface",
    ),
    colorbar_title="Likelihood",
)
```

The small high-likelihood region shows parameter combinations most consistent with the data.

## Numerical optimization

We can use `Optim.jl` to find maximum likelihood estimates numerically.

```{julia}
# Define negative log-likelihood (loss function)
loss(Î¸) = -normal_log_lik(y_multi, Î¸[1], Î¸[2])

# Set bounds and initial guess
lower = [0.0001, 0.0001]  # Ïƒ > 0
upper = [Inf, Inf]
guess = [1.0, 1.0]

# Optimize
res = optimize(loss, lower, upper, guess)
Î¸_MLE = Optim.minimizer(res)

println("MLE estimates:")
println("Î¼Ì‚ = $(round(Î¸_MLE[1], digits=3))")
println("ÏƒÌ‚ = $(round(Î¸_MLE[2], digits=3))")

# Convert to distribution
dist_MLE = Normal(Î¸_MLE[1], Î¸_MLE[2])
```

## Linear regression example

Maximum likelihood estimation for linear regression is equivalent to least squares under Gaussian noise assumptions.

```{julia}
# Generate example data
X = [9.4, 11.2, 18.5, 5.7, 6.4, 4.4, 10.3, 16.0, 7.8, 12.2]
y = [19.4, 25.0, 41.6, 11.9, 10.6, 8.0, 21.8, 33.8, 15.4, 24.9]

fig = scatter(
    X, y;
    axis=(
        xlabel="X",
        ylabel="y",
        title="Regression Data",
    ),
    markersize=8,
)
```

### Likelihood functions

```{julia}
# Likelihood for single point
function reg_loglik(xi::T, yi::T, Î±::T, Î²::T, Ïƒ::T) where {T<:Real}
    Î¼ = Î± + xi * Î²
    dist = Normal(Î¼, Ïƒ)
    return logpdf(dist, yi)
end

# Likelihood for all data
function reg_loglik(X::Vector{T}, y::Vector{T}, Î±::T, Î²::T, Ïƒ::T) where {T<:Real}
    return sum([reg_loglik(xi, yi, Î±, Î², Ïƒ) for (xi, yi) in zip(X, y)])
end

# Optimize
loss(Î¸) = -reg_loglik(X, y, Î¸[1], Î¸[2], Î¸[3])
lower = [-Inf, -Inf, 0.0001]  # Ïƒ > 0
upper = [Inf, Inf, Inf]
guess = [1.0, 1.0, 1.0]

res = optimize(loss, lower, upper, guess)
Î¸_reg = Optim.minimizer(res)

println("Regression MLE estimates:")
println("Î±Ì‚ (intercept) = $(round(Î¸_reg[1], digits=3))")
println("Î²Ì‚ (slope) = $(round(Î¸_reg[2], digits=3))")
println("ÏƒÌ‚ = $(round(Î¸_reg[3], digits=3))")

# Add fitted line to plot
x_pred = range(minimum(X), maximum(X), length=100)
y_pred = Î¸_reg[1] .+ Î¸_reg[2] .* x_pred

lines!(fig, x_pred, y_pred; color=:red, linewidth=2, label="MLE fit")
axislegend()
fig
```

This demonstrates that MLE for linear regression with Gaussian noise gives the same result as least squares fitting.