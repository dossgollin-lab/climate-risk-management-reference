---
title: Optimization üìù
---

## See First {.unnumbered}

This chapter builds on concepts from:
- [Probability and Statistics](/chapters/fundamentals/probability-stats.qmd)

## Learning Objectives {.unnumbered}

- Understand the components of an optimization problem: decision variables, objective functions, and constraints.
- Learn how optimization is applied in statistics and policy search.
- Explore the trade-offs between problem complexity and solution accuracy.

Optimization is a field of study unto itself, and forms an essential backbone of statistics, machine learning, and operations research.
Here, we briefly summarize key concepts and techniques in optimization, with a focus on their application to statistics and policy search.

## Overview

Optimization is the study of finding the best solution to a problem from a set of feasible solutions.
Thus, optimization problems typically consist of

1. A set of decision variables , $x$, which are the inputs we can control (or the "dials" we can turn).
1. One (or more) objective function(s), $f(x)$, which we want to minimize or maximize.
1. A set of constraints which define the feasible region.

Optimization is an incredibly diverse field.
Problems can have many constraints or none; can have discrete or continuous decision variables (or both); can be static or dynamic; deterministic or stochastic; linear or nonlinear; or much more.
When you encounter an optimization problem in the wild, you should always make sure you understand the decision variables (which define what is being optimized), the objective function(s), which define what makes a solution "good", and the constraints, which define what solutions are being considered.
When you build an optimization model, you should always make sure to communicate these three components clearly.

Because optimization is a widely studied field, exact solutions to some classes of problems are known, and nearly exact solutions to many others are known.
However, these exact solutions often require formulating the problem in a very specific way.
Thus, there is generally a trade-off between designing an optimization problem that is (relatively) easy to solve vs. one that represents the relevant system dynamics and uncertainties (relatively) accurately.
Consequently, converting a decision problem into an optimization problem is somewhat of an art form, rather than an exact science; this is largely the field of operations research.

## Motivation: Inference

One common application of optimization is in statistics.
Specifically, we often want to evaluate how consistent the data are with different values of the parameters, and to find the values of the parameters that are most consistent with the data.

### General Case

#### Likelihood

The likelihood is the probability of the **observations** $y$ given some **parameters** $\theta$:
$$
p(y | \theta)
$$

Often, we want to study how the likelihood changes for different values of $\theta$, holding $y$ fixed.
This is just $p(y | \theta)$ for a range of $\theta$.


::: {.callout-note}
You will sometimes see this referred to as $\mathcal{L}(\theta)$.
However, since this is a probability distribution, it doesn't really need its own notation.
:::

#### Likelihood function for multiple observations

Often, we have multiple observations $y_1, y_2, \ldots, y_n$.


Independent and identically distributed (i.i.d.) assumption:
$$
\begin{aligned}
p(y_1, y_2, \ldots, y_n) &= p(y_1) p(y_2) \times \ldots \times p(y_n)\\
 &= \prod_{i=1}^n p(y_i)
\end{aligned}
$$


Usually we have more than one data point.
Say we measure $y = y_1, y_2, \ldots, y_n$:
$$
p(y | \theta) = \prod_{i=1}^n p(y_i | \theta)
$$

#### Log trick

Recall: $\log(AB) = \log(A) + \log(B)$ or, more generally,
$$
\log \left( \prod_{i=1}^n f_i \right) = \sum_{i=1}^n \log(f_i)
$$

Thus, we can work with the "log likelihood":
$$
\log p(y | \theta) =  \log \left( \prod_{i=1}^n p(y_i | \theta) \right) = \sum_{i=1}^n \log \left( p(y_i | \theta) \right)
$$

Adding small numbers is also more numerically stable than multiplying them

Can we find the parameters $\theta^*$ that maximize the likelihood $p(y | \theta)$?

#### Log likelihood

We can use the log likelihood $\log p(y | \theta)$ instead of the likelihood $p(y | \theta)$.

The log likelihood is monotonic with the likelihood, so 
$$
\arg \max \log p(y | \theta) = \arg \max p(y | \theta)
$$


Solving things analytically takes time up-front, but can be much faster to run because you can avoid the optimization step.
Consider the (potentially multivariate) Gaussian example with known covariance matrix $\Sigma$.
We want to *maximize* the likelihood
$$
\sum_{i=1}^n p(y_i | \mu, \Sigma)
$$


To maximize, we set its derivative with respect to $\mu$, which we'll denote with $\nabla_\mu$, to zero:
$$
\sum_{i=1}^n \nabla_\mu \log p(y_i | \mu, \Sigma) = 0
$$


Substituting in the multivariate Gaussian likelihood we get:
$$
\begin{aligned}
0 & =\sum_{i=1}^n \nabla_\mu \log \frac{1}{\sqrt{(2 \pi)^d|\Sigma|}} \exp \left(-\frac{1}{2}\left(x_i-\mu\right)^{\top} \Sigma^{-1}\left(x_i-\mu\right)\right) \\
& =\sum_{i=1}^n \nabla_\mu\left(\log \left(\frac{1}{\sqrt{(2 \pi)^d|\Sigma|}}\right)\right)+\log \left(\exp \left(-\frac{1}{2}\left(x_i-\mu\right)^{\top} \Sigma^{-1}\left(x_i-\mu\right)\right)\right) \\
& =\sum_{i=1}^n \nabla_\mu\left(-\frac{1}{2}\left(x_i-\mu\right)^{\top} \Sigma^{-1}\left(x_i-\mu\right)\right)\\
&=\sum_{i=1}^n \Sigma^{-1}\left(x_i-\mu\right) \\
0 &= \sum_{i=1}^n (x_i - \mu) \\
\mu &= \frac{1}{n} \sum_{i=1}^n x_i
\end{aligned}
$$


You are not expected to remember the above equations and I won't ask you to do this derivation in a time-constrained exam.
You should understand the general procedure:

1. write down log likelihood for all data points
    1. write down likelihood for one data point
    1. write down log likelihood for one data point
    1. sum over all data points
1. take $\frac{d}{d\theta}$ and set equal to zero to maximize
1. solve for $\theta^*$.

### Example: Linear Regression

Let's consider the generic regression probelem where we have paired observations $\left\{x_i, y_i\right\}_{i=1}^n$.
In general, we can write this regression as
$$
y_i | \alpha, \beta, \epsilon \sim \mathcal{N}(\alpha + x_i \beta, \sigma^2)
$$
where $x_i$ and $\beta$ may be vectors.

We can use the same approach to derive the maximum likelihood estimate for linear regression:

1. Write the likelihood for one data point
1. Write the log likelihood for one data point
1. Write the log likelihood for all data points
1. Take $\frac{d}{d\theta}$ and set equal to zero to maximize

If you want a walkthrough, see [Ryan Adams's lecture notes](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf) starting at about equation 11.

If we work through the math, we can show that the log likelihood for the linear regression problem is
$$
\log p(y | X, \beta, \sigma) = \frac{N}{2} \log (2 \sigma^2 \pi)  - \frac{1}{2 \sigma^2} \left( X \beta - y \right)^T \left( X \beta - y \right)
$$

::: {.callout-note}
## Linear algebra notation

There is no intercept here!
This is a common notation and assumes that the first column of $X$ is all ones.
That is equivalent to writing down an intercept, but lets us use linear algebra notation and keep track of fewer variable names
:::


From this, we can show that terms drop out and
$$
\beta^\text{MLE} = \arg \min_\beta \left( X \beta - y \right)^T \left( X \beta - y \right)
$$
which is exactly the least squares problem (minimize squared error):
$$
\min_{\theta} \sum_{i=1}^n (y_i - y_i^\text{pred})^2
$$

::: {.callout-important}
## Key point

"Least squares can be interpreted as assuming Gaussian noise, and particular choices of likelihood can be interpreted directly as (usually exponentiated) loss functions" --[Adams](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf)
:::

If we then want to estimate $\sigma$, we can estimate the standard deviation of the residuals.

## Motivation: Policy Search

## Optimization Toolkit

### Gradient-Based Optimization

This is useful for thinking about neural networks, but also for many other optimization problems.
The key idea is that we can use the gradient of the objective function to find the direction in which to move to improve our solution.
This is often called **gradient descent**.
The basic idea is to take a step in the direction of the gradient, which is the direction of steepest ascent.
In the simplest case,
$$
\Delta x = -\alpha \nabla f(x)
$$
where $\alpha$ is the step size or learning rate.
This is a hyperparameter that we can tune.

The primary limitation of gradient descent is that it can get stuck in local minima.
This is especially true for non-convex problems, where there may be many local minima.
A vast range of techniques, a treatment of which merits a textbook on its own, have been developed to address this issue, including for example the Adam optimizer [@kingma_adam:2017].

### Stochastic Optimization

### Sequential Decision Problems

### High-Dimensional Optimization

## Further Reading {.unnumbered}

- **@sutton_reinforcement:2018** provides a comprehensive introduction to reinforcement learning, which is broadly the study of sequential decision-making under uncertainty.
- **@powell_textbook:2022** provides a comprehensive treatment of sequential decision-making under uncertainty, aiming at a unified framework