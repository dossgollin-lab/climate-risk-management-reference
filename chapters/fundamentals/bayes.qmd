---
title: Bayesian Inference
---

::: {.callout-note}
## Under Construction
This chapter is still under active construction. Please check back regularly for updates and new content.
:::

::: {.callout-tip}
## Learning Objectives

- Use Markov Chain Monte Carlo (MCMC) techniques to estimate posterior distributions.
- Understand the role of utility theory in decision-making under uncertainty.
:::


1. Parameters have **probability distributions**, not single point values 
1. Start with some prior distribution for parameters
1. Goal: what is the distribution of the parameters given the data?

Builds on Bayes' Theorem.
Bayes' rule for distributions.

$$
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)}
$$

If we are drawing samples from a distribution, we can calculate up to a constant of proportionality and  -- since $p(y)$ doesn't depend on $\theta$ -- we can usually ignore it.


## Maximum A Posteriori (MAP) Estimation

## Exact Posterior Estimation

## Posterior Approximation with MCMC

## Hierarchical Models

## Missing Data

## Censored Data

## Bayesian Workflow and Philosophy

### Priors

The posterior is a compromise between the prior and the likelihood.

- Bad priors lead to bad inferences
- The choice of prior is subjective, which some people hate,
    - We will approach this in a principled manner [@gelman_philosophy:2013;@gelman_workflow:2020]
    - Lots of other steps are also subjective (choice of likelihood model, which data to use, problem framing, etc)
    - False sense of objectivity is dangerous anyways!

## Further Reading {.unnumbered}

- Although about a decade old, @gelman_bda3:2014 is the definitive reference for Bayesian statistics
- @mcelreath_rethinking2:2020 takes a different pedagogical approach to @gelman_bda3:2014, covering similar concepts from a somewhat more applied perspective. Lectures are available [on YouTube](https://www.youtube.com/@rmcelreath).
- @gelman_workflow:2020 is a mini-textbook on Bayesian workflow
- @gelman_subjectiveobjective:2017 and @gelman_workflow:2020 cleanly outline how to think about interpreting probability -- which is often misunderstood.


---

## Bayesian Utility Theory

We will often be interested in making decisions despite not having perfect information.
In the simplest, most ideal world, Bayesian Decision Theory provides a coherent framework for making optimal decisions:
$$
\mathbb{E}[U(a)] = \int U(a, s) p(s) ds,
$$
where $U(a, s)$ is the utility of action $a$ given "state of the world" $s$, and $p(s)$ is the probability density function of the state.
The "best" action is the one that maximizes expected utility.
This clean starting point provides a jumping-off point for framing the challenges of this book.
Although much of this book is focused on the reasons why this idealized framework is not sufficient, this simplifed framework is useful for framing a key goal, which is that we are interested not in forecasting the future through a crystal ball, but rather in estimating the probability distribution of future states of the world.
Specifically, we are interested in the {{< glossary "joint probability distribution" >}} of relevant variables, which may include both physical (e.g., climate hazard) and socioeconomic (e.g., vulnerability) variables.

### Types of uncertainty

- Aleatory uncertainty: inherent variability in the system (ie, randomness)
- Epistemic uncertainty: uncertainty in our knowledge of the system, often useful to divide into
    - Parametric uncertainty (what we will use here)
    - Model structure uncertainty (treated qualitatively)

### History

- First conceived as a way to try to explain human behavior
    - Turns out not to be a great model -- people do not always behave rationally

### Utility

- Some function that we want to maximize
- Alternatively can be thought of as a cost function that we want to minimize
- $U(a, \theta)$
    - $a$ is the action we take
    - $\theta$ is the state of the world

### Expected Utility

$$
\mathbb{E} \left[ U(a, \theta) \right] = \int_{\theta \in \Theta} U(a, \theta) p(\theta) d\theta
$$

or in the discrete case

$$
\mathbb{E} \left[ U(a, \theta) \right] = \sum_{\theta \in \Theta} U(a, \theta) p(\theta)
$$