---
title: Bayesian Inference
---

::: {.callout-note}
## Under Construction
This chapter is still under active construction. Please check back regularly for updates and new content.
:::

## Learning Objectives {.unnumbered}

- Use Markov Chain Monte Carlo (MCMC) techniques to estimate posterior distributions.
- Understand the role of utility theory in decision-making under uncertainty.


## Bayesian Inference

1. Parameters have **probability distributions**, not single point values 
1. Start with some prior distribution for parameters
1. Goal: what is the distribution of the parameters given the data?

Builds on Bayes' Theorem.
Bayes' rule for distributions.

$$
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)}
$$

If we are drawing samples from a distribution, we can calculate up to a constant of proportionality and  -- since $p(y)$ doesn't depend on $\theta$ -- we can usually ignore it.

$$
\overbrace{p(\theta \mid y)}^\rm{posterior} \propto \underbrace{p(y \mid \theta)}_\rm{likelihood} \overbrace{p(\theta)}^\rm{prior}
$$


## Maximum A Posteriori (MAP) Estimation

## Exact Posterior Estimation

## Posterior Approximation with MCMC

## Hierarchical Models

## Missing Data

## Censored Data

## Bayesian Workflow and Philosophy

### Priors

The posterior is a compromise between the prior and the likelihood.

- Bad priors lead to bad inferences
- The choice of prior is subjective, which some people hate,
    - We will approach this in a principled manner [@gelman_philosophy:2013;@gelman_workflow:2020]
    - Lots of other steps are also subjective (choice of likelihood model, which data to use, problem framing, etc)
    - False sense of objectivity is dangerous anyways!

## Further Reading

- Although about a decade old, @gelman_bda3:2014 is the definitive reference for Bayesian statistics
- @mcelreath_rethinking2:2020 takes a different pedagogical approach to @gelman_bda3:2014, covering similar concepts from a somewhat more applied perspective. Lectures are available [on YouTube](https://www.youtube.com/@rmcelreath).
- @gelman_workflow:2020 is a mini-textbook on Bayesian workflow
- @gelman_subjectiveobjective:2017 and @gelman_workflow:2020 cleanly outline how to think about interpreting probability -- which is often misunderstood.

