---
title: Monte Carlo Methods ðŸš§
---

## See first {.unnumbered}

This chapter builds on concepts from:
- [Probability and Statistics](/chapters/fundamentals/probability-stats.qmd)

## Learning objectives {.unnumbered}

- Apply Monte Carlo methods to approximate integrals and expectations
- Use Markov Chain Monte Carlo (MCMC) techniques to estimate posterior distributions
- Understand parameter uncertainty propagation in decision-making
- Implement Monte Carlo simulation for climate risk assessment

Monte Carlo is a powerful class of methods used to estimate the properties of a distribution by generating random samples from that distribution.
Specifically, Monte Carlo methods are used to estimate the expected value of a function of random variables.

## Overview

Monte Carlo methods are used to approximate integrals using sums.
If we want to compute an expectation $\mathbb{E}[f(X)]$ where $X$ has probability density $p(x)$:

$$
\mathbb{E}[f(X)] = \int f(x) p(x) dx
$$

we can approximate this using Monte Carlo:
$$
\mathbb{E}[f(X)] \approx \frac{1}{N} \sum_{i=1}^N f(x_i)
$$
where $x_1, x_2, \ldots, x_N$ are samples drawn from $p(x)$.

This is particularly powerful when $p(x)$ is a complex posterior distribution that we can only sample from, not evaluate directly.

## Motivation: Bayesian inference

Bayesian inference provides a principled framework for updating our beliefs about parameters given observed data. The key insight is that parameters have probability distributions, not single point values.

### Bayes' theorem

For continuous parameters, Bayes' theorem states:
$$
p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}
$$

where:
- $p(\theta | y)$ is the **posterior** distribution (what we want)
- $p(y | \theta)$ is the **likelihood** (probability of data given parameters)
- $p(\theta)$ is the **prior** distribution (our initial beliefs)
- $p(y)$ is the **marginal likelihood** (normalizing constant)

Since $p(y)$ doesn't depend on $\theta$, we often work with:
$$
p(\theta | y) \propto p(y | \theta) p(\theta)
$$

### The challenge

For most real-world problems, the posterior distribution $p(\theta | y)$ cannot be computed analytically. This is where Monte Carlo methods become essential - we use them to draw samples from complex posterior distributions.

### Markov Chain Monte Carlo (MCMC)

MCMC methods generate samples from posterior distributions by:
1. Starting at some initial parameter values
2. Proposing new parameter values
3. Accepting or rejecting proposals based on the posterior probability
4. Repeating to create a "chain" of samples

Modern MCMC algorithms like the No-U-Turn Sampler (NUTS) use gradient information to efficiently explore high-dimensional parameter spaces.

## Motivation: parameter uncertainty

A fundamental challenge in climate risk assessment is that model parameters are uncertain.
Consider a simple flood risk assessment problem:

- We have a probability distribution for flood heights: $p(h)$
- We have a damage function: $d(h)$ (damages as a function of height)
- We want the probability distribution of damages: $p(d)$

Mathematically:
$$
p(d) = \int_h d(h) p(h) \, dh
$$

### The challenge

This integral is often impossible to solve analytically, especially when:
1. We use realistic (non-analytical) damage functions
2. We want to account for parameter uncertainty in $p(h)$
3. We have multiple uncertain parameters

### Monte Carlo Solution

Instead of solving the integral analytically:
1. Sample flood heights: $h_1, h_2, \ldots, h_N \sim p(h)$
2. Apply damage function: $d_i = d(h_i)$ for each sample
3. Analyze the resulting damage samples: $\{d_1, d_2, \ldots, d_N\}$

This approach generalizes to complex, multi-parameter problems where analytical solutions are impossible.

## Monte Carlo Theory

### Fundamental theorem

If $\theta^s \sim p(\theta)$, then by the Law of Large Numbers:
$$
\mathbb{E}[f(\theta)] = \int_{\theta} f(\theta) p(\theta) d\theta \approx \frac{1}{S} \sum_{s=1}^S f(\theta^s)
$$

This convergence is **guaranteed** as $S \to \infty$, making Monte Carlo a robust approach for complex problems.

### Advantages over deterministic integration

**Deterministic approach**:
- Sample $\theta$ at regular grid points
- Compute $f(\theta)$ at each point and sum
- **Problems**: Curse of dimensionality, choice of grid spacing, boundary handling

**Monte Carlo approach**:
- Sample $\theta$ from the actual distribution $p(\theta)$ 
- Automatically focuses computational effort where probability is high
- **Benefits**: Scales well to high dimensions, handles complex domains

## Motivation: resilience

Monte Carlo methods are essential for resilience analysis because they can handle:
- Multiple uncertain hazards occurring simultaneously
- Complex system interactions and cascading failures
- Non-linear damage functions and thresholds
- Rare but high-impact events

## Motivation: sensitivity analysis

Monte Carlo methods excel at sensitivity analysis - understanding how uncertain inputs affect outputs.

### Parameter sensitivity

By systematically varying parameters and observing changes in Monte Carlo outputs, we can:
1. **Identify critical parameters**: Which uncertainties matter most?
2. **Quantify sensitivity**: How much does output uncertainty change with input uncertainty?
3. **Guide data collection**: Where would additional measurements be most valuable?

### Global sensitivity analysis

Unlike local sensitivity (derivatives at a point), Monte Carlo enables **global sensitivity analysis** across the entire parameter space, capturing:
- Non-linear relationships
- Parameter interactions 
- Threshold effects
- Tail behavior

## Bayesian Decision Theory Context

Recall from decision theory:
$$
\mathbb{E}\left[L(a, \theta) \right] = \int_\theta L(a, \theta) p(\theta) d\theta
$$
Where $\theta$ is a vector of parameters, $a$ is some action or decision, and $L$ is the loss function.

::: {.callout-note}
We previously called $\theta$ a "state of the world" and $L$ a "reward function".
:::

## Practical problem: flood risk assessment

You have been commissioned by a client to assess their exposure to flooding.
Specifically, they want to know the probability distribution of annual flood damages at their property if they do not elevate or floodproof their building.

1. $p(h)$: probability distribution of annual maximum flood heights at their property
2. $d(h)$: flood damages as a deterministic function of flood height
3. $p(d) = \int_h d(h) p(h) \, dh$

With this information, they can compute metrics like the expected annual damage, the 99th percentile annual damage, and the probability of any flood occurring that will help them make a decision.

## Functions of random variables: the challenge

Plugging in a bounded logistic model for $d(h)$ and our lognormal model for $h$:
$$
\begin{align}
p(d) &= \int_h d(h) p(h) \, dh \\
&= \int_{-\infty}^\infty \mathbb{I}[h > 0] \text{logistic}(h) \mathcal{N}(h | \mu, \sigma^2) \, dh\\
&= \int_0^\infty \text{logistic}(h) \mathcal{N}(\mu, \sigma^2) \, dh\\
&= \int_0^\infty \frac{1}{1 + \exp(-k * (x - x0))} \frac {1}{\sigma {\sqrt {2\pi }}} \exp \left\{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2} \right\} \, dh
\end{align}
$$

### Limitations: analytic approach

We might be able to solve this analytically (Wolfram Alpha can't...).
But:

1. Numerous simplifying assumptions and approximations.
2. What if we want to use a different distribution?
3. A different damage model?

### Monte Carlo Solution Strategy

A deterministic strategy:
1. sample $h^s = 0, \Delta h, 2\Delta h, \ldots, (S-1)\Delta h$
2. compute $\text{logistic}(h^s) \mathcal{N}(h^s | \mu, \sigma^2)$ at each point and sum
3. drawbacks: we have to go to $\infty$ and select $\Delta h$.

A sampling strategy:
1. sample $h^1, h^2, \ldots, h^S \sim p(h)$ -- which we can do because we have a model for $p(h)$
2. for each value: compute $\mathbb{I}(h^s > 0) \text{logistic}(h^s)$ and take the average
3. this converges to the correct expectation!

## Parameter uncertainty propagation

### The core problem

We have been working with a single probability distribution for flood depths, which we computed by maximum likelihood.

These values are not precise.
What happens if we consider the lognormal distribution with slightly different, but still plausible, parameters?

What about the depth-damage parameters $x_0$ and $k$?

### Parameter uncertainty impact

- Uncertainties in our model parameters propagate to uncertainties in the things we care about.
- As we collect more data, fewer combinations of parameters are consistent with observations
- Different parameter combinations can lead to substantially different risk assessments

## Bayesian Inference with MCMC

### Prior Information: A Simple Example

Everyone is tested for CEVE543acitis, a rare and deadly disease:
1. It is known that 1 in 1,000 people have CEVE543acitis
2. The test is 99% accurate
3. Your test comes back positive. What is the probability that you have CEVE543acitis?

### Bayes' Rule: Discrete Event Version

$$
\Pr \left\{ \theta | y \right\} = \frac{\Pr \left\{ y | \theta \right\} \Pr \left\{ \theta \right\}}{\Pr \left\{ y \right\}}
$$

Define $y$ is getting a positive test result and $\theta$ is having the underlying condition.
Note that we do not observe $\theta$ directly!
Here $y=1$ and we want to know $\Pr\left\{\theta = 1 \mid y=1 \right\}$.

A naive application of maximum likelihood: $\Pr\left\{y=1 \mid \theta=1 \right\} > \Pr\left\{y=1 \mid \theta=0 \right\}$ so best estimate is $\theta=1$

### Key Ideas for Bayesian Inference

1. Parameters have **probability distributions**, not single point values 
2. Start with some prior distribution for parameters
3. Goal: what is the distribution of the parameters given the data?

### Bayes' Rule for Distributions

$$
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)}
$$

If we are drawing samples from a distribution, we can calculate up to a constant of proportionality and -- since $p(y)$ doesn't depend on $\theta$ -- we can usually ignore it.

$$
\overbrace{p(\theta \mid y)}^\rm{posterior} \propto \underbrace{p(y \mid \theta)}_\rm{likelihood} \overbrace{p(\theta)}^\rm{prior}
$$

### Markov Chain Monte Carlo (MCMC)

1. A class of methods for sampling from a probability distribution
2. Random walkers:
   1. Start at some value
   2. Propose a new value
   3. Accept or reject the new value based on some criteria
3. Repeat to get a "chain" of samples

### MCMC Limitations and Modern Solutions

1. Works great for a very simple problem
2. Computation blows up in higher dimensions (`p_accept` gets very small)
3. Have to code a new sampler for each problem

Modern samplers leverage gradients and clever tricks to draw better samples for harder problems.

## Value of Bayesian Inference

### Key Benefits

1. Draw samples from tricky posteriors to compute expectations $\mathbb{E}[f(\theta)]$
2. Quantify parametric uncertainty
   - In practice, sometimes this is a big deal and sometimes model structure uncertainties matter more
3. Force us to specify a data generating process
4. Computational methods fail loudly

### The Posterior as Compromise

The posterior is a compromise between the prior and the likelihood.

- Bad priors lead to bad inferences
- The choice of prior is subjective, which some people hate
- We approach this in a principled manner
- Lots of other steps are also subjective (choice of likelihood model, which data to use, problem framing, etc)
- False sense of objectivity is dangerous anyways!

::: {.callout-tip}
## Computational examples

For detailed computational examples, see the companion files:

1. [Bayesian Inference and MCMC: Computational Examples](/chapters/fundamentals/monte-carlo-bayesian.qmd) - MCMC sampling, prior/posterior analysis, Turing.jl usage

2. [Parameter Uncertainty and Risk Assessment: Computational Examples](/chapters/fundamentals/monte-carlo-uncertainty.qmd) - Monte Carlo simulation for flood risk, parameter uncertainty propagation, sensitivity analysis

Examples include:
- Flood risk assessment with uncertain parameters
- Depth-damage function analysis 
- Parameter sensitivity studies
- Return period analysis with uncertainty
- Bayesian model specification and sampling
- Prior predictive checks
- MCMC diagnostics and trace plots
:::
