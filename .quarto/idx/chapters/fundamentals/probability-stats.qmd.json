{"title":"Basics of Probability and Statistics","markdown":{"yaml":{"title":"Basics of Probability and Statistics"},"headingText":"Learning Objectives","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n- Understand foundational probability concepts (random variables, distributions, moments).\n- Apply descriptive and inferential statistical methods to climate-related datasets.\n- Recognize when and why certain probability models are appropriate for climate variables.\n\nTopics\n\n- Probability distributions (e.g., Normal, Gamma, Exponential) and their properties\n- Descriptive statistics (mean, variance, quantiles)\n- Basic hypothesis testing, p-values, and confidence intervals\n- Common pitfalls in statistical analysis of climate/earth science data\n\n## Old stuff\n\n```{julia}\n#| code-fold: true\n#| output: false\nusing CairoMakie\nusing Distributions\nusing LaTeXStrings\n```\n\n## What is probability?\n\n\n\n## Useful Probability distributions\n\n### The Normal distribution\n\nThe Normal (Gaussian) distribution has *probability distribution function* {{<glossary \"probability density function\">}}:\n\n$$\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n$$\n\n- Mean $\\mu$\n    - Median equal to mean\n- Variance $\\sigma^2$\n- Symmetric\n\n### Central limit theorem\n\nThe *central limit theorem* says that the sum of many independent random variables is approximately normally distributed.\n\nWe can see this with an example:\n\n1. For each sample $i = 1, \\ldots, N$:\n    1. Draw `J` draws from a non-Gaussian distribution $\\mathcal{D}$\n    2. Take the mean and save it as $\\bar{y}_i$\n2. Plot the distribution of $\\bar{y}_i$\n\n```{julia}\ndist = Gamma(2, 1) # a non-Gaussian distribution\nN = 10_000 # number of samples\nJ = 500 # draws per sample\nyÌ„ = [ # <1>\n    mean(rand(dist, J)) for _ in 1:N # <2>\n] # <3>\nhist(\n    yÌ„;\n    bins=50,\n    axis=(\n        xlabel=\"Sample mean\",\n        ylabel=\"Proportion of samples\",\n    ),\n    normalization=:probability\n)\n```\n\n1. To type `yÌ„`, type `y` then type `\\bar` and hit `tab`. Julia allows unicode (or emojis) in variable names\n2. To type `âˆˆ` , type `\\in` and hit `tab`. The `_` isn't doing anything special and we could name it `i` or ðŸ˜¶ or whatever we want but `_` suggests it's a throwaway\n3. This is another *list comprehension*\n\n### Notation\n\nWe will get tired of writing\n\n$$\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n$$\n\nInstead, we will often use shorthand:\n\n$$\ny \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\n### Normal PDF\n\n```{julia}\nlet\n    dist = Normal(0, 1)\n    x = range(-4, 4, 200)\n    y = pdf.(dist, x)\n\n    fig = Figure()\n    ax = Axis(fig[1, 1],\n        xlabel=L\"$x$\",\n        ylabel=L\"$p(x | \\mu=0, \\sigma=1)$\"\n    )\n    lines!(ax, x, y, label=\"Normal Distribution\")\n    axislegend()\n    fig\nend\n```\n\n1. `L\"<string>\"` allows us to use LaTeX in strings\n2. This notation specifies the values of $\\mu$ and $\\sigma$\n\n### Bernoulli distribution\n\nA Bernoulli distribution models a coin flip.\n\n```{julia}\np = 0.5 # probability of heads\nrand(Bernoulli(p), 5) # <1>\n```\n\n1. Draw 5 samples from the Bernoulli distribution with parameter `p`\n\n### Binomial distribution\n\nA Binomial distribution models the distribution of `n` consecutive flips of the same coin\n\n```{julia}\np = 0.5\nN = 5\nrand(Binomial(N, p), 5)\n```\n\n### Multinomial distribution\n\nThe Multinomial extends the Binomial to multiple categories.\nNote that `p` is a *vector*.\nIf there are 2 categories ($K=2$), it's just the binomial with $p_\\text{multinomial} = [p, 1-p]$.\"\n\n```{julia}\np = [0.5, 0.3, 0.2]\nN = 5\ndist = Multinomial(N, p)\nrand(dist, 5) # <1>\n```\n\n1. To be more concise, we could write `rand(Multinimial([0.5, 0.3, 0.2], 5), 5)`. Which is more readable?\n\n### Poisson distribution {.scrollable}\n\nThe Poisson distribution is used to model count data.\nIt is the limit of a Binomial distribution with $p=\\lambda/N$, as $N \\rightarrow \\infty$.\n\nA Poisson distribution has mean and variance equal to $\\lambda$.\n\n```{julia}\ndist = Poisson(2.5) # <1>\nrand(dist, 10) # <2>\n```\n\n1. The Poisson distribution has one parameter, $\\lambda$\n2. Draw 10 samples from the Poisson distribution\n\n### Negative binomial distribution\n\nThe `NegativeBinomaial` distribution relaxes the Poisson's assumotion that $\\text{mean} = \\text{variance}$.\n\nThis distribution models the number of successes in a sequence of independent and identically distributed Bernoulli trials with probability `p` before a specified (non-random) number of failures (`r`) occurs.\nFor example, we can define rolling a 6 on a dice as a failure, and rolling any other number as a success, and ask how many successful rolls will occur before we see the third failure (`p = 1/6` and `r = 3`).\n\n### What other distributions do you know?\n\n. . .\n\n- Uniform\n- Exponential\n- Gamma (see above)\n- Beta\n- Pareto\n- Student t\n- Boltzmann\n- Many more!\n\n## Statistics\n\n### Mean\n\nThe mean of a sample is just the sample average:\n$$\n\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y_i\n$$\n\n. . .\n\nThe mean of a distribution is the expected value of the distribution:\n$$\n\\mathbb{E}(u) = \\int u p(u) \\, du\n$$\n\n### Variance {.smaller}\n\nVariance measures how points differ from the mean\n\n. . .\n\nYou may be familiar with sample variance:\n$$\nS^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n$$\n\n. . .\n\nFor a distribution:\n$$\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u))^2 p(u) \\, du\n$$\nor, for a vector\n$$\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u)) (u - \\mathbb{E}(u))^T p(u) \\, du\n$$\n\n## PDFs and CDFs\n\n### PDF and CDF {.smaller .scrollable}\n\nIf $F_X$ is the *cumulative distribution function* (CDF) of $X$ and $f_X$ is the *probability density function* (PDF) of $X$, then:\n$$\nF_X ( x ) = \\int_{-\\infty}^x f_X(u) \\, du,\n$$\n\nand (if $f_X$ is continuous at $x$ which it typically will be)\n$$\nf_{X}(x)={\\frac {d}{dx}}F_{X}(x).\n$$\nA useful property is\n$$\n\\Pr[a\\leq X\\leq b]=\\int _{a}^{b}f_{X}(x)\\,dx\n$$\n\n::: {.callout-important}\nWe can only talk about the probability that $y$ is in some interval $[a, b]$, which is given by the integral of the PDF over that interval.\nThe probability that $y$ takes on the value $y^*$, written $p(y=y^*)$, is zero.\n:::\n\n### PDF example {.scrollable}\n\n```{julia}\n#| show: false\n## https://www.matecdev.com/posts/julia-numerical-integration.html\nfunction quad_trap(f, a, b, N)\n    h = (b - a) / N\n    int = h * (f(a) + f(b)) / 2\n    for k in 1:(N-1)\n        xk = (b - a) * k / N + a\n        int = int + h * f(xk)\n    end\n    return int\nend;\n```\n\nSimple example to illustrate that\n$$\nF_X(2) = \\int_{-\\infty}^2 f_X(u) \\, du\n$$\n\nWe will use a standard Normal distribution as an example\n\n```{julia}\ndist = Normal() ## <1>\nf(x) = pdf(dist, x) ## <2>\napprox = quad_trap(f, -100, 2, 1000) ## <3>\nexact = cdf(dist, 2)\n\napprox, exact\n```\n\n1. Mean 0 and standard deviation 1 by default\n2. `pdf(d, x)` tells us the probability density function of distribution `d` evaluated at `x`\n3. `quad_trap` is a trapezoidal approximation of the integral with arguments: function, lower bound, upper bound, and number of points\n\n### PMFs {.scrollable}\n\n```{julia}\nx = 0:20\ny = pdf.(Poisson(5), x)\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"x^*\",\n    ylabel=L\"p(x=x^*)\"\n)\nscatter!(ax, x, y; label=\"PMF\")\nlines!(ax, x, y)\naxislegend()\nfig\n```\n\n## Joint, marginal, and conditional distributions\n\n### Bayes' Rule\n\n$$\np(\\theta, y) = p(\\theta) p(y | \\theta)\n$$\nand thus\n$$\np(\\theta | y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(\\theta) p(y | \\theta)}{p(y)}\n$$\ngenerally:\n$$\np(\\theta | y) \\propto p(\\theta) p(y | \\theta)\n$$\n\n### Marginal probability\n\nProbability of event $A$: $\\Pr(A)$\n\n. . .\n\nWe will write the marginal probability density function as\n$$\np(\\theta) \\quad \\text{or} \\quad p(y)\n$$\n\n### Joint probability\n\nProbability of events $A$ and $B$: $\\Pr(A  \\& B)$\n\n. . .\n\n$$\np(\\theta, y)\n$$\n\n### Conditional probability\n\nProbability of event $A$ given event $B$: $\\Pr(A | B)$\n\n. . .\n\n$$\np(\\theta | y) \\quad \\text{or} \\quad p(y | \\theta)\n$$\n\n### Example: two-dice wager\n\n> A gambler presents you with an even-money wager.\nYou will roll two dice, and if the highest number showing is one, two, three or four, then you win.\nIf the highest number on either die is five or six, then she wins.\nShould you take the bet?\n\n## Example: linear regression\n\n### Overview\n\nStandard linear regression model, let's assume $x \\in \\mathbb{R}$ for simplicity (1 predictor):\n$$\ny_i = ax_i + b + \\epsilon_i\n$$\nwhere $\\epsilon_i \\sim N(0, \\sigma^2)$.\n\n### Conditional distribution of $y_i$\n\nThe conditional probability density of $y_i$ given $x_i$ is\n$$\np(y_i | x_i, a, b, \\sigma) = N(ax_i + b, \\sigma^2)\n$$\nwhich is a shorthand for writing out the full equation for the Normal PDF.\nWe can (and often will) write this as\n$$\ny_i \\sim \\mathcal{N}(ax_i + b, \\sigma^2)\n$$\nFinally, we will sometimes write $p(y_i | x_i)$ as a shorthand for $p(y_i | x_i, a, b, \\sigma)$.\nWhile fine in many circumstances, we should take care to make sure we are extremely clear about what parameters we are conditioning on.\n\n### Marginal distribution of $y_i$\n\nThe marginal probability density of $y_i$ is\n$$\np(y_i | a, b, \\sigma) = \\int p(y_i | x_i, a, b, \\sigma) p(x_i) \\, dx_i\n$$\nwhere $p(x_i)$ is the probability density of $x_i$.\n\n### Joint distribution of $y_i$ and $x_i$\n\nThe joint probability density of $y_i$ and $x_i$ is\n$$\np(y_i, x_i | a, b, \\sigma) = p(y_i | x_i, a, b, \\sigma) p(x_i)\n$$\nwhere $p(x_i)$ is the probability density of $x_i$.\n\n### Simulation {.scrollable .smaller}\n\n```{julia}\n#| output: false\nm = 2\nb = 1\nÏƒ = 1.5\n```\n\n. . .\n\nIf $x=2$, we can simulate from the conditional distribution of $y$:\n\n```{julia}\nN_sim = 10_000\nx = 2\ny = rand(Normal(m * x + b, Ïƒ), N_sim)\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    title=L\"p(y | x=2)\",\n    xlabel=L\"y\",\n    ylabel=\"Density\"\n)\nhist!(ax, y; bins=50, normalization=:pdf, label=\"Sampled\")\naxislegend()\nfig\n```\n\n\nIf $x \\sim N(0, 1)$, then we can simulate from the joint distribution of $x$ and $y$:\n\n```{julia}\nx = rand(Normal(0, 1), 10_000)\ny = [rand(Normal(m * xáµ¢ + b, Ïƒ)) for xáµ¢ in x] ## <1>\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"x\",\n    ylabel=L\"y\"\n)\nscatter!(ax, x, y; label=L\"p(x,y)\")\naxislegend()\nfig\n```\n\n1. A list comprehension here is less elegant than writing `rand.(Normal.(m .* x .+ b, Ïƒ))` but it is easy to read. The results are the same.\n\n\nFinally, assuming the same distribution, we can simulate from the marginal distribution of $y$:\n\n```{julia}\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"y\",\n    ylabel=\"Density\"\n)\nhist!(ax, y; bins=50, normalization=:pdf, label=L\"p(y)\")\naxislegend()\nfig\n```\n\n\n\n## Example: negative binomial as a mixture\n\n### Overview\n\nThe Negative Binomial distribution (see last lecture) can be interpreted as a Gamma-Poisson *mixture*:\n\n$$\n\\begin{align}\ny &\\sim \\textrm{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\textrm{Gamma}\\left(r, \\frac{p}{1-p} \\right)\n\\end{align}\n$$\n\n### Mathematical derivation\n\nWe can show mathematically that if $y ~ \\textrm{Negative Binomial}(r, p)$, that is equivalent to the mixture model $y ~ \\textrm{Poisson}(\\lambda)$ and $\\lambda ~ \\textrm{Gamma}(r, p / (1 - p))$.\n$$\n\\begin{align}\n& \\int_0^{\\infty} f_{\\text {Poisson }(\\lambda)}(y) \\times f_{\\operatorname{Gamma}\\left(r, \\frac{p}{1-p}\\right)}(\\lambda) \\mathrm{d} \\lambda \\\\\n& = \\int_0^{\\infty} \\frac{\\lambda^y}{y !} e^{-\\lambda} \\times \\frac{1}{\\Gamma(r)}\\left(\\frac{p}{1-p} \\lambda\\right)^{r-1} e^{-\\frac{p}{1-p} \\lambda}\\left(\\frac{p}{1-p} \\mathrm{~d} \\lambda\\right) \\\\\n\\ldots \\\\\n&= f_{\\text {Negative Binomial }(r, p)}(y)\n\\end{align}\n$$\nFor all the steps see [Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture){target=_blank}.\n\n### Simulation example {.scrollable}\n\nWe can see this with simulation.\nFirst we define a function to simulate from the Gamma-Poisson mixture:\n\n```{julia}\nfunction gamma_poisson(r, p)\n    g_dist = Gamma(r, (1 - p) / p)\n    Î» = rand(g_dist)\n    p_dist = Poisson(Î»)\n    return rand(p_dist)\nend\n```\n\n. . .\n\nThen we can simulate from the mixture and compare to the Negative Binomial distribution:\n\n```{julia}\nr = 3 # number of failures\np = 1 / 6 # probability of failure\ndist = NegativeBinomial(r, p)\n\n# simulate rolls\nN = 1_000_000\nrolls = [gamma_poisson(r, p) for _ in 1:N]\n\n# plot the samples\nxticks = 0:1:60 # specify the bin values\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=\"Number of rolls\",\n    ylabel=\"PMF\",\n)\n\n# Add histogram of simulated data\nhist!(ax, rolls;\n    bins=length(xticks) + 1,\n    normalization=:pdf,\n    label=\"Gamma-Poisson\",\n)\n\n# Add the PDF of the negative binomial distribution\nlines!(ax, xticks, pdf.(dist, xticks);\n    linewidth=2,\n    label=\"Neg. Binom.\"\n)\n\nxlims!(ax, 0, 60)\naxislegend()\nfig\n```\n\n### So what? {.smaller .scrollable}\n\nI don't need you to know all the details of this particular mixture model.\nWhat I *do* want you to understand is:\n\n1. We can model data using combinations of simpler distributions\n1. We can use simple simulation approaches to approximate more complex relationships\n    1. For example, if we wanted to know $\\Pr(y > 10)$ when $y \\sim \\text{Negative Binomial}(r, p)$ but we didn't have a Negative Binomial distribution in our software package we could estimate our quantity of interest\n    1. This isn't very interesting for this model (there is an analytic solution!) but lots of models we might want to write down don't have analytic solutions\n\n## Wrapup\n\n### Key ideas\n\n- Conditional probability\n- Joint probability\n- Marginal probability\n- Bayes' Rule\n- Likelihood\n- Posterior\n- Simulation methods\n\n\n\n","srcMarkdownNoYaml":"\n\n## Learning Objectives {.unnumbered}\n\n- Understand foundational probability concepts (random variables, distributions, moments).\n- Apply descriptive and inferential statistical methods to climate-related datasets.\n- Recognize when and why certain probability models are appropriate for climate variables.\n\nTopics\n\n- Probability distributions (e.g., Normal, Gamma, Exponential) and their properties\n- Descriptive statistics (mean, variance, quantiles)\n- Basic hypothesis testing, p-values, and confidence intervals\n- Common pitfalls in statistical analysis of climate/earth science data\n\n## Old stuff\n\n```{julia}\n#| code-fold: true\n#| output: false\nusing CairoMakie\nusing Distributions\nusing LaTeXStrings\n```\n\n## What is probability?\n\n\n\n## Useful Probability distributions\n\n### The Normal distribution\n\nThe Normal (Gaussian) distribution has *probability distribution function* {{<glossary \"probability density function\">}}:\n\n$$\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n$$\n\n- Mean $\\mu$\n    - Median equal to mean\n- Variance $\\sigma^2$\n- Symmetric\n\n### Central limit theorem\n\nThe *central limit theorem* says that the sum of many independent random variables is approximately normally distributed.\n\nWe can see this with an example:\n\n1. For each sample $i = 1, \\ldots, N$:\n    1. Draw `J` draws from a non-Gaussian distribution $\\mathcal{D}$\n    2. Take the mean and save it as $\\bar{y}_i$\n2. Plot the distribution of $\\bar{y}_i$\n\n```{julia}\ndist = Gamma(2, 1) # a non-Gaussian distribution\nN = 10_000 # number of samples\nJ = 500 # draws per sample\nyÌ„ = [ # <1>\n    mean(rand(dist, J)) for _ in 1:N # <2>\n] # <3>\nhist(\n    yÌ„;\n    bins=50,\n    axis=(\n        xlabel=\"Sample mean\",\n        ylabel=\"Proportion of samples\",\n    ),\n    normalization=:probability\n)\n```\n\n1. To type `yÌ„`, type `y` then type `\\bar` and hit `tab`. Julia allows unicode (or emojis) in variable names\n2. To type `âˆˆ` , type `\\in` and hit `tab`. The `_` isn't doing anything special and we could name it `i` or ðŸ˜¶ or whatever we want but `_` suggests it's a throwaway\n3. This is another *list comprehension*\n\n### Notation\n\nWe will get tired of writing\n\n$$\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n$$\n\nInstead, we will often use shorthand:\n\n$$\ny \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\n### Normal PDF\n\n```{julia}\nlet\n    dist = Normal(0, 1)\n    x = range(-4, 4, 200)\n    y = pdf.(dist, x)\n\n    fig = Figure()\n    ax = Axis(fig[1, 1],\n        xlabel=L\"$x$\",\n        ylabel=L\"$p(x | \\mu=0, \\sigma=1)$\"\n    )\n    lines!(ax, x, y, label=\"Normal Distribution\")\n    axislegend()\n    fig\nend\n```\n\n1. `L\"<string>\"` allows us to use LaTeX in strings\n2. This notation specifies the values of $\\mu$ and $\\sigma$\n\n### Bernoulli distribution\n\nA Bernoulli distribution models a coin flip.\n\n```{julia}\np = 0.5 # probability of heads\nrand(Bernoulli(p), 5) # <1>\n```\n\n1. Draw 5 samples from the Bernoulli distribution with parameter `p`\n\n### Binomial distribution\n\nA Binomial distribution models the distribution of `n` consecutive flips of the same coin\n\n```{julia}\np = 0.5\nN = 5\nrand(Binomial(N, p), 5)\n```\n\n### Multinomial distribution\n\nThe Multinomial extends the Binomial to multiple categories.\nNote that `p` is a *vector*.\nIf there are 2 categories ($K=2$), it's just the binomial with $p_\\text{multinomial} = [p, 1-p]$.\"\n\n```{julia}\np = [0.5, 0.3, 0.2]\nN = 5\ndist = Multinomial(N, p)\nrand(dist, 5) # <1>\n```\n\n1. To be more concise, we could write `rand(Multinimial([0.5, 0.3, 0.2], 5), 5)`. Which is more readable?\n\n### Poisson distribution {.scrollable}\n\nThe Poisson distribution is used to model count data.\nIt is the limit of a Binomial distribution with $p=\\lambda/N$, as $N \\rightarrow \\infty$.\n\nA Poisson distribution has mean and variance equal to $\\lambda$.\n\n```{julia}\ndist = Poisson(2.5) # <1>\nrand(dist, 10) # <2>\n```\n\n1. The Poisson distribution has one parameter, $\\lambda$\n2. Draw 10 samples from the Poisson distribution\n\n### Negative binomial distribution\n\nThe `NegativeBinomaial` distribution relaxes the Poisson's assumotion that $\\text{mean} = \\text{variance}$.\n\nThis distribution models the number of successes in a sequence of independent and identically distributed Bernoulli trials with probability `p` before a specified (non-random) number of failures (`r`) occurs.\nFor example, we can define rolling a 6 on a dice as a failure, and rolling any other number as a success, and ask how many successful rolls will occur before we see the third failure (`p = 1/6` and `r = 3`).\n\n### What other distributions do you know?\n\n. . .\n\n- Uniform\n- Exponential\n- Gamma (see above)\n- Beta\n- Pareto\n- Student t\n- Boltzmann\n- Many more!\n\n## Statistics\n\n### Mean\n\nThe mean of a sample is just the sample average:\n$$\n\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y_i\n$$\n\n. . .\n\nThe mean of a distribution is the expected value of the distribution:\n$$\n\\mathbb{E}(u) = \\int u p(u) \\, du\n$$\n\n### Variance {.smaller}\n\nVariance measures how points differ from the mean\n\n. . .\n\nYou may be familiar with sample variance:\n$$\nS^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n$$\n\n. . .\n\nFor a distribution:\n$$\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u))^2 p(u) \\, du\n$$\nor, for a vector\n$$\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u)) (u - \\mathbb{E}(u))^T p(u) \\, du\n$$\n\n## PDFs and CDFs\n\n### PDF and CDF {.smaller .scrollable}\n\nIf $F_X$ is the *cumulative distribution function* (CDF) of $X$ and $f_X$ is the *probability density function* (PDF) of $X$, then:\n$$\nF_X ( x ) = \\int_{-\\infty}^x f_X(u) \\, du,\n$$\n\nand (if $f_X$ is continuous at $x$ which it typically will be)\n$$\nf_{X}(x)={\\frac {d}{dx}}F_{X}(x).\n$$\nA useful property is\n$$\n\\Pr[a\\leq X\\leq b]=\\int _{a}^{b}f_{X}(x)\\,dx\n$$\n\n::: {.callout-important}\nWe can only talk about the probability that $y$ is in some interval $[a, b]$, which is given by the integral of the PDF over that interval.\nThe probability that $y$ takes on the value $y^*$, written $p(y=y^*)$, is zero.\n:::\n\n### PDF example {.scrollable}\n\n```{julia}\n#| show: false\n## https://www.matecdev.com/posts/julia-numerical-integration.html\nfunction quad_trap(f, a, b, N)\n    h = (b - a) / N\n    int = h * (f(a) + f(b)) / 2\n    for k in 1:(N-1)\n        xk = (b - a) * k / N + a\n        int = int + h * f(xk)\n    end\n    return int\nend;\n```\n\nSimple example to illustrate that\n$$\nF_X(2) = \\int_{-\\infty}^2 f_X(u) \\, du\n$$\n\nWe will use a standard Normal distribution as an example\n\n```{julia}\ndist = Normal() ## <1>\nf(x) = pdf(dist, x) ## <2>\napprox = quad_trap(f, -100, 2, 1000) ## <3>\nexact = cdf(dist, 2)\n\napprox, exact\n```\n\n1. Mean 0 and standard deviation 1 by default\n2. `pdf(d, x)` tells us the probability density function of distribution `d` evaluated at `x`\n3. `quad_trap` is a trapezoidal approximation of the integral with arguments: function, lower bound, upper bound, and number of points\n\n### PMFs {.scrollable}\n\n```{julia}\nx = 0:20\ny = pdf.(Poisson(5), x)\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"x^*\",\n    ylabel=L\"p(x=x^*)\"\n)\nscatter!(ax, x, y; label=\"PMF\")\nlines!(ax, x, y)\naxislegend()\nfig\n```\n\n## Joint, marginal, and conditional distributions\n\n### Bayes' Rule\n\n$$\np(\\theta, y) = p(\\theta) p(y | \\theta)\n$$\nand thus\n$$\np(\\theta | y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(\\theta) p(y | \\theta)}{p(y)}\n$$\ngenerally:\n$$\np(\\theta | y) \\propto p(\\theta) p(y | \\theta)\n$$\n\n### Marginal probability\n\nProbability of event $A$: $\\Pr(A)$\n\n. . .\n\nWe will write the marginal probability density function as\n$$\np(\\theta) \\quad \\text{or} \\quad p(y)\n$$\n\n### Joint probability\n\nProbability of events $A$ and $B$: $\\Pr(A  \\& B)$\n\n. . .\n\n$$\np(\\theta, y)\n$$\n\n### Conditional probability\n\nProbability of event $A$ given event $B$: $\\Pr(A | B)$\n\n. . .\n\n$$\np(\\theta | y) \\quad \\text{or} \\quad p(y | \\theta)\n$$\n\n### Example: two-dice wager\n\n> A gambler presents you with an even-money wager.\nYou will roll two dice, and if the highest number showing is one, two, three or four, then you win.\nIf the highest number on either die is five or six, then she wins.\nShould you take the bet?\n\n## Example: linear regression\n\n### Overview\n\nStandard linear regression model, let's assume $x \\in \\mathbb{R}$ for simplicity (1 predictor):\n$$\ny_i = ax_i + b + \\epsilon_i\n$$\nwhere $\\epsilon_i \\sim N(0, \\sigma^2)$.\n\n### Conditional distribution of $y_i$\n\nThe conditional probability density of $y_i$ given $x_i$ is\n$$\np(y_i | x_i, a, b, \\sigma) = N(ax_i + b, \\sigma^2)\n$$\nwhich is a shorthand for writing out the full equation for the Normal PDF.\nWe can (and often will) write this as\n$$\ny_i \\sim \\mathcal{N}(ax_i + b, \\sigma^2)\n$$\nFinally, we will sometimes write $p(y_i | x_i)$ as a shorthand for $p(y_i | x_i, a, b, \\sigma)$.\nWhile fine in many circumstances, we should take care to make sure we are extremely clear about what parameters we are conditioning on.\n\n### Marginal distribution of $y_i$\n\nThe marginal probability density of $y_i$ is\n$$\np(y_i | a, b, \\sigma) = \\int p(y_i | x_i, a, b, \\sigma) p(x_i) \\, dx_i\n$$\nwhere $p(x_i)$ is the probability density of $x_i$.\n\n### Joint distribution of $y_i$ and $x_i$\n\nThe joint probability density of $y_i$ and $x_i$ is\n$$\np(y_i, x_i | a, b, \\sigma) = p(y_i | x_i, a, b, \\sigma) p(x_i)\n$$\nwhere $p(x_i)$ is the probability density of $x_i$.\n\n### Simulation {.scrollable .smaller}\n\n```{julia}\n#| output: false\nm = 2\nb = 1\nÏƒ = 1.5\n```\n\n. . .\n\nIf $x=2$, we can simulate from the conditional distribution of $y$:\n\n```{julia}\nN_sim = 10_000\nx = 2\ny = rand(Normal(m * x + b, Ïƒ), N_sim)\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    title=L\"p(y | x=2)\",\n    xlabel=L\"y\",\n    ylabel=\"Density\"\n)\nhist!(ax, y; bins=50, normalization=:pdf, label=\"Sampled\")\naxislegend()\nfig\n```\n\n\nIf $x \\sim N(0, 1)$, then we can simulate from the joint distribution of $x$ and $y$:\n\n```{julia}\nx = rand(Normal(0, 1), 10_000)\ny = [rand(Normal(m * xáµ¢ + b, Ïƒ)) for xáµ¢ in x] ## <1>\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"x\",\n    ylabel=L\"y\"\n)\nscatter!(ax, x, y; label=L\"p(x,y)\")\naxislegend()\nfig\n```\n\n1. A list comprehension here is less elegant than writing `rand.(Normal.(m .* x .+ b, Ïƒ))` but it is easy to read. The results are the same.\n\n\nFinally, assuming the same distribution, we can simulate from the marginal distribution of $y$:\n\n```{julia}\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"y\",\n    ylabel=\"Density\"\n)\nhist!(ax, y; bins=50, normalization=:pdf, label=L\"p(y)\")\naxislegend()\nfig\n```\n\n\n\n## Example: negative binomial as a mixture\n\n### Overview\n\nThe Negative Binomial distribution (see last lecture) can be interpreted as a Gamma-Poisson *mixture*:\n\n$$\n\\begin{align}\ny &\\sim \\textrm{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\textrm{Gamma}\\left(r, \\frac{p}{1-p} \\right)\n\\end{align}\n$$\n\n### Mathematical derivation\n\nWe can show mathematically that if $y ~ \\textrm{Negative Binomial}(r, p)$, that is equivalent to the mixture model $y ~ \\textrm{Poisson}(\\lambda)$ and $\\lambda ~ \\textrm{Gamma}(r, p / (1 - p))$.\n$$\n\\begin{align}\n& \\int_0^{\\infty} f_{\\text {Poisson }(\\lambda)}(y) \\times f_{\\operatorname{Gamma}\\left(r, \\frac{p}{1-p}\\right)}(\\lambda) \\mathrm{d} \\lambda \\\\\n& = \\int_0^{\\infty} \\frac{\\lambda^y}{y !} e^{-\\lambda} \\times \\frac{1}{\\Gamma(r)}\\left(\\frac{p}{1-p} \\lambda\\right)^{r-1} e^{-\\frac{p}{1-p} \\lambda}\\left(\\frac{p}{1-p} \\mathrm{~d} \\lambda\\right) \\\\\n\\ldots \\\\\n&= f_{\\text {Negative Binomial }(r, p)}(y)\n\\end{align}\n$$\nFor all the steps see [Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture){target=_blank}.\n\n### Simulation example {.scrollable}\n\nWe can see this with simulation.\nFirst we define a function to simulate from the Gamma-Poisson mixture:\n\n```{julia}\nfunction gamma_poisson(r, p)\n    g_dist = Gamma(r, (1 - p) / p)\n    Î» = rand(g_dist)\n    p_dist = Poisson(Î»)\n    return rand(p_dist)\nend\n```\n\n. . .\n\nThen we can simulate from the mixture and compare to the Negative Binomial distribution:\n\n```{julia}\nr = 3 # number of failures\np = 1 / 6 # probability of failure\ndist = NegativeBinomial(r, p)\n\n# simulate rolls\nN = 1_000_000\nrolls = [gamma_poisson(r, p) for _ in 1:N]\n\n# plot the samples\nxticks = 0:1:60 # specify the bin values\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=\"Number of rolls\",\n    ylabel=\"PMF\",\n)\n\n# Add histogram of simulated data\nhist!(ax, rolls;\n    bins=length(xticks) + 1,\n    normalization=:pdf,\n    label=\"Gamma-Poisson\",\n)\n\n# Add the PDF of the negative binomial distribution\nlines!(ax, xticks, pdf.(dist, xticks);\n    linewidth=2,\n    label=\"Neg. Binom.\"\n)\n\nxlims!(ax, 0, 60)\naxislegend()\nfig\n```\n\n### So what? {.smaller .scrollable}\n\nI don't need you to know all the details of this particular mixture model.\nWhat I *do* want you to understand is:\n\n1. We can model data using combinations of simpler distributions\n1. We can use simple simulation approaches to approximate more complex relationships\n    1. For example, if we wanted to know $\\Pr(y > 10)$ when $y \\sim \\text{Negative Binomial}(r, p)$ but we didn't have a Negative Binomial distribution in our software package we could estimate our quantity of interest\n    1. This isn't very interesting for this model (there is an analytic solution!) but lots of models we might want to write down don't have analytic solutions\n\n## Wrapup\n\n### Key ideas\n\n- Conditional probability\n- Joint probability\n- Marginal probability\n- Bayes' Rule\n- Likelihood\n- Posterior\n- Simulation methods\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","reference-location":"section","html-math-method":"mathjax","output-file":"probability-stats.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../references.bib"],"glossary":{"path":"../../_glossary.yml","popup":"click","show":true},"kernel":"julia-1.11","theme":"spacelab","anchor-sections":true,"callout-appearance":"simple","citations-hover":true,"code-annotations":"hover","date-format":"ddd., MMM. D","title":"Basics of Probability and Statistics"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}